<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>GAN,VAE和流模型的原理（以流模型为主） | Raphael's Home</title><meta name="author" content="Raphael Hyaan"><meta name="copyright" content="Raphael Hyaan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="GAN,VAE和流模型的原理（以流模型为主） 2025&#x2F;02&#x2F;26 以便自己查看  生成式神经网络 生成式神经网络是一种以生成与被提供的数据集相似的新数据为目的的人工神经网络。它试图构建一种模型分布$p{model}$，使其尽可能接近原始数据分布$p{data}$。由于不依赖其他标签数据，生成式模型往往是无监督学习。特别的，生成式模型也可以预测在$y$条件下$x$的分布$P(x|y)$，比如要求一">
<meta property="og:type" content="article">
<meta property="og:title" content="GAN,VAE和流模型的原理（以流模型为主）">
<meta property="og:url" content="http://raphaelhyaan.cn/2025/02/26/informatique/deeplearning/Generative-11/index.html">
<meta property="og:site_name" content="Raphael&#39;s Home">
<meta property="og:description" content="GAN,VAE和流模型的原理（以流模型为主） 2025&#x2F;02&#x2F;26 以便自己查看  生成式神经网络 生成式神经网络是一种以生成与被提供的数据集相似的新数据为目的的人工神经网络。它试图构建一种模型分布$p{model}$，使其尽可能接近原始数据分布$p{data}$。由于不依赖其他标签数据，生成式模型往往是无监督学习。特别的，生成式模型也可以预测在$y$条件下$x$的分布$P(x|y)$，比如要求一">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F%E8%9D%B61.png">
<meta property="article:published_time" content="2025-02-26T08:30:58.000Z">
<meta property="article:modified_time" content="2025-06-12T14:49:11.434Z">
<meta property="article:author" content="Raphael Hyaan">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F%E8%9D%B61.png"><link rel="shortcut icon" href="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Flana cat 002.png"><link rel="canonical" href="http://raphaelhyaan.cn/2025/02/26/informatique/deeplearning/Generative-11/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GAN,VAE和流模型的原理（以流模型为主）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-06-12 22:49:11'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/butterflyChange/css/code.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-2.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">187</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/Reading/"><i class="fa-fw fas fa-book"></i><span> Reading</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F蝶1.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Raphael's Home"><span class="site-name">Raphael's Home</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/Reading/"><i class="fa-fw fas fa-book"></i><span> Reading</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">GAN,VAE和流模型的原理（以流模型为主）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-26T08:30:58.000Z" title="发表于 2025-02-26 16:30:58">2025-02-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-12T14:49:11.434Z" title="更新于 2025-06-12 22:49:11">2025-06-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%94%9F%E6%88%90%E5%BC%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">生成式神经网络</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="GAN,VAE和流模型的原理（以流模型为主）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="GAN-VAE和流模型的原理（以流模型为主）"><a href="#GAN-VAE和流模型的原理（以流模型为主）" class="headerlink" title="GAN,VAE和流模型的原理（以流模型为主）"></a>GAN,VAE和流模型的原理（以流模型为主）</h1><blockquote>
<p>2025/02/26 以便自己查看</p>
</blockquote>
<h3 id="生成式神经网络"><a href="#生成式神经网络" class="headerlink" title="生成式神经网络"></a>生成式神经网络</h3><hr>
<p>生成式神经网络是一种以生成与被提供的数据集相似的新数据为目的的人工神经网络。它试图构建一种模型分布$p<em>{model}$，使其尽可能接近原始数据分布$p</em>{data}$。由于不依赖其他标签数据，生成式模型往往是无监督学习。<br>特别的，生成式模型也可以预测在$y$条件下$x$的分布$P(x|y)$，比如要求一个用于生成水果图像的模型生成一个苹果：$P(\text{水果}|\text{苹果})$。</p>
<p>对于数据的集合$\mathbf{X}$，生成式神经网络理想的损失函数是模型分布和数据分布之间的似然函数：</p>
<script type="math/tex; mode=display">
\begin{align}    \mathcal{L}(model(\theta) \mid \mathbf{X})= \prod_{x \in \mathbf{X}} p_{model(\theta)}(x)\end{align}</script><p>似然函数是模型可调节参数$\theta$的函数，用于衡量模型分布对数据分布的拟合程度，所以最大化似然函数是模型训练的目的。<br>在实际训练中，我们不希望计算大量0到1之间的数值的乘积，并且我们希望最小化损失函数，因此一般使用负对数似然函数作为损失函数。理想的模型参数$\hat \theta$即为负对数似然函数取最小值时的模型参数：</p>
<script type="math/tex; mode=display">
\begin{align}    &loss=-\sum_{x \in \mathbf{X}} \log p_{model(\theta)}(x)\\    &\hat \theta = argmin(-\sum_{x \in \mathbf{X}} \log p_{model(\theta)}(x))\end{align}</script><p>然而，对于高维问题，直接计算概率$p_{model(\theta)}(x)$是不可行的。对此，有三种主流的解决方法：隐性建模，近似显性建模和易解显性建模。</p>
<ul>
<li>隐性建模不关注于分布的估计，这些模型专注于数据的生成，并使用其他方法保证生成的数据与真实数据分布接近。</li>
<li>近似显性建模尝试使用近似方式估测模型分布和数据分布间的差异。</li>
<li>易解显性建模直接定义了方便求解的似然分布，并且直接应用似然函数优化模型。</li>
</ul>
<h3 id="GAN-生成对抗网络"><a href="#GAN-生成对抗网络" class="headerlink" title="GAN 生成对抗网络"></a>GAN 生成对抗网络</h3><hr>
<p>对抗生成网络(GAN)包含两个子网络，分别称为生成器(G)和判别器(D)。前者从某一简单分布$p<em>Z(z)$出发，构建模型分布$p</em>{G}(x)$。后者分辨数据来源自生成还是属于真实数据，并将由生成器生成的数据标记为0，真实数据标记为1。</p>
<p>具体来说，GAN并没有使用最大似然函数法进行模型训练，而是重新定义了一个最小最大问题，即最小化生成器的欺骗判别器的能力，并最大化判别器的辨识由生成器生成的数据的能力。</p>
<script type="math/tex; mode=display">
\begin{align}    & V(G,D) =  \mathbb{E}_{x \sim p_{data }(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]  \\    & G^*, D^* = \underset{G}{\operatorname{argmax}}  \ \underset{D}{\operatorname{argmax}} V(G,D)\end{align}</script><p>在细节方面，为了训练模型，需评价模型分布$p<em>{G}(x)$和数据分布$p</em>{data}(x)$的相似性。在GAN中，辨别器扮演这一角色。我们将重点关注这种方法的理论依据。假设生成器参数固定，展开V(G,D)得：</p>
<script type="math/tex; mode=display">
\begin{align}     V(G,D)&=\int_{x} \left[p_{data }(x) \log (D(x))+p_{G}(x) \log (1-D(x))\right] d x\end{align}</script><p>设D在这种情况下的的最佳参数取值$D^*$为：</p>
<script type="math/tex; mode=display">
\begin{align}    D^* &=  \underset{D}{\operatorname{argmax}} \int_{x} \left[p_{data }(x) \log (D(x))+p_{G}(x) \log (1-D(x))\right] d x\end{align}</script><p>其中，$p<em>{data }(x) \log (D(x))+p</em>{G}(x) \log (1-D(x))$在$[0,1]$之间有最大值。求导以求出最大时对应的$D(x)$的表达式，可得在生成器网络参数固定时，对于每一个$x$，理想的判别器$D^*$为：</p>
<script type="math/tex; mode=display">
\begin{align}    D^*(x) = \frac{p_{data}(x)}{p_{data}(x)+p_{G}(x)}\end{align}</script><p>将这一表示带回公式$(4)$:</p>
<script type="math/tex; mode=display">
\begin{align}    V(G,D^*)&=\mathbb{E}_{x \sim p_{data }(x)}[\log \frac{p_{data}(x)}{p_{data}(x)+p_{G}(x)}]\nonumber\\    &+\mathbb{E}_{z \sim p_{G}(x)}[\log \frac{p_{G}(x)}{p_{data}(x)+p_{G}(x)}]    \end{align}</script><p>为了进一步理解公式，引入KL散度和JS散度。它们都是衡量两个概率分布之间的相似性的指标。</p>
<script type="math/tex; mode=display">
\begin{align}        &D_{K L}(p || q)=\mathbb{E}[\log \frac {p\left(x_i\right)}{q\left(x_i\right)}]\\    &D_{J S}(p || q)=\frac{1}{2} D_{K L}(p || \frac{p+q}{2})+\frac{1}{2} D_{K L}(q || \frac{p+q}{2})\end{align}</script><p>使用两种散度表示公式$(9)$，可得：</p>
<script type="math/tex; mode=display">
\begin{align}    V(G,D^*)&=-\log(4)+2\cdot D_{JS}(p_{data}||p_{G})\end{align}</script><p>由此可知，在理想情况下，GAN期望训练判别器网络，使得其近似于数据分布和模型分布的JS散度，从而将其作为衡量两个分布相似性的指标。然后使用由判别器网络近似的JS散度作为损失函数，并将其最小化获得最优的生成器模型。</p>
<p>实际情况下，在GAN的训练中，生成器和判别器的训练是交替进行的，因此为了达到上述效果，只能期望在一次将判别器训练至最优。但这是十分难以实现的。实际情况下损失函数是通过神经网络方法对JS散度的隐性近似。</p>
<h3 id="VAE-变分自动编码器"><a href="#VAE-变分自动编码器" class="headerlink" title="VAE 变分自动编码器"></a>VAE 变分自动编码器</h3><hr>
<p>变分自动编码器(VAE)是贝叶斯概率在深度学习领域中最广为人知的应用之一。VAE包含编码器(E)和解码器(D)两个网络。在生成时，从某先验分布$p_Z(z)$中采样潜在空间向量$z$，并使用解码器解码得到$x$。<br>由于似然度难以直接计算，VAE使用变分方法和KL散度确定了似然度的下界，然后对下界进行进一步的优化</p>
<p>具体来说，解码器代表给定潜在空间向量z时，变量x的条件分布$p<em>{model_D}(x|z)$，根据全概率公式和贝叶斯定理，可以将模型分布$p</em>{D}(x)$表示为潜在空间向量的先验分布和条件分布的乘积的积分：</p>
<script type="math/tex; mode=display">
\begin{align}    p_{D}(x)=\int p_{D}(x|z)p_{Z}(z)dz\end{align}</script><p>在这个公式中，$p<em>{D}(x)$与前文的$p</em>{model(\theta)}(x)$等价，代表VAE模型中的模型分布。此时，负对数似然可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align}
    &loss=-\sum_{x \in \mathbf{X}} \log p_{D}(x) = -\sum_{x \in \mathbf{X}} \log \int p_{D}(x|z)p_{Z}(z)dz
\end{align}</script><p>然而，这个高维积分有着很高的计算复杂度，是难以计算的。一个可行的简化思路为缩小连续随机变量z的取值范围，将积分问题转化为求和问题。在这个过程中，为了不影响生成的精度，尽可能选取最能生成可靠数据的潜在空间向量是至关重要的。为此，VAE不再使用z的先验分布，而是使用与真实数据相联系的后验概率分布。</p>
<script type="math/tex; mode=display">
\begin{align}
    p_{D}(z|x) = \frac{p_{D}(x|z)\cdot p_{Z}(z)} {p_{D}(x)}
\end{align}</script><p>由于$p<em>{D}(x|z)$和$p</em>{D}(x)$难以表述，后验概率的直接计算同样是不可能的。为此，需要引入编码器，构成另一个分布$q_E(z|x)$去近似后验概率分布，并同时引入KL散度作为两个分布相似程度的评价标准。</p>
<script type="math/tex; mode=display">
\begin{align}
    D_{KL}(q_E(z|x)||p_{D}(x|z)) &= \mathbb{E}_{z\sim q_E(z|x)}\left[\log\left(\frac{q_E(z|x)}{p_{D}(x|z)}\right)\right]
     = \mathbb{E}_{z\sim q_E(z|x)}\left[\log\frac{q_E(z|x)\cdot p_{D}(x)}{p_{Z}(z) \cdot p_{D}(x|z)}\right]\nonumber\\
    & = D_{KL}(q_E(z|x)||p_{Z}(z))- \mathbb{E}_{z\sim q_E(z|x)}\left[p_{D}(x|z)\right]+ \log p_{D}(x)

\end{align}</script><p>其中，$\log p_{D}(x)$是我们需要计算最小负对数似然的部分，整理公式得：</p>
<script type="math/tex; mode=display">
\begin{align}
    \log p_{D}(x) = D_{KL}(q_E(z|x)||p_{D}(x|z)) + \mathbb{E}_{z\sim q_E(z|x)}\left[p_{D}(x|z)\right]-D_{KL}(q_E(z|x)||p_{Z}(z))
\end{align}</script><p>尽管$D<em>{KL}(q_E(z|x)||p</em>{D}(x|z))$依然无法运算，但其恒为正数，仅在$q<em>E(z|x)$和$p</em>{D}(x|z)$完全相同时取0，因此$\log p_{D}(x)$的下界可以确认。这个下界被称为变分下界，是VAE的核心思想。</p>
<script type="math/tex; mode=display">
\begin{align}
    \log p_{D}(x) \ge \mathbb{E}_{z\sim q_E(z|x)}\left[p_{D}(x|z)\right]-D_{KL}(q_E(z|x)||p_{Z}(z))
\end{align}</script><p>其中，第一项衡量输入向量x和将其编码为z，再解码得到的新数据$\hat x$的差异，称为重构误差。第二项衡量编码器和先验分布之间的差异。通过变分下界，VAE成功的取得了似然函数的近似值，使得模型训练成为可能。</p>
<h3 id="流模型"><a href="#流模型" class="headerlink" title="流模型"></a>流模型</h3><hr>
<p><strong>流模型的可逆性和雅可比行列式可计算性</strong></p>
<p>易解显性建模的关键在于似然分布的计算。流模型通过本身具备的可逆性和雅可比矩阵行列式可计算性，将难以计算似然的分布转化为如正态分布这般易于计算的分布</p>
<p>具体来说，流模型定义了一组可逆函数$x=g(z) \Leftrightarrow z=f(x)$可逆将输入向量$x$从样本空间映射到潜在空间，并假定潜在空间向量$p_{model}(z)$为多元标准正态分布。根据双射的性质，$x$和$z$必须是等维度的。根据概率密度的换元公式，有：</p>
<script type="math/tex; mode=display">
\begin{align}    &p_{model}(x) = p_{model}(z)\left | det\left(\frac{\partial z}{\partial x}\right)\right |\end{align}</script><p>其中，$det\left(\frac{\partial z}{\partial x}\right)$是雅克比矩阵行列式，用于保证在换元过程中概率密度函数是归一的。由此，对数似然可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align}     &\sum_{x \in \mathbf{X}} \log p_{model}(x) = \sum_{x \in \mathbf{X}} \log p_{model}(z) + \sum_{x \in \mathbf{X}} log\left | det\left(\frac{\partial z}{\partial x}\right)\right |\end{align}</script><p>对于第一项，一个k维的多元标准正态分布的概率密度函数为:</p>
<script type="math/tex; mode=display">
\begin{align}    p_{model}(z)=\frac{1}{(2 \pi)^{\frac{k}{2}}} \exp \left(-\frac{1}{2} z^T z\right)\end{align}</script><p>假设输入样本数量为N，公式$(20)$的第一项即可表示为：</p>
<script type="math/tex; mode=display">
\begin{align}    \sum_{x \in \mathbf{X}} \log p_{model}(z) = \sum_{x \in \mathbf{X}}\left(-\frac{k}{2} \log (2 \pi)-\frac{1}{2} z_i^T z_i\right) = -\frac{N k}{2} \log (2 \pi)-\frac{1}{2} \sum_{i=1}^n z_i^T z_i\end{align}</script><p>对于固定的数据表示方式和数据集，公式$(22)$的第一项为常数，不会影响梯度下降过程。第二项即为潜在向量的平方和，其计算复杂度仅为$O(kN)$，十分便于计算。公式$(20)$的第二项是雅可比矩阵的行列式的对数和，单个$k\cdot k$的矩阵的行列式运算复杂度已经达到了$O(k^3)$。<br>这意味着常规情况下我们不可能把一个需要计算雅可比矩阵行列式的函数作为损失函数。因此，流模型的主要设计目标是通过网络结构的设计，保证雅可比矩阵的可计算性。另一个设计重点是函数$z = f(x)$的可逆性。如果只能实现从样本空间到潜在空间的映射，即使计算似然度是成功的，模型也并不具备生成能力，这将背离生成式神经网络的初衷。</p>
<p><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_11%2Fimage.png" alt=""></p>
<p>在原理上，流模型通过构建可逆且雅可比行列式可计算的子层来保证网络整体的两个相应性质。雅可比行列式可计算性一般借助对角矩阵的行列式即为其对角线乘积的特点和行列式本身的乘法性质。假设$f_i$是组成流模型的子层，$g_i$是相应的逆函数，整体网络可以被表示为$f = f_n \circ \ldots \circ f_2 \circ f_1, \ g = g_n \circ \ldots \circ g_2 \circ g_1$，如图所示。其中，蓝色框代表节点，$\mathbf{X}$为样本空间中的数据，$\mathbf{Z}$为潜在空间中的数据。数据就像流体一样在模型中流动。此时行列式可以用以下公式表示：</p>
<script type="math/tex; mode=display">
\begin{align}      det\left(\frac{\partial z}{\partial x}\right) =  det\left(\frac{\partial f(x)}{\partial x}\right) = det\left(\frac{\partial ( f_n \circ \ldots \circ f_2 \circ f_1(x))}{\partial x}\right)\end{align}</script><p>根据链式法则，可以将公式(23)展开为：</p>
<script type="math/tex; mode=display">
\begin{align}    det\left(\frac{\partial z}{\partial x}\right) =det\left(\frac{\partial \left( f_n \circ \ldots \circ f_2 \circ f_1(x)\right)}{\partial \left(f_{n-1} \circ \ldots \circ f_2 \circ f_1(x)\right)}\cdot\ldots\cdot\frac{\partial \left(f_2 \circ f_1(x)\right)}{\partial \left(f_1(x)\right)}\cdot\frac{\partial f_1(x)}{\partial x}\right)\end{align}</script><p>根据行列式乘法定义，进一步展开公式(24)：</p>
<script type="math/tex; mode=display">
\begin{align}    det\left(\frac{\partial z}{\partial x}\right) =det\left(\frac{\partial \left( f_n \circ \ldots \circ f_2 \circ f_1(x)\right)}{\partial \left(f_{n-1} \circ \ldots \circ f_2 \circ f_1(x)\right)}\right)\cdot\ldots\cdot det\left(\frac{\partial \left(f_2 \circ f_1(x)\right)}{\partial \left(f_1(x)\right)}\right)\cdot det\left(\frac{\partial f_1(x)}{\partial x}\right)\end{align}</script><p>如果每个子层的雅可比矩阵都为对角阵，则(25)是易于计算的。因此，子层的雅可比矩阵行列式可计算性和可逆性，是流模型具备相应两个性质和易解显性建模可行的充分条件。最终，通过对模型的设计，流模型在不做任何近似的情况下，可以使用一种具备可接受时间复杂度的方式，表述原本难以计算的似然函数。</p>
<hr>
<p><strong>流模型的基本实现原理</strong></p>
<p>流模型的实现依赖于具备可逆性和雅可比行列式可计算性的子层的实现。然而，在实践中，寻找一种同时具备可逆性和雅可比矩阵可计算性的神经网络子层同样是十分具有挑战性的。在流模型中，经典的解决方法是使用耦合层(coupling layer)。</p>
<p><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_11%2Fimage_1.png" alt=""></p>
<p>图中是一种经典的耦合层的正向变换方式，图中包含两个耦合层。其中$x<em>i$代表数据，$s$和$t$分别为耦合层神经网络计算给出的缩放因子和平移因子。在使用耦合层处理数据时，首先会将数据分为两个部分，在经过一次耦合层时，第一部分数据$x</em>{i}[1:k/2]$进入耦合层神经网络，并通过计算给出缩放因子$s$和平移因子$t$。然而，与一般的神经网络不同的是，这并不意味着第一部分数据发生了任何的改变。应用神经网络的目的是为了得到指引第二部分数据$x_{i}[k/2+1:k]$变换的因子：</p>
<script type="math/tex; mode=display">
\begin{align}       \left\{        \begin{aligned}            &x_{i+1}[1:k/2] = x_{i}[1:k/2]\\            &x_{i+1}[k/2+1:k] = x_{i}[k/2+1:k] \odot \exp(s(x_{i}[1:k/2])) + t(x_{i}[1:k/2])        \end{aligned}    \right.\end{align}</script><p>其中，$\odot$为元素级别的乘法。为了避免指数项$\exp(s(x<em>{i}[1:k/2]))$过大而破坏模型稳定性，将$s$限定在$[-1,1]$之间是必要的。根据公式(26)，发现$x</em>{i+1}[1:k/2]$与$x_{i+1}[k/2+1:k]$完全无关。这意味着雅可比矩阵是下三角矩阵(或上三角矩阵，如果保持不变的是后半部分)。计算这一层内的雅可比矩阵行列式：</p>
<script type="math/tex; mode=display">
\begin{align}       det\left(\frac{\partial x_{i+1}}{\partial x_{i}}\right) = \left|\begin{array}{ccc}        \frac{\partial x_{i+1}[1]}{\partial x_{i}[1]} & \cdots & \frac{\partial x_{i+1}[1]}{\partial x_{i}[k]} \\        \ddots & \vdots & \\        \frac{\partial x_{i+1}[k]}{\partial x_{i}[1]} & \cdots & \frac{\partial x_{i+1}[k]}{\partial x_{i+1}[k]}        \end{array}\right|         = \left|\begin{array}{cc}            \mathbf{I} & 0 \\            \frac{\partial x_{i+1}[k/2+1: k]}{\partial x_{i}[1: k/2]} & \operatorname{diag}\left(\exp \left[s\left(x_{i}[1: k/2]\right)\right]\right)            \end{array}\right|\end{align}</script><p>三角矩阵的行列式即为对角线乘积：</p>
<script type="math/tex; mode=display">
\begin{align}    det\left(\frac{\partial x_{i+1}}{\partial x_{i}}\right) = \exp(\left[\sum s\left(x_{i}[1: k/2]\right)\right])\end{align}</script><p><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_11%2Fimage_2.png" alt=""></p>
<p>因此，耦合层的雅可比矩阵行列式是易于计算的，其时间复杂度为$O(k)$。然后关注另一个必要的性质，可逆性。首先，耦合层的输入和输出是等维度的，这保证了可逆性的必要条件。然后，耦合层的每一次变换都保证了有一部分数据没有发生改变，且整体变换仅依赖于这部分不变的数据。这使得逆函数非常容易设计，如图所示。</p>
<script type="math/tex; mode=display">
\begin{align}     \left\{        \begin{aligned}            &x_{i}[1:k/2] = x_{i+1}[1:k/2]\\            &x_{i}[k/2+1:k] = x_{i+1}[k/2+1:k] \div \exp(s(x_{i+1}[1:k/2])) - t(x_{i+1}[1:k/2])        \end{aligned}    \right.\end{align}</script><p>在单层可逆且雅可比矩阵行列式可计算的基础上，根据公式(25)的原理堆叠多个耦合层，即可实现流模型。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn">Raphael Hyaan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn/2025/02/26/informatique/deeplearning/Generative-11/">http://raphaelhyaan.cn/2025/02/26/informatique/deeplearning/Generative-11/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://raphaelhyaan.cn" target="_blank">Raphael's Home</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/04/21/eles-1/" title="Amplification 放大器"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F入云.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Amplification 放大器</div></div></a></div><div class="next-post pull-right"><a href="/2025/02/11/informatique/java/java-5/" title="J05 同步"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fanon cat 006.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">J05 同步</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/20/informatique/deeplearning/Generative_1/" title="Chapter 1 Generative Modeling"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 1 Generative Modeling</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_10/" title="Chapter 10 Advanced GANs 各种各样的GAN"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 10 Advanced GANs 各种各样的GAN</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_5/" title="Chapter 5 Autoregressive Models 自回归模型"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 5 Autoregressive Models 自回归模型</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_4/" title="Chapter 4 Generative Adversarial Networks 生成对抗网络"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 4 Generative Adversarial Networks 生成对抗网络</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_3/" title="Chapter 3 Variational Autoencoders 自动变分编码器"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 3 Variational Autoencoders 自动变分编码器</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_2/" title="Chapter 2 Deep Learning"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 2 Deep Learning</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-2.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="author-info__name">Raphael Hyaan</div><div class="author-info__description">何日可谓归去来</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">187</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/raphaelhyaan"><i class="fab fa-github"></i><span>Bonjour</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/raphaelhyaan" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:raphael.ma.yuhan@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">仰观宇宙之大，俯察品类之盛</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#GAN-VAE%E5%92%8C%E6%B5%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%88%E4%BB%A5%E6%B5%81%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%B8%BB%EF%BC%89"><span class="toc-text">GAN,VAE和流模型的原理（以流模型为主）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">生成式神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GAN-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C"><span class="toc-text">GAN 生成对抗网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VAE-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-text">VAE 变分自动编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%81%E6%A8%A1%E5%9E%8B"><span class="toc-text">流模型</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/01/22/esl-1/" title="无监督学习和流形学习 Unsupervised Learning &amp; Manifold Learning"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Flf-01.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="无监督学习和流形学习 Unsupervised Learning &amp; Manifold Learning"/></a><div class="content"><a class="title" href="/2026/01/22/esl-1/" title="无监督学习和流形学习 Unsupervised Learning &amp; Manifold Learning">无监督学习和流形学习 Unsupervised Learning &amp; Manifold Learning</a><time datetime="2026-01-22T06:00:17.000Z" title="发表于 2026-01-22 14:00:17">2026-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-5/" title="第五部分-应对不确定性与大规模问题"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第五部分-应对不确定性与大规模问题"/></a><div class="content"><a class="title" href="/2026/01/06/algo-5/" title="第五部分-应对不确定性与大规模问题">第五部分-应对不确定性与大规模问题</a><time datetime="2026-01-05T16:07:15.000Z" title="发表于 2026-01-06 00:07:15">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-4/" title="第四部分-计算复杂性与近似解"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第四部分-计算复杂性与近似解"/></a><div class="content"><a class="title" href="/2026/01/06/algo-4/" title="第四部分-计算复杂性与近似解">第四部分-计算复杂性与近似解</a><time datetime="2026-01-05T16:07:14.000Z" title="发表于 2026-01-06 00:07:14">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-3/" title="第三部分-精确最优化策略"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第三部分-精确最优化策略"/></a><div class="content"><a class="title" href="/2026/01/06/algo-3/" title="第三部分-精确最优化策略">第三部分-精确最优化策略</a><time datetime="2026-01-05T16:07:12.000Z" title="发表于 2026-01-06 00:07:12">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-2/" title="第二部分-基于规模的策略：分解与变换 (Structure Decomposition)"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第二部分-基于规模的策略：分解与变换 (Structure Decomposition)"/></a><div class="content"><a class="title" href="/2026/01/06/algo-2/" title="第二部分-基于规模的策略：分解与变换 (Structure Decomposition)">第二部分-基于规模的策略：分解与变换 (Structure Decomposition)</a><time datetime="2026-01-05T16:07:11.000Z" title="发表于 2026-01-06 00:07:11">2026-01-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By Raphael Hyaan</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="framework-info"><span>备案号: </span><a href="href=&quot;https://beian.miit.gov.cn/&quot; ">京ICP备2024051904号</a><span class="footer-separator">|</span><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2F%E5%A4%87%E6%A1%88%E5%9B%BE%E6%A0%87.png" alt="MIT License" height="20" align="top"/><span> </span><a href="href=&quot;https://beian.mps.gov.cn/#/query/webSearch?code=11010802044068&quot; ">京公网安备11010802044068号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>