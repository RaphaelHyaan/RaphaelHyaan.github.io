<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>GAN,VAE和流模型的原理（以流模型为主） | Raphael's Home</title><meta name="author" content="Raphael Hyaan"><meta name="copyright" content="Raphael Hyaan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="GAN,VAE和流模型的原理（以流模型为主）  2025&#x2F;02&#x2F;26 以便自己查看  生成式神经网络  生成式神经网络是一种以生成与被提供的数据集相似的新数据为目的的人工神经网络。它试图构建一种模型分布\(p_{model}\)，使其尽可能接近原始数据分布\(p_{data}\)。由于不依赖其他标签数据，生成式模型往往是无监督学习。 特别的，生成式模型也可以预测在\(y\)条件下\(">
<meta property="og:type" content="article">
<meta property="og:title" content="GAN,VAE和流模型的原理（以流模型为主）">
<meta property="og:url" content="http://raphaelhyaan.cn/2025/02/26/informatique/deeplearning/Generative-11/index.html">
<meta property="og:site_name" content="Raphael&#39;s Home">
<meta property="og:description" content="GAN,VAE和流模型的原理（以流模型为主）  2025&#x2F;02&#x2F;26 以便自己查看  生成式神经网络  生成式神经网络是一种以生成与被提供的数据集相似的新数据为目的的人工神经网络。它试图构建一种模型分布\(p_{model}\)，使其尽可能接近原始数据分布\(p_{data}\)。由于不依赖其他标签数据，生成式模型往往是无监督学习。 特别的，生成式模型也可以预测在\(y\)条件下\(">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F%E8%9D%B61.png">
<meta property="article:published_time" content="2025-02-26T08:30:58.000Z">
<meta property="article:modified_time" content="2025-02-26T08:31:49.774Z">
<meta property="article:author" content="Raphael Hyaan">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F%E8%9D%B61.png"><link rel="shortcut icon" href="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Flana cat 002.png"><link rel="canonical" href="http://raphaelhyaan.cn/2025/02/26/informatique/deeplearning/Generative-11/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GAN,VAE和流模型的原理（以流模型为主）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-02-26 16:31:49'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/butterflyChange/css/code.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-2.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">173</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">53</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/Reading/"><i class="fa-fw fas fa-book"></i><span> Reading</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F蝶1.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Raphael's Home"><span class="site-name">Raphael's Home</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/Reading/"><i class="fa-fw fas fa-book"></i><span> Reading</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">GAN,VAE和流模型的原理（以流模型为主）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-26T08:30:58.000Z" title="发表于 2025-02-26 16:30:58">2025-02-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-26T08:31:49.774Z" title="更新于 2025-02-26 16:31:49">2025-02-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%94%9F%E6%88%90%E5%BC%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">学习笔记-生成式神经网络</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="GAN,VAE和流模型的原理（以流模型为主）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1
id="ganvae和流模型的原理以流模型为主">GAN,VAE和流模型的原理（以流模型为主）</h1>
<blockquote>
<p>2025/02/26 以便自己查看</p>
</blockquote>
<h3 id="生成式神经网络">生成式神经网络</h3>
<hr />
<p>生成式神经网络是一种以生成与被提供的数据集相似的新数据为目的的人工神经网络。它试图构建一种模型分布<span
class="math inline">\(p_{model}\)</span>，使其尽可能接近原始数据分布<span
class="math inline">\(p_{data}\)</span>。由于不依赖其他标签数据，生成式模型往往是无监督学习。
特别的，生成式模型也可以预测在<span
class="math inline">\(y\)</span>条件下<span
class="math inline">\(x\)</span>的分布<span
class="math inline">\(P(x|y)\)</span>，比如要求一个用于生成水果图像的模型生成一个苹果：<span
class="math inline">\(P(\text{水果}|\text{苹果})\)</span>。</p>
<p>对于数据的集合<span
class="math inline">\(\mathbf{X}\)</span>，生成式神经网络理想的损失函数是模型分布和数据分布之间的似然函数：</p>
<p><span class="math display">\[
\begin{align}    \mathcal{L}(model(\theta) \mid \mathbf{X})= \prod_{x
\in \mathbf{X}} p_{model(\theta)}(x)\end{align}
\]</span></p>
<p>似然函数是模型可调节参数<span
class="math inline">\(\theta\)</span>的函数，用于衡量模型分布对数据分布的拟合程度，所以最大化似然函数是模型训练的目的。
在实际训练中，我们不希望计算大量0到1之间的数值的乘积，并且我们希望最小化损失函数，因此一般使用负对数似然函数作为损失函数。理想的模型参数<span
class="math inline">\(\hat
\theta\)</span>即为负对数似然函数取最小值时的模型参数：</p>
<p><span class="math display">\[
\begin{align}    &amp;loss=-\sum_{x \in \mathbf{X}} \log
p_{model(\theta)}(x)\\    &amp;\hat \theta = argmin(-\sum_{x \in
\mathbf{X}} \log p_{model(\theta)}(x))\end{align}
\]</span></p>
<p>然而，对于高维问题，直接计算概率<span
class="math inline">\(p_{model(\theta)}(x)\)</span>是不可行的。对此，有三种主流的解决方法：隐性建模，近似显性建模和易解显性建模。</p>
<ul>
<li>隐性建模不关注于分布的估计，这些模型专注于数据的生成，并使用其他方法保证生成的数据与真实数据分布接近。</li>
<li>近似显性建模尝试使用近似方式估测模型分布和数据分布间的差异。</li>
<li>易解显性建模直接定义了方便求解的似然分布，并且直接应用似然函数优化模型。</li>
</ul>
<h3 id="gan-生成对抗网络">GAN 生成对抗网络</h3>
<hr />
<p>对抗生成网络(GAN)包含两个子网络，分别称为生成器(G)和判别器(D)。前者从某一简单分布<span
class="math inline">\(p_Z(z)\)</span>出发，构建模型分布<span
class="math inline">\(p_{G}(x)\)</span>。后者分辨数据来源自生成还是属于真实数据，并将由生成器生成的数据标记为0，真实数据标记为1。</p>
<p>具体来说，GAN并没有使用最大似然函数法进行模型训练，而是重新定义了一个最小最大问题，即最小化生成器的欺骗判别器的能力，并最大化判别器的辨识由生成器生成的数据的能力。</p>
<p><span class="math display">\[
\begin{align}    &amp; V(G,D) =  \mathbb{E}_{x \sim p_{data }(x)}[\log
D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]  \\    &amp; G^*,
D^* = \underset{G}{\operatorname{argmax}}  \
\underset{D}{\operatorname{argmax}} V(G,D)\end{align}
\]</span></p>
<p>在细节方面，为了训练模型，需评价模型分布<span
class="math inline">\(p_{G}(x)\)</span>和数据分布<span
class="math inline">\(p_{data}(x)\)</span>的相似性。在GAN中，辨别器扮演这一角色。我们将重点关注这种方法的理论依据。假设生成器参数固定，展开V(G,D)得：</p>
<p><span class="math display">\[
\begin{align}     V(G,D)&amp;=\int_{x} \left[p_{data }(x) \log
(D(x))+p_{G}(x) \log (1-D(x))\right] d x\end{align}
\]</span></p>
<p>设D在这种情况下的的最佳参数取值<span
class="math inline">\(D^*\)</span>为：</p>
<p><span class="math display">\[
\begin{align}    D^* &amp;=  \underset{D}{\operatorname{argmax}}
\int_{x} \left[p_{data }(x) \log (D(x))+p_{G}(x) \log (1-D(x))\right] d
x\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(p_{data }(x) \log (D(x))+p_{G}(x)
\log (1-D(x))\)</span>在<span
class="math inline">\([0,1]\)</span>之间有最大值。求导以求出最大时对应的<span
class="math inline">\(D(x)\)</span>的表达式，可得在生成器网络参数固定时，对于每一个<span
class="math inline">\(x\)</span>，理想的判别器<span
class="math inline">\(D^*\)</span>为：</p>
<p><span class="math display">\[
\begin{align}    D^*(x) =
\frac{p_{data}(x)}{p_{data}(x)+p_{G}(x)}\end{align}
\]</span></p>
<p>将这一表示带回公式<span class="math inline">\((4)\)</span>:</p>
<p><span class="math display">\[
\begin{align}    V(G,D^*)&amp;=\mathbb{E}_{x \sim p_{data }(x)}[\log
\frac{p_{data}(x)}{p_{data}(x)+p_{G}(x)}]\nonumber\\    &amp;+\mathbb{E}_{z
\sim p_{G}(x)}[\log
\frac{p_{G}(x)}{p_{data}(x)+p_{G}(x)}]    \end{align}
\]</span></p>
<p>为了进一步理解公式，引入KL散度和JS散度。它们都是衡量两个概率分布之间的相似性的指标。</p>
<p><span class="math display">\[
\begin{align}        &amp;D_{K L}(p || q)=\mathbb{E}[\log \frac
{p\left(x_i\right)}{q\left(x_i\right)}]\\    &amp;D_{J S}(p ||
q)=\frac{1}{2} D_{K L}(p || \frac{p+q}{2})+\frac{1}{2} D_{K L}(q ||
\frac{p+q}{2})\end{align}
\]</span></p>
<p>使用两种散度表示公式<span
class="math inline">\((9)\)</span>，可得：</p>
<p><span class="math display">\[
\begin{align}    V(G,D^*)&amp;=-\log(4)+2\cdot
D_{JS}(p_{data}||p_{G})\end{align}
\]</span></p>
<p>由此可知，在理想情况下，GAN期望训练判别器网络，使得其近似于数据分布和模型分布的JS散度，从而将其作为衡量两个分布相似性的指标。然后使用由判别器网络近似的JS散度作为损失函数，并将其最小化获得最优的生成器模型。</p>
<p>实际情况下，在GAN的训练中，生成器和判别器的训练是交替进行的，因此为了达到上述效果，只能期望在一次将判别器训练至最优。但这是十分难以实现的。实际情况下损失函数是通过神经网络方法对JS散度的隐性近似。</p>
<h3 id="vae-变分自动编码器">VAE 变分自动编码器</h3>
<hr />
<p>变分自动编码器(VAE)是贝叶斯概率在深度学习领域中最广为人知的应用之一。VAE包含编码器(E)和解码器(D)两个网络。在生成时，从某先验分布<span
class="math inline">\(p_Z(z)\)</span>中采样潜在空间向量<span
class="math inline">\(z\)</span>，并使用解码器解码得到<span
class="math inline">\(x\)</span>。
由于似然度难以直接计算，VAE使用变分方法和KL散度确定了似然度的下界，然后对下界进行进一步的优化</p>
<p>具体来说，解码器代表给定潜在空间向量z时，变量x的条件分布<span
class="math inline">\(p_{model_D}(x|z)\)</span>，根据全概率公式和贝叶斯定理，可以将模型分布<span
class="math inline">\(p_{D}(x)\)</span>表示为潜在空间向量的先验分布和条件分布的乘积的积分：</p>
<p><span class="math display">\[
\begin{align}    p_{D}(x)=\int p_{D}(x|z)p_{Z}(z)dz\end{align}
\]</span></p>
<p>在这个公式中，<span
class="math inline">\(p_{D}(x)\)</span>与前文的<span
class="math inline">\(p_{model(\theta)}(x)\)</span>等价，代表VAE模型中的模型分布。此时，负对数似然可以表示为：</p>
<p><span class="math display">\[
\begin{align}
    &amp;loss=-\sum_{x \in \mathbf{X}} \log p_{D}(x) = -\sum_{x \in
\mathbf{X}} \log \int p_{D}(x|z)p_{Z}(z)dz
\end{align}
\]</span></p>
<p>然而，这个高维积分有着很高的计算复杂度，是难以计算的。一个可行的简化思路为缩小连续随机变量z的取值范围，将积分问题转化为求和问题。在这个过程中，为了不影响生成的精度，尽可能选取最能生成可靠数据的潜在空间向量是至关重要的。为此，VAE不再使用z的先验分布，而是使用与真实数据相联系的后验概率分布。</p>
<p><span class="math display">\[
\begin{align}
    p_{D}(z|x) = \frac{p_{D}(x|z)\cdot p_{Z}(z)} {p_{D}(x)}
\end{align}
\]</span></p>
<p>由于<span class="math inline">\(p_{D}(x|z)\)</span>和<span
class="math inline">\(p_{D}(x)\)</span>难以表述，后验概率的直接计算同样是不可能的。为此，需要引入编码器，构成另一个分布<span
class="math inline">\(q_E(z|x)\)</span>去近似后验概率分布，并同时引入KL散度作为两个分布相似程度的评价标准。</p>
<p>$$ <span class="math display">\[\begin{align}
    D_{KL}(q_E(z|x)||p_{D}(x|z)) &amp;= \mathbb{E}_{z\sim
q_E(z|x)}\left[\log\left(\frac{q_E(z|x)}{p_{D}(x|z)}\right)\right]
     = \mathbb{E}_{z\sim q_E(z|x)}\left[\log\frac{q_E(z|x)\cdot
p_{D}(x)}{p_{Z}(z) \cdot p_{D}(x|z)}\right]\nonumber\\
    &amp; = D_{KL}(q_E(z|x)||p_{Z}(z))- \mathbb{E}_{z\sim
q_E(z|x)}\left[p_{D}(x|z)\right]+ \log p_{D}(x)
    
\end{align}\]</span> $$</p>
<p>其中，<span class="math inline">\(\log
p_{D}(x)\)</span>是我们需要计算最小负对数似然的部分，整理公式得：</p>
<p><span class="math display">\[
\begin{align}
    \log p_{D}(x) = D_{KL}(q_E(z|x)||p_{D}(x|z)) + \mathbb{E}_{z\sim
q_E(z|x)}\left[p_{D}(x|z)\right]-D_{KL}(q_E(z|x)||p_{Z}(z))
\end{align}
\]</span></p>
<p>尽管<span
class="math inline">\(D_{KL}(q_E(z|x)||p_{D}(x|z))\)</span>依然无法运算，但其恒为正数，仅在<span
class="math inline">\(q_E(z|x)\)</span>和<span
class="math inline">\(p_{D}(x|z)\)</span>完全相同时取0，因此<span
class="math inline">\(\log
p_{D}(x)\)</span>的下界可以确认。这个下界被称为变分下界，是VAE的核心思想。</p>
<p><span class="math display">\[
\begin{align}
    \log p_{D}(x) \ge \mathbb{E}_{z\sim
q_E(z|x)}\left[p_{D}(x|z)\right]-D_{KL}(q_E(z|x)||p_{Z}(z))
\end{align}
\]</span></p>
<p>其中，第一项衡量输入向量x和将其编码为z，再解码得到的新数据<span
class="math inline">\(\hat
x\)</span>的差异，称为重构误差。第二项衡量编码器和先验分布之间的差异。通过变分下界，VAE成功的取得了似然函数的近似值，使得模型训练成为可能。</p>
<h3 id="流模型">流模型</h3>
<hr />
<p><strong>流模型的可逆性和雅可比行列式可计算性</strong></p>
<p>易解显性建模的关键在于似然分布的计算。流模型通过本身具备的可逆性和雅可比矩阵行列式可计算性，将难以计算似然的分布转化为如正态分布这般易于计算的分布</p>
<p>具体来说，流模型定义了一组可逆函数<span class="math inline">\(x=g(z)
\Leftrightarrow z=f(x)\)</span>可逆将输入向量<span
class="math inline">\(x\)</span>从样本空间映射到潜在空间，并假定潜在空间向量<span
class="math inline">\(p_{model}(z)\)</span>为多元标准正态分布。根据双射的性质，<span
class="math inline">\(x\)</span>和<span
class="math inline">\(z\)</span>必须是等维度的。根据概率密度的换元公式，有：</p>
<p><span class="math display">\[
\begin{align}    &amp;p_{model}(x) = p_{model}(z)\left |
det\left(\frac{\partial z}{\partial x}\right)\right |\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(det\left(\frac{\partial z}{\partial
x}\right)\)</span>是雅克比矩阵行列式，用于保证在换元过程中概率密度函数是归一的。由此，对数似然可以表示为：</p>
<p><span class="math display">\[
\begin{align}     &amp;\sum_{x \in \mathbf{X}} \log p_{model}(x) =
\sum_{x \in \mathbf{X}} \log p_{model}(z) + \sum_{x \in \mathbf{X}}
log\left | det\left(\frac{\partial z}{\partial x}\right)\right
|\end{align}
\]</span></p>
<p>对于第一项，一个k维的多元标准正态分布的概率密度函数为:</p>
<p><span class="math display">\[
\begin{align}    p_{model}(z)=\frac{1}{(2 \pi)^{\frac{k}{2}}} \exp
\left(-\frac{1}{2} z^T z\right)\end{align}
\]</span></p>
<p>假设输入样本数量为N，公式<span
class="math inline">\((20)\)</span>的第一项即可表示为：</p>
<p><span class="math display">\[
\begin{align}    \sum_{x \in \mathbf{X}} \log p_{model}(z) = \sum_{x \in
\mathbf{X}}\left(-\frac{k}{2} \log (2 \pi)-\frac{1}{2} z_i^T z_i\right)
= -\frac{N k}{2} \log (2 \pi)-\frac{1}{2} \sum_{i=1}^n z_i^T
z_i\end{align}
\]</span></p>
<p>对于固定的数据表示方式和数据集，公式<span
class="math inline">\((22)\)</span>的第一项为常数，不会影响梯度下降过程。第二项即为潜在向量的平方和，其计算复杂度仅为<span
class="math inline">\(O(kN)\)</span>，十分便于计算。公式<span
class="math inline">\((20)\)</span>的第二项是雅可比矩阵的行列式的对数和，单个<span
class="math inline">\(k\cdot
k\)</span>的矩阵的行列式运算复杂度已经达到了<span
class="math inline">\(O(k^3)\)</span>。
这意味着常规情况下我们不可能把一个需要计算雅可比矩阵行列式的函数作为损失函数。因此，流模型的主要设计目标是通过网络结构的设计，保证雅可比矩阵的可计算性。另一个设计重点是函数<span
class="math inline">\(z =
f(x)\)</span>的可逆性。如果只能实现从样本空间到潜在空间的映射，即使计算似然度是成功的，模型也并不具备生成能力，这将背离生成式神经网络的初衷。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_11%2Fimage.png" /></p>
<p>在原理上，流模型通过构建可逆且雅可比行列式可计算的子层来保证网络整体的两个相应性质。雅可比行列式可计算性一般借助对角矩阵的行列式即为其对角线乘积的特点和行列式本身的乘法性质。假设<span
class="math inline">\(f_i\)</span>是组成流模型的子层，<span
class="math inline">\(g_i\)</span>是相应的逆函数，整体网络可以被表示为<span
class="math inline">\(f = f_n \circ \ldots \circ f_2 \circ f_1, \ g =
g_n \circ \ldots \circ g_2 \circ
g_1\)</span>，如图所示。其中，蓝色框代表节点，<span
class="math inline">\(\mathbf{X}\)</span>为样本空间中的数据，<span
class="math inline">\(\mathbf{Z}\)</span>为潜在空间中的数据。数据就像流体一样在模型中流动。此时行列式可以用以下公式表示：</p>
<p><span class="math display">\[
\begin{align}      det\left(\frac{\partial z}{\partial x}\right)
=  det\left(\frac{\partial f(x)}{\partial x}\right) =
det\left(\frac{\partial ( f_n \circ \ldots \circ f_2 \circ
f_1(x))}{\partial x}\right)\end{align}
\]</span></p>
<p>根据链式法则，可以将公式(23)展开为：</p>
<p><span class="math display">\[
\begin{align}    det\left(\frac{\partial z}{\partial x}\right)
=det\left(\frac{\partial \left( f_n \circ \ldots \circ f_2 \circ
f_1(x)\right)}{\partial \left(f_{n-1} \circ \ldots \circ f_2 \circ
f_1(x)\right)}\cdot\ldots\cdot\frac{\partial \left(f_2 \circ
f_1(x)\right)}{\partial \left(f_1(x)\right)}\cdot\frac{\partial
f_1(x)}{\partial x}\right)\end{align}
\]</span></p>
<p>根据行列式乘法定义，进一步展开公式(24)：</p>
<p><span class="math display">\[
\begin{align}    det\left(\frac{\partial z}{\partial x}\right)
=det\left(\frac{\partial \left( f_n \circ \ldots \circ f_2 \circ
f_1(x)\right)}{\partial \left(f_{n-1} \circ \ldots \circ f_2 \circ
f_1(x)\right)}\right)\cdot\ldots\cdot det\left(\frac{\partial \left(f_2
\circ f_1(x)\right)}{\partial \left(f_1(x)\right)}\right)\cdot
det\left(\frac{\partial f_1(x)}{\partial x}\right)\end{align}
\]</span></p>
<p>如果每个子层的雅可比矩阵都为对角阵，则(25)是易于计算的。因此，子层的雅可比矩阵行列式可计算性和可逆性，是流模型具备相应两个性质和易解显性建模可行的充分条件。最终，通过对模型的设计，流模型在不做任何近似的情况下，可以使用一种具备可接受时间复杂度的方式，表述原本难以计算的似然函数。</p>
<hr />
<p><strong>流模型的基本实现原理</strong></p>
<p>流模型的实现依赖于具备可逆性和雅可比行列式可计算性的子层的实现。然而，在实践中，寻找一种同时具备可逆性和雅可比矩阵可计算性的神经网络子层同样是十分具有挑战性的。在流模型中，经典的解决方法是使用耦合层(coupling
layer)。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_11%2Fimage_1.png" /></p>
<p>图中是一种经典的耦合层的正向变换方式，图中包含两个耦合层。其中<span
class="math inline">\(x_i\)</span>代表数据，<span
class="math inline">\(s\)</span>和<span
class="math inline">\(t\)</span>分别为耦合层神经网络计算给出的缩放因子和平移因子。在使用耦合层处理数据时，首先会将数据分为两个部分，在经过一次耦合层时，第一部分数据<span
class="math inline">\(x_{i}[1:k/2]\)</span>进入耦合层神经网络，并通过计算给出缩放因子<span
class="math inline">\(s\)</span>和平移因子<span
class="math inline">\(t\)</span>。然而，与一般的神经网络不同的是，这并不意味着第一部分数据发生了任何的改变。应用神经网络的目的是为了得到指引第二部分数据<span
class="math inline">\(x_{i}[k/2+1:k]\)</span>变换的因子：</p>
<p><span class="math display">\[
\begin{align}       \left\{        \begin{aligned}            &amp;x_{i+1}[1:k/2]
= x_{i}[1:k/2]\\            &amp;x_{i+1}[k/2+1:k] = x_{i}[k/2+1:k] \odot
\exp(s(x_{i}[1:k/2])) +
t(x_{i}[1:k/2])        \end{aligned}    \right.\end{align}
\]</span></p>
<p>其中，<span
class="math inline">\(\odot\)</span>为元素级别的乘法。为了避免指数项<span
class="math inline">\(\exp(s(x_{i}[1:k/2]))\)</span>过大而破坏模型稳定性，将<span
class="math inline">\(s\)</span>限定在<span
class="math inline">\([-1,1]\)</span>之间是必要的。根据公式(26)，发现<span
class="math inline">\(x_{i+1}[1:k/2]\)</span>与<span
class="math inline">\(x_{i+1}[k/2+1:k]\)</span>完全无关。这意味着雅可比矩阵是下三角矩阵(或上三角矩阵，如果保持不变的是后半部分)。计算这一层内的雅可比矩阵行列式：</p>
<p><span class="math display">\[
\begin{align}       det\left(\frac{\partial x_{i+1}}{\partial
x_{i}}\right) = \left|\begin{array}{ccc}        \frac{\partial
x_{i+1}[1]}{\partial x_{i}[1]} &amp; \cdots &amp; \frac{\partial
x_{i+1}[1]}{\partial x_{i}[k]} \\        \ddots &amp; \vdots &amp;
\\        \frac{\partial x_{i+1}[k]}{\partial x_{i}[1]} &amp; \cdots
&amp; \frac{\partial x_{i+1}[k]}{\partial
x_{i+1}[k]}        \end{array}\right|         =
\left|\begin{array}{cc}            \mathbf{I} &amp; 0
\\            \frac{\partial x_{i+1}[k/2+1: k]}{\partial x_{i}[1: k/2]}
&amp; \operatorname{diag}\left(\exp \left[s\left(x_{i}[1:
k/2]\right)\right]\right)            \end{array}\right|\end{align}
\]</span></p>
<p>三角矩阵的行列式即为对角线乘积：</p>
<p><span class="math display">\[
\begin{align}    det\left(\frac{\partial x_{i+1}}{\partial x_{i}}\right)
= \exp(\left[\sum s\left(x_{i}[1: k/2]\right)\right])\end{align}
\]</span></p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_11%2Fimage_2.png" /></p>
<p>因此，耦合层的雅可比矩阵行列式是易于计算的，其时间复杂度为<span
class="math inline">\(O(k)\)</span>。然后关注另一个必要的性质，可逆性。首先，耦合层的输入和输出是等维度的，这保证了可逆性的必要条件。然后，耦合层的每一次变换都保证了有一部分数据没有发生改变，且整体变换仅依赖于这部分不变的数据。这使得逆函数非常容易设计，如图所示。</p>
<p><span class="math display">\[
\begin{align}     \left\{        \begin{aligned}            &amp;x_{i}[1:k/2]
= x_{i+1}[1:k/2]\\            &amp;x_{i}[k/2+1:k] = x_{i+1}[k/2+1:k]
\div \exp(s(x_{i+1}[1:k/2])) -
t(x_{i+1}[1:k/2])        \end{aligned}    \right.\end{align}
\]</span></p>
<p>在单层可逆且雅可比矩阵行列式可计算的基础上，根据公式(25)的原理堆叠多个耦合层，即可实现流模型。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn">Raphael Hyaan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn/2025/02/26/informatique/deeplearning/Generative-11/">http://raphaelhyaan.cn/2025/02/26/informatique/deeplearning/Generative-11/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://raphaelhyaan.cn" target="_blank">Raphael's Home</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/04/21/eles-1/" title="Amplification 放大器"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F入云.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Amplification 放大器</div></div></a></div><div class="next-post pull-right"><a href="/2025/02/11/informatique/java/java-5/" title="J05 同步"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fanon cat 006.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">J05 同步</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/20/informatique/deeplearning/Generative_2/" title="Chapter 2 Deep Learning"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 2 Deep Learning</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_10/" title="Chapter 10 Advanced GANs 各种各样的GAN"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 10 Advanced GANs 各种各样的GAN</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_1/" title="Chapter 1 Generative Modeling"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 1 Generative Modeling</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_6/" title="Chapter 6 Normalizing Flow Models 标准化流模型"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 6 Normalizing Flow Models 标准化流模型</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_4/" title="Chapter 4 Generative Adversarial Networks 生成对抗网络"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 4 Generative Adversarial Networks 生成对抗网络</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_9/" title="Chapter 9 Transformers"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 9 Transformers</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-2.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="author-info__name">Raphael Hyaan</div><div class="author-info__description">何日可谓归去来</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">173</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">53</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/raphaelhyaan"><i class="fab fa-github"></i><span>Bonjour</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/raphaelhyaan" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:raphael.ma.yuhan@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">仰观宇宙之大，俯察品类之盛</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ganvae%E5%92%8C%E6%B5%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E6%B5%81%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%B8%BB"><span class="toc-number">1.</span> <span class="toc-text">GAN,VAE和流模型的原理（以流模型为主）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.0.1.</span> <span class="toc-text">生成式神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gan-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C"><span class="toc-number">1.0.2.</span> <span class="toc-text">GAN 生成对抗网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vae-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.0.3.</span> <span class="toc-text">VAE 变分自动编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%81%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.0.4.</span> <span class="toc-text">流模型</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/04/26/eles-r/" title="电子系统课程报告"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F入云.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="电子系统课程报告"/></a><div class="content"><a class="title" href="/2025/04/26/eles-r/" title="电子系统课程报告">电子系统课程报告</a><time datetime="2025-04-26T08:10:29.000Z" title="发表于 2025-04-26 16:10:29">2025-04-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/25/eles-4/" title="锁相环 Phase Locked Loop, PLL"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F入云.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="锁相环 Phase Locked Loop, PLL"/></a><div class="content"><a class="title" href="/2025/04/25/eles-4/" title="锁相环 Phase Locked Loop, PLL">锁相环 Phase Locked Loop, PLL</a><time datetime="2025-04-25T08:50:16.000Z" title="发表于 2025-04-25 16:50:16">2025-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/25/eles-3/" title="Génération de signaux 信号发生器"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F入云.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="Génération de signaux 信号发生器"/></a><div class="content"><a class="title" href="/2025/04/25/eles-3/" title="Génération de signaux 信号发生器">Génération de signaux 信号发生器</a><time datetime="2025-04-25T08:50:15.000Z" title="发表于 2025-04-25 16:50:15">2025-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/25/eles-2/" title="filtre 滤波器"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F入云.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="filtre 滤波器"/></a><div class="content"><a class="title" href="/2025/04/25/eles-2/" title="filtre 滤波器">filtre 滤波器</a><time datetime="2025-04-25T08:50:14.000Z" title="发表于 2025-04-25 16:50:14">2025-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/24/zj-gccgdp/" title="关于g++/gdp的安装和vscode配置"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F蔀易染.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="关于g++/gdp的安装和vscode配置"/></a><div class="content"><a class="title" href="/2025/04/24/zj-gccgdp/" title="关于g++/gdp的安装和vscode配置">关于g++/gdp的安装和vscode配置</a><time datetime="2025-04-24T11:38:12.000Z" title="发表于 2025-04-24 19:38:12">2025-04-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Raphael Hyaan</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="framework-info"><span>备案号: </span><a href="href=&quot;https://beian.miit.gov.cn/&quot; ">京ICP备2024051904号</a><span class="footer-separator">|</span><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2F%E5%A4%87%E6%A1%88%E5%9B%BE%E6%A0%87.png" alt="MIT License" height="20" align="top"/><span> </span><a href="href=&quot;https://beian.mps.gov.cn/#/query/webSearch?code=11010802044068&quot; ">京公网安备11010802044068号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>