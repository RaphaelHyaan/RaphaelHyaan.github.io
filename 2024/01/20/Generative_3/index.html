<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Chapter 3 Variational Autoencoders 自动变分编码器 | Raphael's Home</title><meta name="author" content="Raphael Hyaan"><meta name="copyright" content="Raphael Hyaan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Chapter 3 Variational Autoencoders 自动变分编码器  介绍  2013 年，Diederik P. Kingma 和 Max Welling 发表了一篇论文，为一种称为变分自编码器 (VAE) 的神经网络奠定了基础。  故事  💡 想象一下，你面前的地板上堆满了你所有的衣服——裤子、上衣、鞋子和外套，款式各异。你的造型师布莱恩越来越沮丧，因">
<meta property="og:type" content="article">
<meta property="og:title" content="Chapter 3 Variational Autoencoders 自动变分编码器">
<meta property="og:url" content="http://raphaelhyaan.cn/2024/01/20/Generative_3/index.html">
<meta property="og:site_name" content="Raphael&#39;s Home">
<meta property="og:description" content="Chapter 3 Variational Autoencoders 自动变分编码器  介绍  2013 年，Diederik P. Kingma 和 Max Welling 发表了一篇论文，为一种称为变分自编码器 (VAE) 的神经网络奠定了基础。  故事  💡 想象一下，你面前的地板上堆满了你所有的衣服——裤子、上衣、鞋子和外套，款式各异。你的造型师布莱恩越来越沮丧，因">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg">
<meta property="article:published_time" content="2024-01-20T08:19:08.000Z">
<meta property="article:modified_time" content="2024-01-20T08:22:31.619Z">
<meta property="article:author" content="Raphael Hyaan">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg"><link rel="shortcut icon" href="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Freading1.png"><link rel="canonical" href="http://raphaelhyaan.cn/2024/01/20/Generative_3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Chapter 3 Variational Autoencoders 自动变分编码器',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-20 16:22:31'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/butterflyChange/css/code.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Fe747243b42e6168d02fdf8bccbbd4ea.jpg" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">111</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/Reading/"><i class="fa-fw fas fa-book"></i><span> Reading</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Raphael's Home"><span class="site-name">Raphael's Home</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/Reading/"><i class="fa-fw fas fa-book"></i><span> Reading</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Chapter 3 Variational Autoencoders 自动变分编码器</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-20T08:19:08.000Z" title="发表于 2024-01-20 16:19:08">2024-01-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-20T08:22:31.619Z" title="更新于 2024-01-20 16:22:31">2024-01-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%94%9F%E6%88%90%E5%BC%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">学习笔记-生成式神经网络</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Chapter 3 Variational Autoencoders 自动变分编码器"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="chapter-3-variational-autoencoders-自动变分编码器">Chapter 3
Variational Autoencoders 自动变分编码器</h1>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled.png" /></p>
<h1 id="介绍">介绍</h1>
<blockquote>
<p>2013 年，Diederik P. Kingma 和 Max Welling
发表了一篇论文，为一种称为变分自编码器 (VAE) 的神经网络奠定了基础。</p>
</blockquote>
<h2 id="故事">故事</h2>
<aside>
<p>💡
想象一下，你面前的地板上堆满了你所有的衣服——裤子、上衣、鞋子和外套，款式各异。你的造型师布莱恩越来越沮丧，因为他花了很长时间才能找到你需要的物品，因此他制定了一个聪明的计划。他告诉你把你的衣服整理成一个无限高和无限宽的衣柜。当您想要索取特定物品时，您只需告诉布莱恩它的位置，他就会使用他值得信赖的缝纫机从头开始缝制该物品。很快就会发现，您需要将相似的项目彼此靠近放置，以便
Brian 可以仅根据其位置准确地重新创建每个项目。</p>
<p>经过几周的练习，你和布莱恩已经适应了彼此对衣柜布局的理解。现在您可以告诉布莱恩您想要的任何衣服的位置，他可以从头开始准确地缝制它！这给了你一个想法——如果你给布莱恩一个空的衣柜位置，会发生什么？<strong>令你惊讶的是，你发现布莱恩能够完全生成以前不存在的新衣服！</strong>这个过程并不完美，但你现在有无限的选择来生成新衣服，只需在无限衣柜中选择一个空位置，然后让布莱恩用缝纫机施展他的魔法。</p>
</aside>
<h1 id="自动编码器">自动编码器</h1>
<h3 id="与故事的对应">与故事的对应</h3>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_1.png" /></p>
<p>将每件衣服移动到衣柜中的某个位置。这个过程称为编码。布莱恩扮演解码器的角色，在衣柜中找到一个位置并尝试重新创建该物品。这个过程称为解码。</p>
<p>例如，图中的裤子被编码为点 [6.3,
–0.9]。该向量也称为<strong>嵌入<em>embedding</em></strong>，因为编码器尝试将尽可能多的信息嵌入其中，以便解码器可以产生准确的重建</p>
<h3 id="自动编码器-1">自动编码器</h3>
<blockquote>
<p>自动编码器只是一个神经网络，经过训练来执行对项目进行编码和解码的任务，以便该过程的输出尽可能接近原始项目。</p>
</blockquote>
<h3 id="fashion-mnist-数据集">Fashion-MNIST 数据集</h3>
<p>Fashion-MNIST是一个替代MNIST手写数字集的图像数据集。其涵盖了10种类别的衣物，包括T恤、裤子、套头衫等等。每个类别有7000个训练图像和3000个测试图像。数据集中的每个图像都是28x28的灰度图像。</p>
<p>********************加载数据集********************</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets</span><br><span class="line">(x_train,y_train), (x_test,y_test) = datasets.fashion_mnist.load_data()</span><br></pre></td></tr></table></figure>
<p><strong>数据集处理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">imgs</span>):</span><br><span class="line">	imgs = imgs.astype(<span class="string">&quot;float32&quot;</span>) / <span class="number">255.0</span></span><br><span class="line">	imgs = np.pad(imgs, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>)), constant_values=<span class="number">0.0</span>)</span><br><span class="line">	imgs = np.expand_dims(imgs, -<span class="number">1</span>)</span><br><span class="line">	<span class="keyword">return</span> imgs</span><br><span class="line">x_train = preprocess(x_train)</span><br><span class="line">x_test = preprocess(x_test)</span><br></pre></td></tr></table></figure>
<h2 id="自动编码器架构">自动编码器架构</h2>
<p>自动编码器是一个由两部分组成的神经网络</p>
<ul>
<li>编码器网络，将高维输入数据（例如图像）压缩为低维嵌入向量</li>
<li>解码器网络，将给定的嵌入向量解压回原始值域（例如，返回图像）</li>
</ul>
<h3 id="编码器">编码器</h3>
<p>编码器在自动编码器中，编码器的工作是获取输入图像并将其映射到潜在空间中的嵌入向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">encoder_input = layers.Input(</span><br><span class="line">shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>), name = <span class="string">&quot;encoder_input&quot;</span></span><br><span class="line">)</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), strides = <span class="number">2</span>, activation = <span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&quot;same&quot;</span>)(</span><br><span class="line">encoder_input</span><br><span class="line">)</span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), strides = <span class="number">2</span>, activation = <span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line">x = layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), strides = <span class="number">2</span>, activation = <span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line">shape_before_flattening = K.int_shape(x)[<span class="number">1</span>:]</span><br><span class="line">x = layers.Flatten()(x)</span><br><span class="line">encoder_output = layers.Dense(<span class="number">2</span>, name=<span class="string">&quot;encoder_output&quot;</span>)(x)</span><br><span class="line">encoder = models.Model(encoder_input, encoder_output)</span><br></pre></td></tr></table></figure>
<h3 id="转置卷积层">转置卷积层</h3>
<p>标准卷积层允许我们通过设置 strides = 2
将输入张量的两个维度（高度和宽度）的大小减半。卷积转置层使用与标准卷积层相同的原理（在图像上传递过滤器），但不同之处在于设置
strides = 2 使输入张量在两个维度上的大小加倍。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_2.png" /></p>
<p>在 Keras 中，<code>**Conv2DTranspose**</code>
层允许我们对张量执行卷积转置操作。通过堆叠这些层，我们可以使用步长 2
逐渐扩大每层的大小，直到回到原始图像尺寸 32 × 32</p>
<h3 id="解码器">解码器</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">decoder_input = layers.Input(shape=(<span class="number">2</span>,), name=<span class="string">&quot;decoder_input&quot;</span>)</span><br><span class="line">x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)</span><br><span class="line">x = layers.Reshape(shape_before_flattening)(x)</span><br><span class="line">x = layers.Conv2DTranspose(</span><br><span class="line"><span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, activation = <span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&quot;same&quot;</span></span><br><span class="line">)(x)</span><br><span class="line">x = layers.Conv2DTranspose(</span><br><span class="line"><span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, activation = <span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&quot;same&quot;</span></span><br><span class="line">)(x)</span><br><span class="line">x = layers.Conv2DTranspose(</span><br><span class="line"><span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, activation = <span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&quot;same&quot;</span></span><br><span class="line">)(x)</span><br><span class="line">decoder_output = layers.Conv2D(</span><br><span class="line"><span class="number">1</span>,</span><br><span class="line">(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">strides = <span class="number">1</span>,</span><br><span class="line">activation=<span class="string">&quot;sigmoid&quot;</span>,</span><br><span class="line">padding=<span class="string">&quot;same&quot;</span>,</span><br><span class="line">name=<span class="string">&quot;decoder_output&quot;</span></span><br><span class="line">)(x)</span><br><span class="line">decoder = models.Model(decoder_input, decoder_output)</span><br></pre></td></tr></table></figure>
<h3 id="连接编码器和解码器">连接编码器和解码器</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder = Model(encoder_input, decoder(encoder_output))</span><br></pre></td></tr></table></figure>
<p>我们只需要指定自动编码器的输出编码器通过解码器后的输出。</p>
<h2 id="模型训练">模型训练</h2>
<h3 id="选择合适的优化器和损失函数">选择合适的优化器和损失函数</h3>
<ul>
<li>优化器选择常规的Adam优化器</li>
<li>损失函数通常选择原始图像和重建图像的各个像素之间的均方根误差（RMSE）或二元交叉熵</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;adam&quot;</span>, loss=<span class="string">&quot;binary_crossentropy&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>均方根误差（RMSE）和二元交叉熵，都是度量预测值和真实值之间差异的方法</p>
<ul>
<li>均方根误差（RMSE）是观测值与真实值之间差异的平方和的平方根。RMSE
对所有类型的预测错误都有相同的权重，无论预测值是高估还是低估真实值。</li>
<li>另一方面，二元交叉熵损失函数度量的是预测值和真实值之间的“距离”。二元交叉熵损失函数对预测错误的处理并不对称。如果真实值接近1，模型预测越接近1，损失就越小；反之，如果真实值接近0，模型预测越接近0，损失就越小。这就意味着，如果你的模型对某个值的预测结果过高或过低，那么损失函数的值会明显增加。</li>
</ul>
<p>这两种损失函数各有优势，选择哪种损失函数应该根据你的具体需求和实验结果来决定。</p>
<h3 id="开始训练">开始训练</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.fit(</span><br><span class="line">		x_train,</span><br><span class="line">		x_train,</span><br><span class="line">		epochs=<span class="number">5</span>,</span><br><span class="line">		batch_size=<span class="number">100</span>,</span><br><span class="line">		shuffle=<span class="literal">True</span>,</span><br><span class="line">		validation_data=(x_test, x_test),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="重建图像">重建图像</h3>
<blockquote>
<p>使用测试机测试重建图像的能力</p>
</blockquote>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_3.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">example_images = x_test[:<span class="number">5000</span>]</span><br><span class="line">predictions = autoencoder.predict(example_images)</span><br></pre></td></tr></table></figure>
<h3 id="生成新的图像">生成新的图像</h3>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_4.png" /></p>
<p>我们首先生成获得可能的潜在空间</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embeddings = encoder.predict(example_images)</span><br><span class="line">mins, maxs = np.<span class="built_in">min</span>(embeddings, axis=<span class="number">0</span>), np.<span class="built_in">max</span>(embeddings, axis=<span class="number">0</span>)</span><br><span class="line">sample = np.random.uniform(mins, maxs, size=(<span class="number">18</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>我们首先通过编码器，对训练集的数据进行编码以获得可能的潜在空间。</li>
<li>然后我们认为采样空间是包含潜在空间的最小矩形。</li>
<li>在这个空间内均匀采样，并使用解码器进行预测</li>
</ul>
<p>由此即可生成新的图像，并绘制：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">18</span>  <span class="comment"># Number of images to display</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="comment"># Display original images</span></span><br><span class="line">    <span class="comment"># ax = plt.subplot(2, n, i + 1)</span></span><br><span class="line">    <span class="comment"># plt.imshow(example_images[i].reshape(32, 32), cmap=&#x27;gray&#x27;)</span></span><br><span class="line">    <span class="comment"># plt.title(&quot;Original&quot;)</span></span><br><span class="line">    <span class="comment"># plt.axis(&#x27;off&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display reconstructed images</span></span><br><span class="line">    ax = plt.subplot(<span class="number">2</span>, n, i + <span class="number">1</span> + n)</span><br><span class="line">    plt.imshow(reconstructions[i].reshape(<span class="number">32</span>, <span class="number">32</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Reconstructed&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="自动编码器存在的问题">自动编码器存在的问题</h2>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_5.png" /></p>
<p>自动编码器存在若干问题，这些问题可以从图中看出。</p>
<ul>
<li>首先，采样空间存在疏密，如橙色点，在密度较高的地方预测会更加准确，而在空白部分则预测不准确。</li>
<li>其次，不同种类之间的潜在空间存在交叉，这使得在这些重合部分，生成并不准确。</li>
<li>最后，我们很难取的在合理的位置进行采样。</li>
</ul>
<p>基于这些问题，我们可以使用自动变分编码器改进。</p>
<h1 id="使用pytorch的尝试">使用pytorch的尝试</h1>
<p>目前因为对两种方法的理解否尚且十分有限，也难以比较使用pytorch更方便还是tensorflow更方便，因此在这里补充使用pytorch的方法。</p>
<h3 id="数据预处理">数据预处理</h3>
<p><strong>使用tensorflow实现的代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">imgs</span>):</span><br><span class="line">    imgs = imgs.astype(<span class="string">&quot;float32&quot;</span>) / <span class="number">255.0</span></span><br><span class="line">    imgs = np.pad(imgs, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>)), constant_values=<span class="number">0.0</span>)</span><br><span class="line">    imgs = np.expand_dims(imgs, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> imgs</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>imgs = imgs.astype("float32") / 255.0</code></strong>：这行代码将图像的数据类型转换为浮点数，并将像素值归一化到0-1的范围。这是因为图像的原始像素值通常在0-255之间，归一化可以使得数据更适合神经网络的处理。</li>
<li><strong><code>imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)</code></strong>：这行代码对图像进行了填充操作。<strong><code>np.pad</code></strong>函数会在图像的边缘添加额外的像素。这里，它在图像的上下左右各添加了2个像素，填充的值为0。</li>
<li><strong><code>imgs = np.expand_dims(imgs, -1)</code></strong>：这行代码增加了一个维度。在处理图像时，我们通常需要一个表示颜色通道的维度。对于彩色图像，这个维度的大小通常为3（对应RGB三个颜色通道）。对于灰度图像，这个维度的大小为1。这行代码就是在最后一个维度（<strong><code>1</code></strong>表示最后一个维度）上增加了一个大小为1的维度。</li>
</ul>
<p><strong>使用pytorch实现的代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">transform = transform.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,)),</span><br><span class="line">    transforms.Pad(<span class="number">2</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>transforms.ToTensor()</code></strong>：这个转换将PIL图像或者NumPy的ndarray转换为PyTorch的张量（Tensor）。它会将图像的像素强度值从0-255（常见的数据类型为uint8）变为0-1的浮点数，并且，它还会改变数据的维度，它会自动为图像增加一个维度，对于图像，维度通常从<strong><code>(高, 宽, 通道)</code></strong>变为<strong><code>(通道, 高, 宽)</code></strong>。</li>
<li><strong><code>transforms.Normalize((0.5,), (0.5,))</code></strong>：这个转换会对张量进行归一化。这里，<strong><code>(0.5,)</code></strong>是均值，<strong><code>(0.5,)</code></strong>是标准差。这意味着，我们会从每个通道中减去0.5，然后除以0.5。这样做可以使得数据的分布接近标准正态分布，即均值为0，标准差为1，有助于神经网络的训练。</li>
<li><strong><code>transforms.Pad(2)</code></strong>：这个转换会在图像的每一边添加2个像素的填充。这对于某些卷积神经网络是必要的，因为它们可能会减小图像的尺寸。</li>
</ul>
<h3 id="数据加载">数据加载</h3>
<p>******************************************************使用tensorflow实现的代码******************************************************</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(x_train,y_train), (x_test,y_test) = datasets.fashion_mnist.load_data()</span><br><span class="line">x_train = preprocess(x_train)</span><br><span class="line">x_test = preprocess(x_test)</span><br></pre></td></tr></table></figure>
<p>******************************************使用pytorch实现的代码******************************************</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trainset = datasets.FashionMNIST(<span class="string">&#x27;~/.pytorch/F_MNIST_data/&#x27;</span>, download=<span class="literal">True</span>, train=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testset = datasets.FashionMNIST(<span class="string">&#x27;~/.pytorch/F_MNIST_data/&#x27;</span>, download=<span class="literal">True</span>, train=<span class="literal">False</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="自动编码器-2">自动编码器</h3>
<p>原本的编码器的代码不在这里展示。</p>
<p>使用pytorch的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Autoencoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Autoencoder,self).__init__()</span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>,<span class="number">32</span>,<span class="number">3</span>,stride = <span class="number">2</span>,padding = <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">3</span>,stride = <span class="number">2</span>,padding = <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,<span class="number">3</span>,stride = <span class="number">2</span>,padding = <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">2</span>,<span class="number">128</span>*<span class="number">4</span>*<span class="number">4</span>),</span><br><span class="line">            nn.Unflatten(<span class="number">1</span>,(<span class="number">128</span>,<span class="number">4</span>,<span class="number">4</span>)),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>,<span class="number">64</span>,<span class="number">3</span>,stride = <span class="number">2</span>,padding = <span class="number">1</span>,output_padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, <span class="number">32</span>, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">32</span>, <span class="number">1</span>, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()            </span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.encoder(x)</span><br><span class="line">        x = self.decoder(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>结构基本与使用keras的实现相同，区别在于不同的函数的参数并不相同。</p>
<h3 id="开始训练-1">开始训练</h3>
<p>使用相同的优化器和损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Autoencoder().cuda()</span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">loss_function = nn.BCELoss().cuda()</span><br></pre></td></tr></table></figure>
<p>需要自己设置训练循环</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> data, _ <span class="keyword">in</span> trainloader:</span><br><span class="line">        data = data.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = loss_function(output, data)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="自动变分编码器">自动变分编码器</h1>
<h3 id="故事-1">故事</h3>
<p>现在假设，您决定分配一个更容易找到该物品的<strong>一般区域</strong>，而不是将每件衣服放在衣柜中的一个位置。您认为这种更轻松的物品定位方法将有助于解决当前衣柜中局部不连续性的问题。</p>
<p>你将尝试将每件物品区域的中心尽可能靠近衣柜的中间，并且这种偏差距离中心的物品应尽可能接近一米（不小于也不大于）。你越偏离这条规则，你就越需要向布莱恩作为你的造型师支付更多费用。</p>
<h2 id="模型架构的修改">模型架构的修改</h2>
<blockquote>
<p>从自动编码器到变分自动编码器，只需要改变编码器和损失函数</p>
</blockquote>
<h3 id="向多元正态分布的映射">向多元正态分布的映射</h3>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_6.png" /></p>
<p>在自动编码器中，每个图像都直接映射到潜在空间中的一个点。在变分自动编码器中，每个图像都被映射到潜在空间中某个点周围的多元正态分布。</p>
<p><strong>正态分布</strong> 我们使用如下方式定义正态分布<span
class="math inline">\(N(\mu,\sigma)\)</span>：</p>
<p><span class="math display">\[
f\left(x \mid \mu, \sigma^2\right)=\frac{1}{\sqrt{2 \pi \sigma^2}}
e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
\]</span></p>
<p><strong>从输入映射到均值向量和方差向量</strong>
编码器将获取每个输入图像并将其编码为均值、方差两个向量，这两个向量共同定义潜在空间中的多元正态分布。</p>
<ul>
<li><span class="math inline">\(z_{mean}\)</span></li>
<li><span class="math inline">\(z_{log,var}\)</span>：<span
class="math inline">\(z_{log,var} = log(\sigma^2)\)</span></li>
</ul>
<p>由此多元正态分布被定义为：</p>
<ul>
<li><span class="math inline">\(z = z_{mean}+exp(\frac 12\cdot
z_{log,var})\cdot \varepsilon\)</span></li>
</ul>
<p>通过这种方式，我们能保证某个点附近的部分也能跟这个点有更多的相似性</p>
<h3 id="采样层">采样层</h3>
<p><strong>采样层<em>Sampling</em></strong>
随我们要定义一种采样层，允许我们从 <span
class="math inline">\(z_{mean}\)</span> 和 <span
class="math inline">\(z_{log,var}\)</span> 定义的分布中进行采样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Sampling</span>(layers.Layer):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">		z_mean, z_log_var = inputs</span><br><span class="line">		batch = tf.shape(z_mean)[<span class="number">0</span>]</span><br><span class="line">		dim = tf.shape(z_mean)[<span class="number">1</span>]</span><br><span class="line">		epsilon = K.random_normal(shape=(batch, dim))</span><br><span class="line">		<span class="keyword">return</span> z_mean + tf.exp(<span class="number">0.5</span> * z_log_var) * epsilon</span><br></pre></td></tr></table></figure>
<p>这段代码定义了一个名为<code>Sampling</code>的类，它继承了<code>layers.Layer</code>这个类，所以它是一个自定义的Keras层。这个自定义层的主要作用是从输入的均值<code>z_mean</code>和对数方差<code>z_log_var</code>定义的多元正态分布中进行采样。</p>
<p><strong>对 Layer 类进行子类化</strong> 可以通过对抽象 Layer
类进行子类化并定义调用方法来在 Keras
中创建新层，该方法描述了层如何转换张量。</p>
<p>最重要的方法是 <code>call</code>
方法，它定义了张量在该层中的转换过程。</p>
<p>例如，以下是一个简单的全连接层（即线性层）的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units=<span class="number">32</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Linear, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        self.w = self.add_weight(shape=(input_shape[-<span class="number">1</span>], self.units),</span><br><span class="line">                                 initializer=<span class="string">&#x27;random_normal&#x27;</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>)</span><br><span class="line">        self.b = self.add_weight(shape=(self.units,),</span><br><span class="line">                                 initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(inputs, self.w) + self.b</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在上面的 <code>Linear</code> 层的定义中，<code>build</code>
方法定义了层的权重（<code>w</code> 和
<code>b</code>），<code>call</code>
方法定义了如何使用这些权重来计算该层的输出。</p>
<p><strong>重新参数化技巧</strong> 我们可以从标准正态分布中对 epsilon
进行采样，然后手动调整样本以获得正确的均值和方差，而不是直接从参数
z_mean 和 z_log_var
的正态分布中进行采样。它意味着梯度可以在层中自由反向传播。通过保持变量
epsilon
中包含的层的所有随机性，层输出相对于其输入的偏导数可以被证明是确定性的。</p>
<p>********************使用pyTorch实现********************
在pyTorch中可以用<code>nn.Module</code>
实现类似的功能，在Module中，使用<code>forward</code>来代替<code>layer</code>中的<code>call</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Sampling</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        z_mean, z_log_var = inputs</span><br><span class="line">        batch = z_mean.shape[<span class="number">0</span>]</span><br><span class="line">        dim = z_mean.shape[<span class="number">1</span>]</span><br><span class="line">        epsilon = torch.randn(batch, dim)</span><br><span class="line">        <span class="keyword">return</span> z_mean + torch.exp(<span class="number">0.5</span> * z_log_var) * epsilon</span><br></pre></td></tr></table></figure>
<h3 id="修改编码器">修改编码器</h3>
<p><strong>tensorflow的修改方案</strong>
原本的编码器需要生成一个点，即一个二维的向量。而现在的要分别生成两个二维的向量：均值和方差。因此，将原本的：<code>encoder_output = layers.Dense(2, name="encoder_output")(x)</code>
替换为现在的两行代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z_mean = layers.Dense(<span class="number">2</span>, name=<span class="string">&quot;z_mean&quot;</span>)(x)</span><br><span class="line">z_log_var = layers.Dense(<span class="number">2</span>, name=<span class="string">&quot;z_log_var&quot;</span>)(x)</span><br></pre></td></tr></table></figure>
<p>并使用新定义的Sampling层，得到最终的分布z</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = Sampling()([z_mean, z_log_var])</span><br></pre></td></tr></table></figure>
<p><strong>pyTorch的修改方案</strong> 在pytorch中额外定义上述三层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.ZMEAN = nn.Linear(<span class="number">128</span>*<span class="number">4</span>*<span class="number">4</span>,EMBEDDING_DIM)</span><br><span class="line">self.ZLOGVAR = nn.Linear(<span class="number">128</span>*<span class="number">4</span>*<span class="number">4</span>,EMBEDDING_DIM)</span><br><span class="line">self.Sampling = Sampling()</span><br></pre></td></tr></table></figure>
<p>并在forward中依次通过：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">    x = self.encoder(x)</span><br><span class="line">    z_mean = self.ZMEAN(x)</span><br><span class="line">    z_log_var = self.ZLOGVAR(x)</span><br><span class="line">    x = self.Sampling([z_mean,z_log_var])</span><br><span class="line">    x = self.decoder(x)</span><br><span class="line">    <span class="keyword">return</span> z_mean,z_log_var,x</span><br></pre></td></tr></table></figure>
<h3 id="修改损失函数">修改损失函数</h3>
<blockquote>
<p>除了原本的损失函数，需要在其上增加一项<strong><em>Kullback–Leibler
(KL) divergence term</em></strong> KL散度项</p>
</blockquote>
<p>************************************************KL散度项的定义************************************************
KL 散度是一种衡量一个概率分布与另一个概率分布差异程度的方法。在 VAE
中，我们想要测量参数 z_mean 和 z_log_var
的正态分布与标准正态分布的差异程度。因此，可以由以下公式计算：</p>
<p><span class="math display">\[
D_{K L}\left[N(\mu, \sigma \| N(0,1)\right]=-\frac{1}{2}
\sum\left(1+\log \left(\sigma^2\right)-\mu^2-\sigma^2\right)
\]</span></p>
<ul>
<li>在tensorflow中的使用以下公式计算：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kl_loss = tf.reduce_mean(</span><br><span class="line">                tf.reduce_sum(</span><br><span class="line">                    -<span class="number">0.5</span></span><br><span class="line">                    * (<span class="number">1</span> + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),</span><br><span class="line">                    axis=<span class="number">1</span>,</span><br><span class="line">                )</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<ul>
<li>在pyTorch中使用以下方法计算：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kl_loss = torch.mean(-<span class="number">0.5</span> * torch.<span class="built_in">sum</span>(<span class="number">1</span> + z_log_var - z_mean.<span class="built_in">pow</span>(<span class="number">2</span>) - z_log_var.exp(),axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>VAE中，我们希望潜在空间有良好的结构，即相似的数据点应该被映射到潜在空间中的相近位置。通过最小化KL散度损失，我们可以使编码器产生的潜在分布接近于目标分布，这样就可以更好地实现我们的目标，即在潜在空间中有良好的数据组织结构。</p>
<p><strong>新的损失函数和引入模型训练的方式</strong>
接下来要将损失函数引入训练过程中。</p>
<ul>
<li>tensorflow中的引入方式：重写train_step。这种方法可以保留fit()的遍历想，同时使用自己的方法训练。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Step run during training.&quot;&quot;&quot;</span></span><br><span class="line">		<span class="comment"># 创建一个tf.GradientTape上下文，只有这个上下文中所有的操作会被记录，用于之后的计算梯度</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        z_mean, z_log_var, reconstruction = self(data)</span><br><span class="line">        <span class="comment"># 计算重构损失，这是原始输入数据和重构数据之间的二元交叉熵损失的平均值。</span></span><br><span class="line">				reconstruction_loss = tf.reduce_mean(</span><br><span class="line">            BETA</span><br><span class="line">            * losses.binary_crossentropy(</span><br><span class="line">                data, reconstruction, axis=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">				<span class="comment"># 计算KL散度损失，这是从标准正态分布到模型的潜在分布的KL散度。</span></span><br><span class="line">        kl_loss = tf.reduce_mean(</span><br><span class="line">            tf.reduce_sum(</span><br><span class="line">                -<span class="number">0.5</span></span><br><span class="line">                * (<span class="number">1</span> + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),</span><br><span class="line">                axis=<span class="number">1</span>,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">				<span class="comment"># 得到总的损失</span></span><br><span class="line">        total_loss = reconstruction_loss + kl_loss</span><br><span class="line">		<span class="comment"># 计算总损失关于可训练权重的梯度</span></span><br><span class="line">    grads = tape.gradient(total_loss, self.trainable_weights)</span><br><span class="line">		<span class="comment"># 使用优化器应用这些梯度，以更新权重</span></span><br><span class="line">    self.optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.trainable_weights))</span><br><span class="line">		<span class="comment"># 更新损失跟踪器的状态，这些跟踪器用于跟踪训练过程中的损失。</span></span><br><span class="line">    self.total_loss_tracker.update_state(total_loss)</span><br><span class="line">    self.reconstruction_loss_tracker.update_state(reconstruction_loss)</span><br><span class="line">    self.kl_loss_tracker.update_state(kl_loss)</span><br><span class="line">    <span class="keyword">return</span> &#123;m.name: m.result() <span class="keyword">for</span> m <span class="keyword">in</span> self.metrics&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>在pyTorch中的引入，只需要在相应的训练过程中使用新定义的损失函数即可。相对来说比较简单。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    <span class="keyword">for</span> x,_ <span class="keyword">in</span> trainloader:</span><br><span class="line">        x = x.cuda()</span><br><span class="line">        z_mean,z_log_var,reconstruction = vae(x)</span><br><span class="line">        loss = vae.loss_function(x,z_mean,z_log_var,reconstruction)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y,_ <span class="keyword">in</span> testloader:</span><br><span class="line">        y = y.cuda()</span><br><span class="line">        z_mean,z_log_var,reconstruction = vae(y)</span><br><span class="line">        test_loss += vae.loss_function(y,z_mean,z_log_var,reconstruction)</span><br><span class="line">    test_loss /= <span class="built_in">len</span>(testloader)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>,  Test Loss: <span class="subst">&#123;test_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练结果">训练结果</h2>
<p>KL 散度损失项确保编码图像的 z_mean 和 z_log_var
值永远不会偏离标准正态分布太远。由于编码器现在是随机的，而不是确定性的，因此潜在空间现在更加连续，因此不存在太多形成不良的图像。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_7.png" /></p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_8.png" /></p>
<p>通过按服装类型对潜在空间中的点进行着色，我们可以看到没有任何一种类型受到优先对待。右侧图显示了转换为
p 值的空间 -
我们可以看到每种颜色的表示大致相同。再次强调，重要的是要记住，在训练期间根本没有使用标签；
VAE 自行学习了各种形式的服装，以帮助最大限度地减少重建损失。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_9.png" /></p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_10.png" /></p>
<h1 id="更高维度的vae">更高维度的VAE</h1>
<h2 id="celeba数据集">CelebA数据集</h2>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_11.png" /></p>
<p>我们将使用 CelebFaces Attributes (CelebA)
数据集来训练我们的下一个变分自动编码器。这是超过 200,000
张名人面孔彩色图像的集合，每张图像都附有各种标签（例如戴帽子、微笑等）。</p>
<h3 id="数据集加载">数据集加载</h3>
<p><strong>tensorflow的加载方式</strong>
使用tensorflow的函数<code>image_dataset_from_directory</code>
从指定的地址加载图像数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_data = utils.image_dataset_from_directory(</span><br><span class="line">    <span class="string">&quot;/app/data/celeba-dataset/img_align_celeba/img_align_celeba&quot;</span>,</span><br><span class="line">    labels=<span class="literal">None</span>,</span><br><span class="line">    color_mode=<span class="string">&quot;rgb&quot;</span>,</span><br><span class="line">    image_size=(IMAGE_SIZE, IMAGE_SIZE),</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    seed=<span class="number">42</span>,</span><br><span class="line">    interpolation=<span class="string">&quot;bilinear&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Preprocess the data</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">img</span>):</span><br><span class="line">    img = tf.cast(img, <span class="string">&quot;float32&quot;</span>) / <span class="number">255.0</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line">train = train_data.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: preprocess(x))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>labels=None</code>：表示不从目录结构中推断标签，因为这可能是一个无监督学习任务。</li>
<li><code>color_mode="rgb"</code>：表示将图像加载为 RGB 三通道。</li>
<li><code>image_size=(IMAGE_SIZE, IMAGE_SIZE)</code>：表示将图像调整为指定的大小。</li>
<li><code>batch_size=BATCH_SIZE</code>：表示在每次迭代中从数据集中抽取的样本数量。</li>
<li><code>shuffle=True</code>：表示在每个 epoch
开始时都要打乱数据集。</li>
<li><code>seed=42</code>：这是随机数生成器的种子，用于打乱数据集和进行其他随机操作。</li>
<li><code>interpolation="bilinear"</code>：这是用于调整图像大小时的插值方法。</li>
<li>最后使用<code>preprocess</code>函数处理这些图像，<code>train = train_data.map(lambda x: preprocess(x))</code>
意味着将这个函数作用在训练集的所有数据上。</li>
</ul>
<p><strong>pyTorch的加载方式</strong> 可以使用<code>ImageFolder</code>
从指定地址加载数据集，并定义自己transform函数来实现转换。相对来说要比keras简单一些。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义转换</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  <span class="comment"># 调整图像大小</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将 PIL 图像或 NumPy ndarray 转换为 tensor</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], std=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">dataset = datasets.ImageFolder(<span class="string">&quot;/app/data/celeba-dataset/img_align_celeba/img_align_celeba&quot;</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型修改">模型修改</h2>
<p>模型的修改涉及到网络上的修改和参数的优化，可直接参考原书的代码，由于基本结构没有变化，只需要根据需要修改的参数和网络修改相应代码即可，这里不再详述。</p>
<h2 id="潜在空间分析">潜在空间分析</h2>
<blockquote>
<p>假设我们想拍摄一张看起来悲伤的人的照片并给他们一个微笑。为此，我们首先需要在潜在空间中找到一个指向微笑增加方向的向量。将此向量添加到潜在空间中原始图像的编码中将为我们提供一个新点，在解码时，该新点应该为我们提供原始图像的更多笑脸版本。</p>
</blockquote>
<h3 id="smiling特征向量">Smiling特征向量</h3>
<p>CelebA 数据集中的每张图像都标有属性，其中之一是微笑。如果我们将具有
Smiling 属性的编码图像在潜在空间中的平均位置减去不具有 Smiling
属性的编码图像的平均位置，我们将获得指向 Smiling 方向的向量。</p>
<p><span class="math display">\[
z_{new} = z+\alpha\cdot feature\_vecture
\]</span></p>
<p>再将这个特征向量一个<span
class="math inline">\(\alpha\)</span>的比例系数加载原本的向量上，则可以得到一个更加接近与微笑的向量。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_12.png" /></p>
<h3 id="面部信息混合">面部信息混合</h3>
<blockquote>
<p>想象潜在空间中的两个点 A 和 B，它们代表两个图像。如果您从 A
点开始，沿直线朝 B
点走，边走边解码线上的每个点，您会看到从起始面到结束面的逐渐过渡。由此可以实现面部信息的混合。</p>
</blockquote>
<p><span class="math display">\[
z_{new} = z_A * (1- \alpha) + z_B * \alpha
\]</span></p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_3%2FUntitled_13.png" /></p>
<h1 id="总结">总结</h1>
<p>在本章中，我们了解到变分自动编码器是生成模型工具箱中的一种强大工具。我们首先探索了如何使用普通的自动编码器将高维度的图像映射到低维度的潜在空间，从而从各个无信息的像素中提取出高级特征。然而，我们很快发现使用普通自动编码器作为生成模型存在一些缺点，例如，从学习的潜在空间中进行采样存在问题。</p>
<p>变分自动编码器通过引入随机性和约束潜在空间中的点分布来解决这些问题。我们看到，通过一些小的调整，我们可以将我们的自动编码器转变为变分自动编码器，从而赋予它成为真正的生成模型的能力。</p>
<p>最后，我们将新的技术应用到面部生成问题中，看到我们如何可以简单地从标准正态分布中解码点来生成新的面部。此外，通过在潜在空间内进行向量算术，我们可以实现一些惊人的效果，如面部变形和特征操作。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn">Raphael Hyaan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn/2024/01/20/Generative_3/">http://raphaelhyaan.cn/2024/01/20/Generative_3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://raphaelhyaan.cn" target="_blank">Raphael's Home</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/20/Generative_4/" title="Chapter 4 Generative Adversarial Networks 生成对抗网络"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Chapter 4 Generative Adversarial Networks 生成对抗网络</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/20/Generative_2/" title="Chapter 2 Deep Learning"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Chapter 2 Deep Learning</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/11/22/DCGAN-1/" title="DCGAN"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FDCGAN%2FUntitled.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-22</div><div class="title">DCGAN</div></div></a></div><div><a href="/2024/01/20/Generative/" title="生成式神经网络总览"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">生成式神经网络总览</div></div></a></div><div><a href="/2024/01/20/Generative_1/" title="Chapter 1 Generative Modeling"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 1 Generative Modeling</div></div></a></div><div><a href="/2024/01/20/Generative_2/" title="Chapter 2 Deep Learning"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 2 Deep Learning</div></div></a></div><div><a href="/2024/01/20/Generative_10/" title="Chapter 10 Advanced GANs 各种各样的GAN"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 10 Advanced GANs 各种各样的GAN</div></div></a></div><div><a href="/2024/01/20/Generative_5/" title="Chapter 5 Autoregressive Models 自回归模型"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 5 Autoregressive Models 自回归模型</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Fe747243b42e6168d02fdf8bccbbd4ea.jpg" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="author-info__name">Raphael Hyaan</div><div class="author-info__description">何日可谓归去来</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">111</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/raphaelhyaan"><i class="fab fa-github"></i><span>Bonjour</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/raphaelhyaan" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:raphael.ma.yuhan@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">仰观宇宙之大，俯察品类之盛</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter-3-variational-autoencoders-%E8%87%AA%E5%8A%A8%E5%8F%98%E5%88%86%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.</span> <span class="toc-text">Chapter 3
Variational Autoencoders 自动变分编码器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%85%E4%BA%8B"><span class="toc-number">2.1.</span> <span class="toc-text">故事</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">自动编码器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E6%95%85%E4%BA%8B%E7%9A%84%E5%AF%B9%E5%BA%94"><span class="toc-number">3.0.1.</span> <span class="toc-text">与故事的对应</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8-1"><span class="toc-number">3.0.2.</span> <span class="toc-text">自动编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fashion-mnist-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.0.3.</span> <span class="toc-text">Fashion-MNIST 数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">自动编码器架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">3.1.1.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">3.1.2.</span> <span class="toc-text">转置卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">3.1.3.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9E%E6%8E%A5%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">3.1.4.</span> <span class="toc-text">连接编码器和解码器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">3.2.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.2.1.</span> <span class="toc-text">选择合适的优化器和损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">3.2.2.</span> <span class="toc-text">开始训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E5%BB%BA%E5%9B%BE%E5%83%8F"><span class="toc-number">3.2.3.</span> <span class="toc-text">重建图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%96%B0%E7%9A%84%E5%9B%BE%E5%83%8F"><span class="toc-number">3.2.4.</span> <span class="toc-text">生成新的图像</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">3.3.</span> <span class="toc-text">自动编码器存在的问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8pytorch%E7%9A%84%E5%B0%9D%E8%AF%95"><span class="toc-number">4.</span> <span class="toc-text">使用pytorch的尝试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">4.0.1.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">4.0.2.</span> <span class="toc-text">数据加载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8-2"><span class="toc-number">4.0.3.</span> <span class="toc-text">自动编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83-1"><span class="toc-number">4.0.4.</span> <span class="toc-text">开始训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%8F%98%E5%88%86%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">5.</span> <span class="toc-text">自动变分编码器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%85%E4%BA%8B-1"><span class="toc-number">5.0.1.</span> <span class="toc-text">故事</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E7%9A%84%E4%BF%AE%E6%94%B9"><span class="toc-number">5.1.</span> <span class="toc-text">模型架构的修改</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E6%98%A0%E5%B0%84"><span class="toc-number">5.1.1.</span> <span class="toc-text">向多元正态分布的映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E5%B1%82"><span class="toc-number">5.1.2.</span> <span class="toc-text">采样层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">5.1.3.</span> <span class="toc-text">修改编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.1.4.</span> <span class="toc-text">修改损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"><span class="toc-number">5.2.</span> <span class="toc-text">训练结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9B%B4%E9%AB%98%E7%BB%B4%E5%BA%A6%E7%9A%84vae"><span class="toc-number">6.</span> <span class="toc-text">更高维度的VAE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#celeba%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.1.</span> <span class="toc-text">CelebA数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD"><span class="toc-number">6.1.1.</span> <span class="toc-text">数据集加载</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%AE%E6%94%B9"><span class="toc-number">6.2.</span> <span class="toc-text">模型修改</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BD%9C%E5%9C%A8%E7%A9%BA%E9%97%B4%E5%88%86%E6%9E%90"><span class="toc-number">6.3.</span> <span class="toc-text">潜在空间分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#smiling%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="toc-number">6.3.1.</span> <span class="toc-text">Smiling特征向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%A2%E9%83%A8%E4%BF%A1%E6%81%AF%E6%B7%B7%E5%90%88"><span class="toc-number">6.3.2.</span> <span class="toc-text">面部信息混合</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">7.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/07/12/mg-m-2/" title="乐理 02 调性，音阶，调号，音程和三和弦"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F乐理和吉他.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="乐理 02 调性，音阶，调号，音程和三和弦"/></a><div class="content"><a class="title" href="/2024/07/12/mg-m-2/" title="乐理 02 调性，音阶，调号，音程和三和弦">乐理 02 调性，音阶，调号，音程和三和弦</a><time datetime="2024-07-12T08:17:12.000Z" title="发表于 2024-07-12 16:17:12">2024-07-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/29/mg/" title="乐理和吉他"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F乐理和吉他.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="乐理和吉他"/></a><div class="content"><a class="title" href="/2024/06/29/mg/" title="乐理和吉他">乐理和吉他</a><time datetime="2024-06-29T08:46:30.000Z" title="发表于 2024-06-29 16:46:30">2024-06-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/29/mg-m-1/" title="乐理 01 声音的特性，音乐记谱法，音高，节奏"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F乐理和吉他.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="乐理 01 声音的特性，音乐记谱法，音高，节奏"/></a><div class="content"><a class="title" href="/2024/06/29/mg-m-1/" title="乐理 01 声音的特性，音乐记谱法，音高，节奏">乐理 01 声音的特性，音乐记谱法，音高，节奏</a><time datetime="2024-06-29T08:46:28.000Z" title="发表于 2024-06-29 16:46:28">2024-06-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/13/ht-2/" title="热交换器，热辐射 échangeur, rayonnement"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fht.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="热交换器，热辐射 échangeur, rayonnement"/></a><div class="content"><a class="title" href="/2024/06/13/ht-2/" title="热交换器，热辐射 échangeur, rayonnement">热交换器，热辐射 échangeur, rayonnement</a><time datetime="2024-06-13T15:32:24.000Z" title="发表于 2024-06-13 23:32:24">2024-06-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/13/ht-1/" title="传热，热传导, 热对流 transfert de chaleur, conduction, convection"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fht.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="传热，热传导, 热对流 transfert de chaleur, conduction, convection"/></a><div class="content"><a class="title" href="/2024/06/13/ht-1/" title="传热，热传导, 热对流 transfert de chaleur, conduction, convection">传热，热传导, 热对流 transfert de chaleur, conduction, convection</a><time datetime="2024-06-13T15:32:22.000Z" title="发表于 2024-06-13 23:32:22">2024-06-13</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Raphael Hyaan</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="framework-info"><span>备案号: </span><a href="href=&quot;https://beian.miit.gov.cn/&quot; ">京ICP备2024051904号</a><span class="footer-separator">|</span><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2F%E5%A4%87%E6%A1%88%E5%9B%BE%E6%A0%87.png" alt="MIT License" height="20" align="top"/><span> </span><a href="href=&quot;https://beian.mps.gov.cn/#/query/webSearch?code=11010802044068&quot; ">京公网安备11010802044068号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>