<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Chapter 8 Diffusion Models 扩散模型 | Raphael's Home</title><meta name="author" content="Raphael Hyaan"><meta name="copyright" content="Raphael Hyaan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Chapter 8 Diffusion Models 扩散模型   这个模型很有意思，我会尝试使用PyTorch实现。  介绍  扩散模型是过去十年中引入的最具影响力和影响力的图像生成生成建模技术之一。扩散这个名字的灵感来自于热力学扩散特性  突破性的扩散模型论文于 2020 年夏天发表。该论文揭示了扩散模型和基于分数的生成模型之间的深层联系，作者训练了一个扩散模型，可以在多">
<meta property="og:type" content="article">
<meta property="og:title" content="Chapter 8 Diffusion Models 扩散模型">
<meta property="og:url" content="http://raphaelhyaan.cn/2024/01/20/Generative_8/index.html">
<meta property="og:site_name" content="Raphael&#39;s Home">
<meta property="og:description" content="Chapter 8 Diffusion Models 扩散模型   这个模型很有意思，我会尝试使用PyTorch实现。  介绍  扩散模型是过去十年中引入的最具影响力和影响力的图像生成生成建模技术之一。扩散这个名字的灵感来自于热力学扩散特性  突破性的扩散模型论文于 2020 年夏天发表。该论文揭示了扩散模型和基于分数的生成模型之间的深层联系，作者训练了一个扩散模型，可以在多">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg">
<meta property="article:published_time" content="2024-01-20T08:19:14.000Z">
<meta property="article:modified_time" content="2024-01-20T08:24:27.578Z">
<meta property="article:author" content="Raphael Hyaan">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg"><link rel="shortcut icon" href="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2F%E5%9C%86%E8%A7%92-00014-2437946578-1girl%2C%20solo%2C%20eyewear%20on%20head%2C%20gloves%2C%20fingerless%20gloves%2C%20jacket%2C%20shorts%2C%20very%20long%20hair%2C%20black%20jacket%2C%20parted%20lips%2C%20choker%2C%20grey.png"><link rel="canonical" href="http://raphaelhyaan.cn/2024/01/20/Generative_8/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Chapter 8 Diffusion Models 扩散模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-20 16:24:27'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/butterflyChange/css/code.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Fe747243b42e6168d02fdf8bccbbd4ea.jpg" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">75</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="Raphael's Home"><span class="site-name">Raphael's Home</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Chapter 8 Diffusion Models 扩散模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-20T08:19:14.000Z" title="发表于 2024-01-20 16:19:14">2024-01-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-20T08:24:27.578Z" title="更新于 2024-01-20 16:24:27">2024-01-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%94%9F%E6%88%90%E5%BC%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">学习笔记-生成式神经网络</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Chapter 8 Diffusion Models 扩散模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="chapter-8-diffusion-models-扩散模型">Chapter 8 Diffusion Models
扩散模型</h1>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled.jpeg" /></p>
<blockquote>
<p>这个模型很有意思，我会尝试使用PyTorch实现。</p>
</blockquote>
<h2 id="介绍">介绍</h2>
<blockquote>
<p>扩散模型是过去十年中引入的最具影响力和影响力的图像生成生成建模技术之一。扩散这个名字的灵感来自于热力学扩散特性</p>
</blockquote>
<p>突破性的扩散模型论文于 2020
年夏天发表。该论文揭示了扩散模型和基于分数的生成模型之间的深层联系，作者训练了一个扩散模型，可以在多个数据集胜过竞争对手
GAN，称为去噪扩散概率模型 (DDPM)</p>
<h3 id="扩散电视">扩散电视</h3>
<blockquote>
<p>我们再次从一个小故事开始。</p>
</blockquote>
<aside>
<p>💡
您站在一家出售电视机的电子商店里。这里有数百台相同的电视机按顺序连接在一起，一直延伸到商店的后面。更奇怪的是，最前面的几台电视机似乎只显示随机的静态噪声。</p>
<p>店主解释说，这是新的DiffuseTV型号。在制造过程中，DiffuseTV接触了数千张以前的电视节目的图像，但是这些图像都被逐渐加入了随机的静态噪声，直到它们与纯随机噪声无法区分。然后，电视机被设计成以小步骤消除随机噪声，尝试预测在加入噪声之前图像是什么样子的。</p>
<p>可以看到，当走进商店时，每台电视机上的图像确实比上一台稍微清晰一些。
最终到达了一长排电视机的尽头，在那里您以看到最后一台电视机上的完美画面。</p>
<p>观众不是选择要观看的频道，而是选择一个随机的初始静态配置。每个配置都会导致不同的输出图像，而且在某些型号中甚至可以由您选择输入的文本提示来指导。与只有有限范围的频道可供观看的普通电视机不同，DiffuseTV给观众无限的选择和自由，可以生成他们想要出现在屏幕上的任何东西！</p>
</aside>
<h1 id="denoising-diffusion-models-ddm"><strong>Denoising Diffusion
Models (DDM)</strong></h1>
<blockquote>
<p>我们尝试使用系列小步骤来给图像“降噪”。最终我们能从一个纯粹的随机噪音开始，逐步降噪到一个看起来是训练集中的数。</p>
</blockquote>
<h2 id="the-flowers-dataset"><strong>The Flowers Dataset</strong></h2>
<blockquote>
<p>Flowers Dataset中包含8000张上色的花朵图片。每张图片为<span
class="math inline">\(64\times64\)</span>的尺寸。</p>
</blockquote>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled.png" /></p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_1.png" /></p>
<h3 id="代码实现">代码实现</h3>
<p>Keras的代码中有几处细节：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_data = utils.image_dataset_from_directory(</span><br><span class="line">    TRAIN_DATA_PATH,</span><br><span class="line">    labels=<span class="literal">None</span>,</span><br><span class="line">    image_size=(IMAGE_SIZE, IMAGE_SIZE),</span><br><span class="line">    batch_size=<span class="literal">None</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    seed=<span class="number">42</span>,</span><br><span class="line">    interpolation=<span class="string">&quot;bilinear&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">img</span>):</span><br><span class="line">    img = tf.cast(img, <span class="string">&quot;float32&quot;</span>) / <span class="number">255.0</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line">train = train_data.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: preprocess(x))</span><br><span class="line">train = train.repeat(DATASET_REPETITIONS)</span><br><span class="line">train = train.batch(BATCH_SIZE, drop_remainder=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>在使用<code>image_dataset_from_directory</code>
的过程中没有指定batch，而是在之后使用<code>train.batch(BATCH_SIZE, drop_remainder=True)</code>
指定，这是为了丢弃最终不足一个批次的数据。我们可以指定PyTorch的<code>DataLoader</code>的<code>drop_last=True</code>来实现</li>
<li><code>train = train.repeat(DATASET_REPETITIONS)</code>
将数据重复了五边。我们可以简单的自定义一个数据集类来实现。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FLowerDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_dir, transform, repetitions</span>):</span><br><span class="line">        self.data = datasets.ImageFolder(data_dir, transform=transform)</span><br><span class="line">        self.repetitions = repetitions</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        orig_index = index % <span class="built_in">len</span>(self.data)</span><br><span class="line">        img, label = self.data[orig_index]</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data) * self.repetitions</span><br></pre></td></tr></table></figure>
<h2 id="前向扩散过程">前向扩散过程</h2>
<blockquote>
<p>我们有一个图像
，我们希望通过大量步骤逐渐损坏它，以便最终它与标准高斯噪声无法区分。</p>
</blockquote>
<h3 id="前向扩散原理">前向扩散原理</h3>
<p>我们可以定义一个函数 <span
class="math inline">\(q\)</span>，将少量方差为 <span
class="math inline">\(β_t\)</span> 的高斯噪声添加到图像。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_2.png" /></p>
<p>由此，向前扩散的过程可以表示为：</p>
<p><span class="math display">\[
\mathbf{x}_t=\sqrt{1-\beta_t} \mathbf{x}_{t-1}+\sqrt{\beta_t}
\epsilon_{t-1}
\]</span></p>
<p>其中，<span
class="math inline">\(\epsilon_{t-1}\)</span>是一个标准正太分布。之所以同时对<span
class="math inline">\(x_{t-1}\)</span>进行缩放是希望在整个过程中图像的方差保持。<span
class="math inline">\(\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)\)</span>。</p>
<p>由此，我们可以定义q</p>
<p><span class="math display">\[
q\left(\mathbf{x}_t \mid
\mathbf{x}_{t-1}\right)=\mathscr{N}\left(\mathbf{x}_t ; \sqrt{1-\beta_t}
\mathbf{x}_{t-1}, \beta_t \mathbf{I}\right)  = LG\left(\mathbf{x}_t ;
\sqrt{1-\beta_t} \mathbf{x}_{t-1}, \sqrt\beta_t \mathbf{I}\right)
\]</span></p>
<h3 id="重参数化技巧-reparameterization-trick">重参数化技巧
<strong><em>Reparameterization Trick</em></strong></h3>
<blockquote>
<p>相比于一个迭代的函数，我们更希望有一个函数，可以直接从图像<span
class="math inline">\(x_0\)</span>跳转到<span
class="math inline">\(x_t\)</span>。这一点我们通过重参数化技巧实现。</p>
</blockquote>
<p>原理十分简单，我们使用<span
class="math inline">\(\alpha_t=1-\beta_t\)</span>和<span
class="math inline">\(\bar{\alpha}_t=\prod_{i=1}^t
\alpha_i\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}\mathbf{x}_t &amp; =\sqrt{\alpha_t}
\mathbf{x}_{t-1}+\sqrt{1-\alpha_t} \epsilon_{t-1} \\&amp;
=\sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2}+\sqrt{1-\alpha_t
\alpha_{t-1}} \epsilon \\&amp; =\cdots \\&amp; =\sqrt{\bar{\alpha}_t}
\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \epsilon\end{aligned}
\]</span></p>
<p>由此得到函数q</p>
<p><span class="math display">\[
q\left(\mathbf{x}_t \mid
\mathbf{x}_0\right)=\mathscr{N}\left(\mathbf{x}_t ;
\sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right)
\mathbf{I}\right)
\]</span></p>
<h3 id="扩散方案">扩散方案</h3>
<blockquote>
<p><span
class="math inline">\(\beta_t\)</span>可以随时间变化，原论文中，<span
class="math inline">\(\beta_t\)</span>被要求逐渐增大。即我们使用了线性的扩散方案。除此之外，原书还描述了co<em>sine
and offset cosine diffusion schedules。</em></p>
</blockquote>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_3.png" /></p>
<p>通过这些方案，图像会逐渐接近于无法区分的标准高斯噪音。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_4.png" /></p>
<p>简单的修改原书的Keras的代码，可以得到PyTorch的代码，如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_diffusion_schedule</span>(<span class="params">diffusion_times</span>):</span><br><span class="line">    min_rate = <span class="number">0.0001</span></span><br><span class="line">    max_rate = <span class="number">0.02</span></span><br><span class="line">    betas = min_rate + diffusion_times * (max_rate - min_rate)</span><br><span class="line">    alphas = <span class="number">1</span> - betas</span><br><span class="line">    alpha_bars = torch.cumprod(alphas, dim=<span class="number">0</span>)</span><br><span class="line">    signal_rates = torch.sqrt(alpha_bars)</span><br><span class="line">    noise_rates = torch.sqrt(<span class="number">1</span> - alpha_bars)</span><br><span class="line">    <span class="keyword">return</span> noise_rates, signal_rates</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_diffusion_schedule</span>(<span class="params">diffusion_times</span>):</span><br><span class="line">    signal_rates = torch.cos(diffusion_times * torch.pi / <span class="number">2</span>)</span><br><span class="line">    noise_rates = torch.sin(diffusion_times * torch.pi / <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> noise_rates, signal_rates</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">offset_cosine_diffusion_schedule</span>(<span class="params">diffusion_times</span>):</span><br><span class="line">    min_signal_rate = <span class="number">0.02</span></span><br><span class="line">    max_signal_rate = <span class="number">0.95</span></span><br><span class="line">    start_angle = torch.acos(torch.tensor(max_signal_rate))</span><br><span class="line">    end_angle = torch.acos(torch.tensor(min_signal_rate))</span><br><span class="line">    diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)</span><br><span class="line">    signal_rates = torch.cos(diffusion_angles)</span><br><span class="line">    noise_rates = torch.sin(diffusion_angles)</span><br><span class="line">    <span class="keyword">return</span> noise_rates, signal_rates</span><br></pre></td></tr></table></figure>
<h2 id="反向扩散过程">反向扩散过程</h2>
<blockquote>
<p>我们希望构建一个神经网络来消除噪音，来近似一个函数<span
class="math inline">\(P_\theta\)</span>。</p>
</blockquote>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_5.png" /></p>
<p>这看起来与VAE很相似。他们之间的区别在于，在VAE中将模型转化为噪音也是学习的，而在diffusion中这是参数化的。因此应用类似于VAE的损失函数是有意义的。我们对图像<span
class="math inline">\(x_0\)</span>采样，并经过<span
class="math inline">\(t\)</span>步操作添加噪音。我们将这个添加噪音之后的图像和噪音率<span
class="math inline">\(\bar{\alpha}_t\)</span>提供给神经网络，并<strong>要求神经网络预测噪音<span
class="math inline">\(\epsilon\)</span></strong>，并计算预测值和真值之间的均方差。</p>
<h3 id="噪音消除过程">噪音消除过程</h3>
<p>如上所属，经过训练的神经网络能够对噪音进行预测。下述代码中出现了两个模型，这两个模型在训练模型一部分描述。现在只需知道在训练过程和预测过程中，我们分别使用不同的模型即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">denoise</span>(<span class="params">self, noisy_images, noise_rates, signal_rates, training</span>):</span><br><span class="line">        <span class="keyword">if</span> training:</span><br><span class="line">            network = self.network</span><br><span class="line">            network.train()</span><br><span class="line">            pred_noises = network([noisy_images,torch.<span class="built_in">pow</span>(noise_rates,<span class="number">2</span>)])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            network = self.ema_network</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                network.<span class="built_in">eval</span>()</span><br><span class="line">                pred_noises = network([noisy_images,torch.<span class="built_in">pow</span>(noise_rates,<span class="number">2</span>)])</span><br><span class="line">        </span><br><span class="line">        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> pred_noises, pred_images</span><br></pre></td></tr></table></figure>
<p><code>denoise</code>
函数接受噪声图像、噪声率、信号率和一个表示是否在训练模式下的布尔值。如果在训练模式下，它会使用
<code>self.network</code>，否则会使用
<code>self.ema_network</code>。然后，它会计算预测的噪声和图像，并返回它们。</p>
<p>在模型对噪音进行预测后，我们进行噪音添加的逆向操作，按照噪音添加时的规则，逐步减去预测的噪音以获得原本的图像。</p>
<h3 id="反向传播过程">反向传播过程</h3>
<p>不断重复这一过程，我们即可完成反向传播过程。在这一过程中，我们既使用了已经定义过的噪音去除过程不断地预测噪音和<strong>最初的图像</strong>，由进行正向传播中的噪音添加过程，来获得<strong>上一个图像</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverse_diffusion</span>(<span class="params">self, initial_noise, diffusion_steps</span>):</span><br><span class="line">        <span class="comment"># 获得图像的数量，盖为批次大小</span></span><br><span class="line">        num_images = initial_noise.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 获得每个扩散步数的大小</span></span><br><span class="line">        step_size = <span class="number">1.0</span> / diffusion_steps</span><br><span class="line">        <span class="comment"># 定义当前的图像为噪声</span></span><br><span class="line">        current_images = initial_noise</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(diffusion_steps):</span><br><span class="line">            <span class="comment"># 扩散次数，因为是反向扩散，所以实际的应为1-当前扩散的次数/总次数</span></span><br><span class="line">            diffusion_times = torch.ones((num_images,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>))-step * step_size</span><br><span class="line">            <span class="comment"># 调用扩散方案，获得噪音率和信号率</span></span><br><span class="line">            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)</span><br><span class="line">            <span class="comment"># 调用去噪函数，获得去噪后的噪声和图像</span></span><br><span class="line">            pred_noises, pred_images = self.denoise(current_images, noise_rates, signal_rates, training=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># 计算下一个扩散次数，和下一个噪音率和信号率</span></span><br><span class="line">            next_diffusion_times = diffusion_times - step_size</span><br><span class="line">            next_noise_rates, next_signal_rates = self.diffusion_schedule(next_diffusion_times)</span><br><span class="line">            <span class="comment"># 得到当前图像</span></span><br><span class="line">            current_images = pred_images * next_signal_rates + pred_noises * next_noise_rates</span><br><span class="line">        <span class="keyword">return</span> pred_images</span><br></pre></td></tr></table></figure>
<p><code>reverse_diffusion</code>
函数接受初始噪声和扩散步骤的数量。它首先计算每个扩散步骤的大小，然后在每个步骤中，它会计算扩散时间、噪声率和信号率，然后调用
<code>denoise</code>
函数去噪，然后计算下一个扩散时间和下一个噪声率和信号率，最后更新当前的图像。这个过程会重复进行扩散步骤的数量次，最后返回预测的图像。</p>
<p>需要关注的是，<strong>只有最后一次循环得到的最初的图像在这一过程中被使用，此时上一个图像即为最初的图像。</strong>换句话说，我们会经过一系列小步骤得到图像而非一步到位。</p>
<h3 id="图像生成">图像生成</h3>
<p>根据以上程序，我们可以使用模型生成图像。这里的<code>denormalize</code>
与图像最初的输入方式有关。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, num_images, diffusion_steps, initial_noise=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> initial_noise <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            initial_noise = torch.randn(num_images, <span class="number">3</span>, IMAGE_SIZE, IMAGE_SIZE).to(device)</span><br><span class="line">        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps)</span><br><span class="line">        <span class="keyword">return</span> self.denormalize(generated_images)</span><br></pre></td></tr></table></figure>
<h2 id="the-u-net-denoising-model"><strong>The U-Net Denoising
Model</strong></h2>
<blockquote>
<p>接下来我们来讨论U-Net模型。</p>
</blockquote>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_6.png" /></p>
<p>与变分自动编码器类似，U-Net 由两半组成：</p>
<ul>
<li>下采样，其中输入图像在空间上压缩但在通道上扩展</li>
<li>上采样，其中表示在空间上扩展，而通道数为减少。</li>
</ul>
<p>我们很容易将这个网络表示成这些层的堆叠，但这些层应该如何实现呢？</p>
<h3 id="正弦编码器-sinusoidal_embedding">正弦编码器
<em>sinusoidal_embedding</em></h3>
<blockquote>
<p>使用正弦编码器进行编码</p>
</blockquote>
<p>这是一种将标量值映射到连续高位空间的方法，其编码方式可以被表示为：</p>
<p><span class="math display">\[
\gamma(x)=\left(\sin \left(2 \pi e^{0 f} x\right), \cdots, \sin \left(2
\pi e^{(L-1) f)} x\right), \cos \left(2 \pi e^{0 f} x\right), \cdots,
\cos \left(2 \pi e^{(L-1) f} x\right)\right)
\]</span></p>
<p>我们可以选择<span class="math inline">\(L = 16，f = \frac{\ln
(1000)}{L-1}\)</span>进行建模。由此可以产生一个如下的编码模式。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_7.png" /></p>
<p>其横坐标是<span
class="math inline">\(x\)</span>，即噪音的方差。纵坐标是其维度数，当我们选择<span
class="math inline">\(L =
16\)</span>时，有总共32个维度。所有的值都被映射在<span
class="math inline">\([0,1]\)</span>之间。</p>
<p>将使用tensorflow实现的代码简单的替换为torch，即可复现正弦编码器的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sinusoidal_embedding</span>(<span class="params">x</span>):</span><br><span class="line">    frequencies = torch.exp(</span><br><span class="line">        torch.linspace(</span><br><span class="line">            torch.log(<span class="number">1.0</span>),</span><br><span class="line">            torch.log(<span class="number">1000.0</span>),</span><br><span class="line">            NOISE_EMBEDDING_SIZE // <span class="number">2</span>,</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    angular_speeds = <span class="number">2.0</span> * torch.pi * frequencies</span><br><span class="line">    embeddings = torch.concat(</span><br><span class="line">        [torch.sin(angular_speeds * x), torch.cos(angular_speeds * x)], axis=<span class="number">3</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>
<h3 id="残差块-residualblock">残差块 <em>ResidualBlock</em></h3>
<p>我们已经介绍过残差块，它可以帮助我们在更深的网络中学习更复杂的模式，避免梯度消失等原因带来的模型退化的影响。它的原理如图所示：</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_8.png" /></p>
<p>本程序中使用的残差块的跳跃连接并没有增加额外的卷积层，<strong>除非尺寸不契合</strong>。</p>
<p>在跳跃连接之外，模型包括一个批量归一化层和两个卷积层。它可以被简单的如下实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels,out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock,self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv_residual = nn.Conv2d(in_channels,out_channels,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.bn = nn.BatchNorm2d(out_channels,affine=<span class="literal">False</span>)</span><br><span class="line">        self.width = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        input_width = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> input_width == self.width:</span><br><span class="line">            residual = x</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            residual = self.conv_residual(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        x = F.silu(self.conv1(x))</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = x+residual</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>在实现中有两点需要注意的是：</p>
<ul>
<li>PyTorch在定义卷积层时需要输入通道数的参数。</li>
<li>在输入通道数不等于输出通道数时需要经过一个卷积核尺寸为1的卷积层来改变通道数。</li>
<li>PyTorch的通道数是<code>shape[1]</code>而不是<code>shape[3]</code></li>
</ul>
<h3 id="下采样和上采样块-downblock-and-upblock">下采样和上采样块
<em>DownBlock and UpBlock</em></h3>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_9.png" /></p>
<p>每个 <code>DownBlock</code>
都会通过<code>ResidualBlocks</code>增加通道数，同时还应用最终的<code>AveragePooling2D</code>
层，以便将图像的大小减半。每个 <code>ResidualBlock</code>
输出都会添加到列表中，供 UpBlock 层稍后使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DownBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels,out_channels, block_depth</span>):</span><br><span class="line">        <span class="built_in">super</span>(DownBlock, self).__init__()</span><br><span class="line">        self.residual1 = ResidualBlock(in_channels, out_channels)</span><br><span class="line">        self.residuals = nn.ModuleList([ResidualBlock(out_channels, out_channels) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(block_depth-<span class="number">1</span>)])</span><br><span class="line">        self.pool = nn.AvgPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x, skips = x</span><br><span class="line">        x = self.residual1(x)</span><br><span class="line">        skips.append(x)</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.residuals:</span><br><span class="line">            x = block(x)</span><br><span class="line">            skips.append(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="keyword">return</span> x, skips</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UpBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, skips_channels, block_depth</span>):</span><br><span class="line">        <span class="built_in">super</span>(UpBlock, self).__init__()</span><br><span class="line">        self.residual1 = ResidualBlock(in_channels+skips_channels, out_channels)</span><br><span class="line">        self.residuals = nn.ModuleList([ResidualBlock(out_channels+skips_channels, out_channels) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(block_depth-<span class="number">1</span>)])</span><br><span class="line">        self.up = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x, skips = x</span><br><span class="line">        x = self.up(x)</span><br><span class="line">        x = torch.cat([x, skips.pop()], dim=<span class="number">1</span>)</span><br><span class="line">        x = self.residual1(x)</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.residuals:</span><br><span class="line">            x = torch.cat([x, skips.pop()], dim=<span class="number">1</span>)</span><br><span class="line">            x = block(x)</span><br><span class="line">        <span class="keyword">return</span> x, skips</span><br></pre></td></tr></table></figure>
<p>在残差层的帮助下，我们可以很简单的定义这两个块。需要注意的是：</p>
<ul>
<li>我们使用python风格的列表skips来实现两个块之间的信息共享，这个列表作为输入x的一部分，输入这两个块；然后以返回值输出，尽管这不是必要的。</li>
<li>只有第一个残差块的输入和输出维度不同，我们需要单独定义它们。</li>
<li><code>UpBlock</code>中引入了一个额外的参数<code>skips_channels</code>，指的是<code>skips.pop()</code>的维度，用于计算残差层的真实输入维度。这同样是因为PyTorch需要输入输入的尺寸而引入的。</li>
</ul>
<h3 id="u-net">U-Net</h3>
<blockquote>
<p>在完成这些块的定义之后，我们定义最终的U-Net模型。</p>
</blockquote>
<p>回顾我们在之前的部分给出过的结构图</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_6.png" /></p>
<p>这个模型包括两个输入：</p>
<ul>
<li>Noise Variance</li>
<li>Noisy image</li>
</ul>
<p>前者经过编码和上采样，后者者经过一个卷积层，变为相同的尺寸。连接后经过三个<code>DownBlock</code>，两个<code>ResidualBlock</code>，三个<code>UpBlock</code>，最后经过一个卷积层得到最终的预测噪音。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(UNet, self).__init__()</span><br><span class="line">        self.image_size = image_size</span><br><span class="line">        <span class="comment"># Noisy Image经过的第一个卷积层</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Noise variance 经过的两个层</span></span><br><span class="line">        self.embedding = sinusoidal_embedding</span><br><span class="line">        self.upsample = nn.Upsample(size=self.image_size, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">        <span class="comment"># 三个DownBlock</span></span><br><span class="line">        self.down1 = DownBlock(<span class="number">64</span>, <span class="number">32</span>, block_depth=<span class="number">2</span>)</span><br><span class="line">        self.down2 = DownBlock(<span class="number">32</span>, <span class="number">64</span>, block_depth=<span class="number">2</span>)</span><br><span class="line">        self.down3 = DownBlock(<span class="number">64</span>, <span class="number">96</span>, block_depth=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 两个ResidualBlock</span></span><br><span class="line">        self.res1 = ResidualBlock(<span class="number">96</span>, <span class="number">128</span>)</span><br><span class="line">        self.res2 = ResidualBlock(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="comment"># 三个UpBlock</span></span><br><span class="line">        self.up1 = UpBlock(<span class="number">128</span>, <span class="number">96</span>, <span class="number">96</span>, block_depth=<span class="number">2</span>)</span><br><span class="line">        self.up2 = UpBlock(<span class="number">96</span>, <span class="number">64</span>, <span class="number">64</span>, block_depth=<span class="number">2</span>)</span><br><span class="line">        self.up3 = UpBlock(<span class="number">64</span>, <span class="number">32</span>, <span class="number">32</span>, block_depth=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 最后一个卷积层</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">3</span>, kernel_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>我们根据上图中提到的每一层的信息，在<code>__init__</code>中定义这些层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, noisy_images, noise_variances</span>):</span><br><span class="line">     x = self.conv1(noisy_images)</span><br><span class="line">     noise_embedding = self.embedding(noise_variances)</span><br><span class="line">     noise_embedding = F.interpolate(noise_embedding)</span><br><span class="line">     x = torch.cat([x, noise_embedding], dim=<span class="number">1</span>)</span><br><span class="line">     skips = []</span><br><span class="line">     x, skips = self.down1([x,skips])</span><br><span class="line">     x, skips = self.down2([x,skips])</span><br><span class="line">     x, skips = self.down3([x,skips])</span><br><span class="line">     x = self.res1(x)</span><br><span class="line">     x = self.res2(x)</span><br><span class="line">     x, skips = self.up1([x,skips])</span><br><span class="line">     x, skips = self.up2([x,skips])</span><br><span class="line">     x, skips = self.up3([x,skips])</span><br><span class="line">     x = self.conv2(x)</span><br><span class="line">     <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>得到完整的模型。</p>
<h2 id="训练模型">训练模型</h2>
<blockquote>
<p>接下来补充模型的训练过程</p>
</blockquote>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_10.png" /></p>
<p>这里值得注意的是，扩散模型实际上维护了网络的两个副本：</p>
<ul>
<li>使用梯度下降主动训练的</li>
<li>EMA 网络：先前训练步骤中的权重的指数移动平均值。</li>
</ul>
<p>EMA
网络不太容易受到训练过程中的短期波动和峰值的影响，这使得它比主动训练的网络更具有鲁棒性。因此，每当我们想要从网络生成生成的输出时，我们都会使用
EMA 网络。</p>
<h3 id="了解训练过程">了解训练过程</h3>
<p>训练过程如下，仅仅看训练过程的话并不复杂。这些过程被定义在<code>class DiffusionModel()</code>
中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self,dataloader</span>):</span><br><span class="line">        self.network.train()</span><br><span class="line">        self.ema_network.train()</span><br><span class="line">        self.normalizer.adapt(dataloader)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">            <span class="keyword">for</span> i, (images, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">                images = self.normalizer(images.to(device))</span><br><span class="line">                noises = torch.randn_like(images).to(device)</span><br><span class="line">                <span class="comment"># 获得扩散次数</span></span><br><span class="line">                diffusion_times = torch.rand((BATCH_SIZE,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)).to(device)</span><br><span class="line">                <span class="comment"># 获得噪音率和信号率</span></span><br><span class="line">                noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)</span><br><span class="line">                <span class="comment"># 获得加入噪音后的图像</span></span><br><span class="line">                noisy_images = images + noises * noise_rates</span><br><span class="line">                <span class="comment"># 获得去噪后的噪声和图像</span></span><br><span class="line">                pred_noises, pred_images = self.denoise(noisy_images, noise_rates, signal_rates, training=<span class="literal">True</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算损失</span></span><br><span class="line">                noise_loss = self.loss(pred_noises, noises)</span><br><span class="line">                <span class="comment"># 反向传播</span></span><br><span class="line">                self.optimizer.zero_grad()</span><br><span class="line">                noise_loss.backward()</span><br><span class="line">                self.optimizer.step()</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 更新EMA网络</span></span><br><span class="line">                <span class="keyword">for</span> param, ema_param <span class="keyword">in</span> <span class="built_in">zip</span>(self.network.parameters(), self.ema_network.parameters()):</span><br><span class="line">                    ema_param.data.mul_(EMA).add_((<span class="number">1</span> - EMA) * param.data)</span><br><span class="line">                    </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Batch <span class="subst">&#123;i&#125;</span>, loss: <span class="subst">&#123;noise_loss.item()&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                self.generate_print_image(<span class="number">10</span>,PLOT_DIFFUSION_STEPS)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> noise_loss</span><br></pre></td></tr></table></figure>
<h3 id="normalizer层">Normalizer层</h3>
<p><code>self.normalizer</code>
在PyTorch中并没有提供定义，按照tensorflow的文档的描述，可以近似实现这一层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">normalizer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(normalizer, self).__init__()</span><br><span class="line">        self.mean = torch.zeros(BATCH_SIZE, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">        self.std = torch.ones(BATCH_SIZE, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adapt</span>(<span class="params">self,dataloader</span>):</span><br><span class="line">        mean = []</span><br><span class="line">        std = []</span><br><span class="line">        <span class="keyword">for</span> i, (images, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            images = images.to(device)</span><br><span class="line">            mean.append( torch.mean(images, dim=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>], keepdim=<span class="literal">True</span>).to(device))</span><br><span class="line">            std.append( torch.std(images, dim=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>], keepdim=<span class="literal">True</span>).to(device))</span><br><span class="line">        self.mean = torch.mean(torch.cat(mean, dim=<span class="number">0</span>), dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).expand(BATCH_SIZE,-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">        self.std = torch.mean(torch.cat(std, dim=<span class="number">0</span>), dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).expand(BATCH_SIZE,-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> (x - self.mean) / (self.std + <span class="number">1e-8</span>)</span><br></pre></td></tr></table></figure>
<p>这个定义可以近似实现tensorflow的normalizer层的效果，但还存在一些差异。</p>
<h2 id="结果分析">结果分析</h2>
<p>原书给出了类似这样的训练结果，可见最终的图像已经比较清晰。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_11.png" /></p>
<aside>
<p>💡
但我使用PyTorch实现的程序不能达到这样的效果。具体原因尚未排查出。目前看来有两个主要问题：</p>
<ul>
<li>其一是在epoch数比较低的时候学习的很慢，不能像上图所示在epoch =
6时就给出可以分辨的图像；</li>
<li>其二是在epoch数比较高时会给出纯黑的图像。</li>
</ul>
<p>目前在训练过程中观察到的问题可能包括loss在一定epoch后就不再有明显下降。</p>
<p>未来有时间应该逐步排查以下问题：</p>
<ul>
<li>数据是否被正确加载</li>
<li>尝试将输入图像归一化后反归一化，已验证归一化层知否正确实现</li>
<li>黑色图像是如何产生的</li>
<li>代码是否存在错误的实现</li>
</ul>
<p>如果在实现本身没有问题，那问题可能出在超参数上的不匹配，或者PyTorch和Tensorflow对于一些函数实现上的差异。</p>
</aside>
<h3 id="调整扩散步数">调整扩散步数</h3>
<p>可以尝试在生成图像时使用不同的扩散步数，从下图的结果可以观察到，大约20步之后的步骤增加对图像质量影响不大。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_12.png" /></p>
<h3 id="图像间的插值">图像间的插值</h3>
<p>与VAE类似，也可以使用插值的方式使生成的花朵在图像间过度。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FGenerative_8%2FUntitled_13.png" /></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn">Raphael Hyaan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn/2024/01/20/Generative_8/">http://raphaelhyaan.cn/2024/01/20/Generative_8/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://raphaelhyaan.cn" target="_blank">Raphael's Home</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/20/Generative_9/" title="Chapter 9 Transformers"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Chapter 9 Transformers</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/20/Generative_7/" title="Chapter 7 Energy-Based Models 基于能量的模型"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Chapter 7 Energy-Based Models 基于能量的模型</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/11/22/DCGAN-1/" title="DCGAN"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FDCGAN%2FUntitled.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-22</div><div class="title">DCGAN</div></div></a></div><div><a href="/2024/01/20/Generative/" title="生成式神经网络总览"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">生成式神经网络总览</div></div></a></div><div><a href="/2024/01/20/Generative_1/" title="Chapter 1 Generative Modeling"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 1 Generative Modeling</div></div></a></div><div><a href="/2024/01/20/Generative_10/" title="Chapter 10 Advanced GANs 各种各样的GAN"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 10 Advanced GANs 各种各样的GAN</div></div></a></div><div><a href="/2024/01/20/Generative_2/" title="Chapter 2 Deep Learning"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 2 Deep Learning</div></div></a></div><div><a href="/2024/01/20/Generative_3/" title="Chapter 3 Variational Autoencoders 自动变分编码器"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 3 Variational Autoencoders 自动变分编码器</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Fe747243b42e6168d02fdf8bccbbd4ea.jpg" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="author-info__name">Raphael Hyaan</div><div class="author-info__description">何日可谓归去来</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">75</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/raphaelhyaan"><i class="fab fa-github"></i><span>Bonjour</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/raphaelhyaan" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:raphael.ma.yuhan@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">仰观宇宙之大，俯察品类之盛</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#chapter-8-diffusion-models-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">Chapter 8 Diffusion Models
扩散模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E6%95%A3%E7%94%B5%E8%A7%86"><span class="toc-number">1.1.1.</span> <span class="toc-text">扩散电视</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#denoising-diffusion-models-ddm"><span class="toc-number">2.</span> <span class="toc-text">Denoising Diffusion
Models (DDM)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#the-flowers-dataset"><span class="toc-number">2.1.</span> <span class="toc-text">The Flowers Dataset</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E6%89%A9%E6%95%A3%E8%BF%87%E7%A8%8B"><span class="toc-number">2.2.</span> <span class="toc-text">前向扩散过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E6%89%A9%E6%95%A3%E5%8E%9F%E7%90%86"><span class="toc-number">2.2.1.</span> <span class="toc-text">前向扩散原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96%E6%8A%80%E5%B7%A7-reparameterization-trick"><span class="toc-number">2.2.2.</span> <span class="toc-text">重参数化技巧
Reparameterization Trick</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E6%95%A3%E6%96%B9%E6%A1%88"><span class="toc-number">2.2.3.</span> <span class="toc-text">扩散方案</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E6%89%A9%E6%95%A3%E8%BF%87%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">反向扩散过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%99%AA%E9%9F%B3%E6%B6%88%E9%99%A4%E8%BF%87%E7%A8%8B"><span class="toc-number">2.3.1.</span> <span class="toc-text">噪音消除过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="toc-number">2.3.2.</span> <span class="toc-text">反向传播过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="toc-number">2.3.3.</span> <span class="toc-text">图像生成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-u-net-denoising-model"><span class="toc-number">2.4.</span> <span class="toc-text">The U-Net Denoising
Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%BC%A6%E7%BC%96%E7%A0%81%E5%99%A8-sinusoidal_embedding"><span class="toc-number">2.4.1.</span> <span class="toc-text">正弦编码器
sinusoidal_embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E5%9D%97-residualblock"><span class="toc-number">2.4.2.</span> <span class="toc-text">残差块 ResidualBlock</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E9%87%87%E6%A0%B7%E5%92%8C%E4%B8%8A%E9%87%87%E6%A0%B7%E5%9D%97-downblock-and-upblock"><span class="toc-number">2.4.3.</span> <span class="toc-text">下采样和上采样块
DownBlock and UpBlock</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#u-net"><span class="toc-number">2.4.4.</span> <span class="toc-text">U-Net</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.5.</span> <span class="toc-text">训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%86%E8%A7%A3%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">2.5.1.</span> <span class="toc-text">了解训练过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#normalizer%E5%B1%82"><span class="toc-number">2.5.2.</span> <span class="toc-text">Normalizer层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">2.6.</span> <span class="toc-text">结果分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E6%89%A9%E6%95%A3%E6%AD%A5%E6%95%B0"><span class="toc-number">2.6.1.</span> <span class="toc-text">调整扩散步数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E9%97%B4%E7%9A%84%E6%8F%92%E5%80%BC"><span class="toc-number">2.6.2.</span> <span class="toc-text">图像间的插值</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/04/01/crs-3/" title="CRS III 结构的离散分析 Discrétisation des structures"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fcrs.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="CRS III 结构的离散分析 Discrétisation des structures"/></a><div class="content"><a class="title" href="/2024/04/01/crs-3/" title="CRS III 结构的离散分析 Discrétisation des structures">CRS III 结构的离散分析 Discrétisation des structures</a><time datetime="2024-04-01T12:48:00.000Z" title="发表于 2024-04-01 20:48:00">2024-04-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/04/01/crs-2/" title="CRS II 结构模态计算  Calcul modal des structures"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fcrs.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="CRS II 结构模态计算  Calcul modal des structures"/></a><div class="content"><a class="title" href="/2024/04/01/crs-2/" title="CRS II 结构模态计算  Calcul modal des structures">CRS II 结构模态计算  Calcul modal des structures</a><time datetime="2024-04-01T08:11:45.000Z" title="发表于 2024-04-01 16:11:45">2024-04-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/31/crs-1/" title="CRS I 结构动力学问题的表述：强表述和弱表述 Formulation du problème en dynamique des structures ： formulation forte / formulation faible"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fcrs.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="CRS I 结构动力学问题的表述：强表述和弱表述 Formulation du problème en dynamique des structures ： formulation forte / formulation faible"/></a><div class="content"><a class="title" href="/2024/03/31/crs-1/" title="CRS I 结构动力学问题的表述：强表述和弱表述 Formulation du problème en dynamique des structures ： formulation forte / formulation faible">CRS I 结构动力学问题的表述：强表述和弱表述 Formulation du problème en dynamique des structures ： formulation forte / formulation faible</a><time datetime="2024-03-31T04:02:57.000Z" title="发表于 2024-03-31 12:02:57">2024-03-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/16/system-logique-6/" title="基本简化技术：代数法和卡诺法 Techniques de simplification élémentaires ： méthode algébrique et méthode de Karnaugh"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fsystemlogique.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="基本简化技术：代数法和卡诺法 Techniques de simplification élémentaires ： méthode algébrique et méthode de Karnaugh"/></a><div class="content"><a class="title" href="/2024/03/16/system-logique-6/" title="基本简化技术：代数法和卡诺法 Techniques de simplification élémentaires ： méthode algébrique et méthode de Karnaugh">基本简化技术：代数法和卡诺法 Techniques de simplification élémentaires ： méthode algébrique et méthode de Karnaugh</a><time datetime="2024-03-16T10:50:10.000Z" title="发表于 2024-03-16 18:50:10">2024-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/16/system-logique-5/" title="布尔函数规范、真值表和卡诺表 Spécification d’une fonction booléenne, table de vérité et tableau de Karnaugh"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fsystemlogique.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="布尔函数规范、真值表和卡诺表 Spécification d’une fonction booléenne, table de vérité et tableau de Karnaugh"/></a><div class="content"><a class="title" href="/2024/03/16/system-logique-5/" title="布尔函数规范、真值表和卡诺表 Spécification d’une fonction booléenne, table de vérité et tableau de Karnaugh">布尔函数规范、真值表和卡诺表 Spécification d’une fonction booléenne, table de vérité et tableau de Karnaugh</a><time datetime="2024-03-16T10:50:09.000Z" title="发表于 2024-03-16 18:50:09">2024-03-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Raphael Hyaan</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>