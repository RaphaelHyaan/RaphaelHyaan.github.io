<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer01 gpt模型的原理与实现 | Raphael's Home</title><meta name="author" content="Raphael Hyaan"><meta name="copyright" content="Raphael Hyaan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Transformer01 gpt模型的原理与实现 从RNN到Transformer  2017年，谷歌的研究人员发表了一篇论文，提出了一种用于序列建模的新型神经网络架构，称为Transformer。这种架构在机器翻译任务中，在翻译质量和训练成本方面都优于循环神经网络（RNN）。 与此同时，一种名为ULMFiT的有效迁移学习方法展示了在一个非常大且多样的语料库上训练长短期记忆（LSTM）">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer01 gpt模型的原理与实现">
<meta property="og:url" content="http://raphaelhyaan.cn/2024/12/04/informatique/deeplearning/Transformer-01/index.html">
<meta property="og:site_name" content="Raphael&#39;s Home">
<meta property="og:description" content="Transformer01 gpt模型的原理与实现 从RNN到Transformer  2017年，谷歌的研究人员发表了一篇论文，提出了一种用于序列建模的新型神经网络架构，称为Transformer。这种架构在机器翻译任务中，在翻译质量和训练成本方面都优于循环神经网络（RNN）。 与此同时，一种名为ULMFiT的有效迁移学习方法展示了在一个非常大且多样的语料库上训练长短期记忆（LSTM）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F%E8%9D%B61.png">
<meta property="article:published_time" content="2024-12-04T12:33:01.000Z">
<meta property="article:modified_time" content="2025-02-11T00:24:25.798Z">
<meta property="article:author" content="Raphael Hyaan">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F%E8%9D%B61.png"><link rel="shortcut icon" href="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Flana cat 002.png"><link rel="canonical" href="http://raphaelhyaan.cn/2024/12/04/informatique/deeplearning/Transformer-01/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer01 gpt模型的原理与实现',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-02-11 08:24:25'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/butterflyChange/css/code.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-2.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">186</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/Reading/"><i class="fa-fw fas fa-book"></i><span> Reading</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F蝶1.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Raphael's Home"><span class="site-name">Raphael's Home</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/Reading/"><i class="fa-fw fas fa-book"></i><span> Reading</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/Video/"><i class="fa-fw fas fa-video"></i><span> Video</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer01 gpt模型的原理与实现</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-04T12:33:01.000Z" title="发表于 2024-12-04 20:33:01">2024-12-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-11T00:24:25.798Z" title="更新于 2025-02-11 08:24:25">2025-02-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%94%9F%E6%88%90%E5%BC%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">生成式神经网络</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer01 gpt模型的原理与实现"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="transformer01-gpt模型的原理与实现">Transformer01
gpt模型的原理与实现</h1>
<h2 id="从rnn到transformer">从RNN到Transformer</h2>
<hr />
<p>2017年，谷歌的研究人员发表了一篇论文，提出了一种用于序列建模的新型神经网络架构，称为Transformer。这种架构在机器翻译任务中，在翻译质量和训练成本方面都优于循环神经网络（RNN）。</p>
<p>与此同时，一种名为ULMFiT的有效迁移学习方法展示了在一个非常大且多样的语料库上训练长短期记忆（LSTM）网络，能够在仅有少量标注数据的情况下生成最先进的文本分类器。</p>
<p>这些进展成为了当今两个最著名的Transformer模型的催化剂：生成式预训练Transformer（GPT）和双向编码器表示Transformer（BERT）。通过将Transformer架构与无监督学习相结合，这些模型消除了从头开始训练特定任务架构的需求，并在几乎所有的自然语言处理基准</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage.png" /></p>
<p>Transformer的诞生依赖于三项技术的先后开创：</p>
<ul>
<li>编码器-解码器架构</li>
<li>注意力机制</li>
<li>迁移学习</li>
</ul>
<h3 id="编码器-解码器架构-encoder-decoder-framework">编码器-解码器架构
Encoder-Decoder Framework</h3>
<hr />
<p>编码器-解码器架构早在循环神经网络中iu已经得到广泛应用。循环神经网络是一种“有记忆的”网络。具体的内容和实现方式可以参照之前的生成式神经网络笔记的相应部分。简单来说，这是一种满足<span
class="math inline">\(y_i =
f(x_i,y_{i-1})\)</span>的神经网络结构，最终的输出<span
class="math inline">\(y_t\)</span>理论上能包含所有输入的信息，称为最后的隐藏状态。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_1.png" /></p>
<p>这个过程将输入信息转化为一个向量，可以将其称之为编码。</p>
<p>用相同的方式，将包含所有信息的最后的隐藏状态进行逐步解码，即可实现如翻译等自然语言处理任务。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_2.png" /></p>
<p>但是，在这样的做法中，由于只有最终隐藏状态被解码器获值，可能会造成部分内容在压缩过程中的丢失。</p>
<h3 id="注意力机制">注意力机制</h3>
<hr />
<p>注意力机制是解决上述问题的一个“显而易见的方法”。其核心思想是，将编码器每一步生成的状态都作为输入给予解码器，并为每一个状态赋予相应的权重。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_3.png" /></p>
<p>如此训练的注意力模型应该可以理解输入与生成的复杂对应关系，比如一个翻译方面的例子：</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_4.png" /></p>
<p>一个合格的注意力块正确的给认识了法语和英语在形容词位置上的差异。但是，我们可能会注意到这样做的一种不合理之处：例如对于State
4，注意力衡量的并非！的对结果的影响大小，而是前四个单词提供的所有信息的影响。这显然会造成每一个state中包含的信息是不平等的。同时，计算本质上是顺序进行的，无法在输入序列上并行化处理。</p>
<p>Transformer引入了一种新的建模范式：完全放弃循环结构，转而完全依赖一种特殊形式的注意力机制，称为<strong>自注意力机制</strong>。在这里我们先不详细涉及这种机制的具体实现，首先关注其原理。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_5.png" /></p>
<p>上图展现了这种模型的工作原理。首先，编码器注意力头会将输入信息转化为多个状态，每个状态都是平等的，他们拥有对输入的不同权重分配和一个线性神经网络用于进一步的处理。然后，解码器再根据不同的任务（如对单词Transformer的翻译）决定不同状态的权重，并再次经过线性神经网络得到输出。</p>
<h3 id="迁移学习-transfer-learning">迁移学习 Transfer Learning</h3>
<hr />
<p>迁移学习原理十分简单，即训练一个极其泛化的模型，然后为使用这个模型进行进一步的后续任务。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_6.png" /></p>
<p>具体来说，在早期的迁移学习模型c中，作者提出通用的迁移学习狂降，将训练大致分为三个部分：</p>
<ul>
<li>预训练
Pretraining：对于ULMFiT，预训练进行根据前面的词预测下一个词的简单任务，称之为语言建模。在这个过程中进行无监督训练。</li>
<li>领域适应 Domain
adaption：将预训练完成的模型适用于领域内的语料库，依然使用语言建模。实际上对于较小的任务，这一步可能并非必要的。</li>
<li>微调
Fine-tuning：通过监督学习，为语言模型增加一个任务层，以实行目标任务。</li>
</ul>
<h1 id="数据">数据</h1>
<hr />
<h2 id="字符编码-token-encoding">字符编码 token encoding</h2>
<hr />
<p>相比于图像，文本并非由连续的数值表示。计算机显然不能直接识别字母，因此需要进行分词<strong>Tokenization</strong>。分词是将字符串拆解成模型使用的基本单位的步骤。另外，关于编码<code>encoding</code>，嵌入<code>embedding</code>
，本文认为：</p>
<ul>
<li><strong>Encoding</strong>
是一种更通用的表示，用于将输入数据转换为适合模型处理的某种内部表示。这种表示可能是稠密的或稀疏的，低维或高维的。<code>Encoding</code>可以包括<code>Embedding</code>
。</li>
<li><strong>Embedding</strong>
是特定的编码方法，主要用于将离散的输入映射为低维稠密向量，目的是生成一种可训练的向量表示。</li>
</ul>
<h3 id="字符分词"><strong>字符分词</strong></h3>
<hr />
<p>字符分词是最简单的分词方式。这种方法将一句话分为由单个字符组成的数组，如对于输入<code>"Tokenizing text is a core task of NLP."</code>
，分词后的结果是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;Tokenizing text is a core task of NLP.&quot;</span></span><br><span class="line">tokenized_text = <span class="built_in">list</span>(text)</span><br><span class="line">---`</span><br><span class="line">[<span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;N&#x27;</span>, <span class="string">&#x27;L&#x27;</span>, <span class="string">&#x27;P&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]`</span><br></pre></td></tr></table></figure>
<p>然后为每一个字符分配一个唯一的整数。如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">token2idx = &#123;ch: idx <span class="keyword">for</span> idx, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">sorted</span>(<span class="built_in">set</span>(tokenized_text)))&#125;</span><br><span class="line">---</span><br><span class="line">&#123;<span class="string">&#x27; &#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;L&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;N&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;P&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;T&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;e&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;f&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;g&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;k&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;n&#x27;</span>: <span class="number">13</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;r&#x27;</span>: <span class="number">15</span>, <span class="string">&#x27;s&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;t&#x27;</span>: <span class="number">17</span>, <span class="string">&#x27;x&#x27;</span>: <span class="number">18</span>, <span class="string">&#x27;z&#x27;</span>: <span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure>
<p>如此，输入转化为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_ids = [token2idx[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokenized_text]</span><br><span class="line">---</span><br><span class="line">[<span class="number">5</span>, <span class="number">14</span>, <span class="number">12</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">11</span>, <span class="number">19</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">10</span>, <span class="number">0</span>, <span class="number">17</span>, <span class="number">8</span>, <span class="number">18</span>, <span class="number">17</span>, <span class="number">0</span>, <span class="number">11</span>, <span class="number">16</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">17</span>, <span class="number">6</span>, <span class="number">16</span>, <span class="number">12</span>, <span class="number">0</span>, <span class="number">14</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>然后，可以将输入转化为独热向量<code>one-hot vectors</code>
，这种表示方法中，每一个与字符集大小等长的向量代表一个字符，而向量中只有字符对应的整数为1，其余为0：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">input_ids = torch.tensor(input_ids)</span><br><span class="line">one_hot_encodings = F.one_hot(input_ids, num_classes=<span class="built_in">len</span>(token2idx))</span><br></pre></td></tr></table></figure>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_7.png" /></p>
<p>字符分词忽略了文本中的任何结构，把整个字符串视为字符流。虽然这对处理拼写错误和稀有词有帮助，但主要缺点是需要从数据中学习像词这样的语言结构。这需要大量的计算资源、内存和数据，因此字符分词在实践中很少使用。相反，在分词过程中保留一些文本结构是更常见的做法。</p>
<p>另外，对于中文语料，字符分词会更加常见一点。一方面中文中的字本身就是类似词的语言结构；另一方面，对中文的单词分词过于困难。</p>
<h3 id="单词分词"><strong>单词分词</strong></h3>
<hr />
<p>顾名思义，单词分词简单的方法就是使用空格来分割文本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenized_text = text.split()</span><br><span class="line">---</span><br><span class="line">[<span class="string">&#x27;Tokenizing&#x27;</span>, <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;core&#x27;</span>, <span class="string">&#x27;task&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;NLP&#x27;</span>，<span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>然后，像字符分词一样，将每个单词映射到一个唯一的ID。这样做有许多潜在问题：</p>
<ul>
<li>显而易见的，词汇表大小很容易极其庞大</li>
<li>在分词时，<code>'.'</code>
被认为与单词等价，一些词级分词器会对标点符号有额外的规则处理。</li>
<li>单词可能由各种变形，可能需要应用词干提取或词形还原，将单词归一化为词干（例如，“great”、“greater”和“greatest”都会变成“great”）</li>
</ul>
<p>关于大词汇表，有两种常见的解决方案：</p>
<ul>
<li>假设我们有100万个唯一单词，我们会在神经网络的第一层将这100万维的输入向量压缩为1000维向量。这是大多数NLP架构中的标准步骤，这一层的权重矩阵大小将是100万
× 1000 = 10亿个权重。但这种方法依然过于浪费参数。</li>
<li>另一个常见的做法是限制词汇表的大小，例如只考虑语料库中最常见的10万个单词。词汇表之外的单词被分类为“未知”并映射到一个共享的UNK（未知）token。但会导致部分信息的丢失。</li>
</ul>
<hr />
<p><strong>子词分词</strong></p>
<p>子词分词的核心思想是将常见的高频词保持完整，而低频词或复杂单词被拆分为多个子词。NLP中常用的几种子词分词算法包括WordPiece、Byte
Pair Encoding（BPE）等，这里以前者为例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">model_ckpt = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_ckpt)</span><br><span class="line">encoded_text = tokenizer(text)</span><br><span class="line">okens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)</span><br><span class="line">---</span><br><span class="line"> [<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;token&#x27;</span>, <span class="string">&#x27;##izing&#x27;</span>, <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;core&#x27;</span>, <span class="string">&#x27;task&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;nl&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##p&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>观察上述例子，可见：</p>
<ul>
<li>在序列的开头和结尾分别添加了特殊的<code>[CLS]</code>和<code>[SEP]</code>
token，用于指示序列的开始和结束。</li>
<li>每个token都被转为了小写，这是这个具体模型检查点的一个特性。</li>
<li>像“tokenizing”和“NLP”这样的单词被拆分成了两个token，这是因为它们不是常见词汇。子词前缀<code>##</code>表明该token不是以空格开头，表示它需要与前面的token合并回字符串。</li>
</ul>
<h2 id="代码实现数据库">代码实现：数据库</h2>
<hr />
<p>为了避免可能的错误，在本章中使用原书使用的红酒评论数据库。在后续的笔记中可能会尝试使用更多其他的数据库。</p>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/zynicide/wine-reviews">Wine
Reviews</a></p>
<h3 id="dataset">Dataset</h3>
<hr />
<p>本笔记的重点不在于编程，所以代码部分会简略。代码如下：</p>
<ul>
<li><p><strong>WineDataset</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> vocab</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WineDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path, max_len, min_freq=<span class="number">100</span></span>):</span><br><span class="line">        self.data_path = data_path</span><br><span class="line">        self.filtered_data = <span class="literal">None</span></span><br><span class="line">        self.vocab = <span class="literal">None</span></span><br><span class="line">        self.max_len = max_len</span><br><span class="line">        self.min_freq = min_freq</span><br><span class="line">        self.load_data()</span><br><span class="line">        self.filter_data()</span><br><span class="line">        self.data_preprocessing()</span><br><span class="line">        self.transforme()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.filtered_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        input_ids = self.tokens[idx]</span><br><span class="line">        target = self.targets[idx]</span><br><span class="line">        <span class="keyword">return</span> input_ids, target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transforme</span>(<span class="params">self</span>):</span><br><span class="line">        self.tokens = []</span><br><span class="line">        self.targets = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> self.filtered_data:</span><br><span class="line">            tokens = (</span><br><span class="line">                [self.vocab[<span class="string">&quot;&lt;bos&gt;&quot;</span>]]</span><br><span class="line">                + [self.vocab[token] <span class="keyword">for</span> token <span class="keyword">in</span> text]</span><br><span class="line">                + [self.vocab[<span class="string">&quot;&lt;eos&gt;&quot;</span>]]</span><br><span class="line">            )</span><br><span class="line">            tokens = tokens[: self.max_len]</span><br><span class="line">            tokens += [self.vocab[<span class="string">&quot;&lt;pad&gt;&quot;</span>]] * (self.max_len - <span class="built_in">len</span>(tokens))</span><br><span class="line">            input_ids = torch.tensor(tokens[:-<span class="number">1</span>])</span><br><span class="line">            target = torch.tensor(tokens[<span class="number">1</span>:])</span><br><span class="line">            self.tokens.append(input_ids)</span><br><span class="line">            self.targets.append(target)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.data_path, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data = json.load(f)</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        确保 country（国家）、province（省份）、variety（葡萄品种）以及 description（描述）字段都不为空</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.filtered_data = [</span><br><span class="line">            <span class="string">&quot;wine review : &quot;</span></span><br><span class="line">            + x[<span class="string">&quot;country&quot;</span>]</span><br><span class="line">            + <span class="string">&quot; : &quot;</span></span><br><span class="line">            + x[<span class="string">&quot;province&quot;</span>]</span><br><span class="line">            + <span class="string">&quot; : &quot;</span></span><br><span class="line">            + x[<span class="string">&quot;variety&quot;</span>]</span><br><span class="line">            + <span class="string">&quot; : &quot;</span></span><br><span class="line">            + x[<span class="string">&quot;description&quot;</span>]</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> self.data</span><br><span class="line">            <span class="keyword">if</span> x[<span class="string">&quot;country&quot;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> x[<span class="string">&quot;province&quot;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> x[<span class="string">&quot;variety&quot;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> x[<span class="string">&quot;description&quot;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        ]</span><br><span class="line">        n_wines = <span class="built_in">len</span>(self.filtered_data)</span><br><span class="line">        examples = self.filtered_data[<span class="number">25</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;n_wines&#125;</span> recipes loaded, e.g. <span class="subst">&#123;examples&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pad_punctuation</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        用空格替换标点符号</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        s = re.sub(<span class="string">f&quot;[<span class="subst">&#123;re.escape(string.punctuation)&#125;</span>]&quot;</span>, <span class="string">&quot; &quot;</span>, text)</span><br><span class="line">        s = re.sub(<span class="string">&quot; +&quot;</span>, <span class="string">&quot; &quot;</span>, s)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data_preprocessing</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        对数据进行预处理</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.filtered_data = [</span><br><span class="line">            self.pad_punctuation(x).lower() <span class="keyword">for</span> x <span class="keyword">in</span> self.filtered_data</span><br><span class="line">        ]</span><br><span class="line">        self.filtered_data = [x.split() <span class="keyword">for</span> x <span class="keyword">in</span> self.filtered_data]</span><br><span class="line">        <span class="comment"># wnl = WordNetLemmatizer()</span></span><br><span class="line">        <span class="comment"># self.filtered_data = [[wnl.lemmatize(word) for word in x] for x in self.filtered_data]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建词汇表，将词转换为词频</span></span><br><span class="line">        counter = Counter()</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> self.filtered_data:</span><br><span class="line">            counter.update(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 手动将特殊标记加入 Counter</span></span><br><span class="line">        special_tokens = [<span class="string">&quot;&lt;unk&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;&lt;bos&gt;&quot;</span>, <span class="string">&quot;&lt;eos&gt;&quot;</span>]</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> special_tokens:</span><br><span class="line">            counter[token] = <span class="built_in">float</span>(</span><br><span class="line">                <span class="string">&#x27;inf&#x27;</span></span><br><span class="line">            )  <span class="comment"># 确保这些特殊标记有最高的频率，从而排在词表最前面</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建词汇表, 创建四种特殊标记，分别是代表未知词的&lt;unk&gt;，填充标记&lt;pad&gt;，序列开始标记&lt;bos&gt;，序列结束标记&lt;eos&gt;</span></span><br><span class="line">        self.vocab = vocab(</span><br><span class="line">            counter,</span><br><span class="line">            min_freq=self.min_freq,</span><br><span class="line">            specials=[<span class="string">&quot;&lt;unk&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;&lt;bos&gt;&quot;</span>, <span class="string">&quot;&lt;eos&gt;&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 设置默认索引为未知词索引</span></span><br><span class="line">        self.vocab.set_default_index(self.vocab[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>具体来说，我定义了<code>WineDataset</code>类，这是一个继承自<code>torch.utils.data.Dataset</code>类的子类。除了<code>Dataset</code>要求的<code>__len__</code>和<code>__getitem__</code>之外，该类使用<code>load_data</code>从<code>json</code>文件中读取数据；使用<code>filter_data</code>筛选可靠的数据；使用<code>data_preprocessing</code>函数删除标点和多余的空格，并使用<code>Counter</code>类创建词汇频次表，使用<code>torchtext.vocab</code>的<code>vocab</code>函数创建词汇表【这里注意，使用的是函数<code>vocab</code>而不是类<code>Vocab</code>，尽管前者的返回值是Vocab类】，并设置特殊标记；最后使用<code>transfome</code>函数完成从句子向向量的转变。</p>
<p>最终的结果将类似下面给出的例子，这里展示了两组输入输出。可以看到每组输入都以<code>2：&lt;bos&gt;</code>开始，都以<code>3：&lt;eos&gt;</code>结束，所有的向量都由<code>1：&lt;pad&gt;</code>补充到了举例时设置的最大长度<code>100</code>：</p>
<ul>
<li><p>编码的结果</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">Inputs: tensor([[   <span class="number">2</span>,    <span class="number">4</span>,    <span class="number">5</span>,   <span class="number">66</span>,  <span class="number">636</span>,   <span class="number">89</span>,   <span class="number">36</span>,    <span class="number">4</span>,   <span class="number">37</span>,   <span class="number">40</span>,  <span class="number">670</span>,  <span class="number">528</span>,</span><br><span class="line">           <span class="number">41</span>,   <span class="number">37</span>,  <span class="number">834</span>,   <span class="number">78</span>,    <span class="number">0</span>, <span class="number">1803</span>,  <span class="number">194</span>, <span class="number">2145</span>,   <span class="number">19</span>,   <span class="number">11</span>,  <span class="number">839</span>,   <span class="number">99</span>,</span><br><span class="line">           <span class="number">73</span>, <span class="number">1122</span>,   <span class="number">16</span>,    <span class="number">0</span>,   <span class="number">19</span>,   <span class="number">72</span>,   <span class="number">48</span>,  <span class="number">518</span>,   <span class="number">16</span>, <span class="number">1780</span>,  <span class="number">497</span>,   <span class="number">55</span>,</span><br><span class="line">          <span class="number">563</span>,  <span class="number">593</span>,   <span class="number">73</span>, <span class="number">1557</span>,  <span class="number">157</span>,  <span class="number">642</span>,    <span class="number">0</span>,  <span class="number">179</span>,  <span class="number">194</span>, <span class="number">1189</span>,  <span class="number">956</span>,   <span class="number">55</span>,</span><br><span class="line">          <span class="number">882</span>,  <span class="number">602</span>,  <span class="number">883</span>,    <span class="number">3</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>],</span><br><span class="line">        [   <span class="number">2</span>,    <span class="number">4</span>,    <span class="number">5</span>,  <span class="number">170</span>,  <span class="number">659</span>,  <span class="number">659</span>,   <span class="number">10</span>,   <span class="number">64</span>,  <span class="number">194</span>, <span class="number">1093</span>,  <span class="number">145</span>,    <span class="number">0</span>,</span><br><span class="line">           <span class="number">36</span>,  <span class="number">415</span>,    <span class="number">0</span>,    <span class="number">0</span>,  <span class="number">533</span>,   <span class="number">64</span>,   <span class="number">40</span>,  <span class="number">273</span>,   <span class="number">73</span>,    <span class="number">0</span>,   <span class="number">36</span>, <span class="number">2215</span>,</span><br><span class="line">            <span class="number">4</span>,   <span class="number">37</span>,  <span class="number">169</span>,   <span class="number">51</span>,   <span class="number">40</span>,   <span class="number">82</span>,  <span class="number">444</span>,   <span class="number">16</span>,  <span class="number">376</span>,   <span class="number">31</span>,  <span class="number">679</span>,   <span class="number">16</span>,</span><br><span class="line">          <span class="number">149</span>,   <span class="number">73</span>,    <span class="number">9</span>,   <span class="number">14</span>,  <span class="number">258</span>,   <span class="number">19</span>,  <span class="number">108</span>,   <span class="number">55</span>,   <span class="number">56</span>,  <span class="number">353</span>,  <span class="number">105</span>,  <span class="number">197</span>,</span><br><span class="line">            <span class="number">3</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>]])</span><br><span class="line">Targets: tensor([[   <span class="number">4</span>,    <span class="number">5</span>,   <span class="number">66</span>,  <span class="number">636</span>,   <span class="number">89</span>,   <span class="number">36</span>,    <span class="number">4</span>,   <span class="number">37</span>,   <span class="number">40</span>,  <span class="number">670</span>,  <span class="number">528</span>,   <span class="number">41</span>,</span><br><span class="line">           <span class="number">37</span>,  <span class="number">834</span>,   <span class="number">78</span>,    <span class="number">0</span>, <span class="number">1803</span>,  <span class="number">194</span>, <span class="number">2145</span>,   <span class="number">19</span>,   <span class="number">11</span>,  <span class="number">839</span>,   <span class="number">99</span>,   <span class="number">73</span>,</span><br><span class="line">         <span class="number">1122</span>,   <span class="number">16</span>,    <span class="number">0</span>,   <span class="number">19</span>,   <span class="number">72</span>,   <span class="number">48</span>,  <span class="number">518</span>,   <span class="number">16</span>, <span class="number">1780</span>,  <span class="number">497</span>,   <span class="number">55</span>,  <span class="number">563</span>,</span><br><span class="line">          <span class="number">593</span>,   <span class="number">73</span>, <span class="number">1557</span>,  <span class="number">157</span>,  <span class="number">642</span>,    <span class="number">0</span>,  <span class="number">179</span>,  <span class="number">194</span>, <span class="number">1189</span>,  <span class="number">956</span>,   <span class="number">55</span>,  <span class="number">882</span>,</span><br><span class="line">          <span class="number">602</span>,  <span class="number">883</span>,    <span class="number">3</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>],</span><br><span class="line">        [   <span class="number">4</span>,    <span class="number">5</span>,  <span class="number">170</span>,  <span class="number">659</span>,  <span class="number">659</span>,   <span class="number">10</span>,   <span class="number">64</span>,  <span class="number">194</span>, <span class="number">1093</span>,  <span class="number">145</span>,    <span class="number">0</span>,   <span class="number">36</span>,</span><br><span class="line">          <span class="number">415</span>,    <span class="number">0</span>,    <span class="number">0</span>,  <span class="number">533</span>,   <span class="number">64</span>,   <span class="number">40</span>,  <span class="number">273</span>,   <span class="number">73</span>,    <span class="number">0</span>,   <span class="number">36</span>, <span class="number">2215</span>,    <span class="number">4</span>,</span><br><span class="line">           <span class="number">37</span>,  <span class="number">169</span>,   <span class="number">51</span>,   <span class="number">40</span>,   <span class="number">82</span>,  <span class="number">444</span>,   <span class="number">16</span>,  <span class="number">376</span>,   <span class="number">31</span>,  <span class="number">679</span>,   <span class="number">16</span>,  <span class="number">149</span>,</span><br><span class="line">           <span class="number">73</span>,    <span class="number">9</span>,   <span class="number">14</span>,  <span class="number">258</span>,   <span class="number">19</span>,  <span class="number">108</span>,   <span class="number">55</span>,   <span class="number">56</span>,  <span class="number">353</span>,  <span class="number">105</span>,  <span class="number">197</span>,    <span class="number">3</span>,</span><br><span class="line">				    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,</span><br><span class="line">            <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>]])</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>此外，如以下例子，直接应用词汇表将被映射为第二行的数字，<code>brimstone</code>
一词被映射为<code>0：&lt;unk&gt;</code>，这是因为在常见词汇表时设置了最低频率为<code>100</code>
，这将有效减少词汇表。事实上，当<code>min_freq</code>设置为1时，词汇表大小为35972；当<code>min_freq</code>设置为10时，词大小为9390；设置为100时将减少到3041。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;wine&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;italy&#x27;</span>, <span class="string">&#x27;sicily&#x27;</span>, <span class="string">&#x27;sardinia&#x27;</span>, <span class="string">&#x27;white&#x27;</span>, <span class="string">&#x27;blend&#x27;</span>, <span class="string">&#x27;aromas&#x27;</span>, <span class="string">&#x27;include&#x27;</span>, <span class="string">&#x27;tropical&#x27;</span>, <span class="string">&#x27;fruit&#x27;</span>, <span class="string">&#x27;broom&#x27;</span>, <span class="string">&#x27;brimstone&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;dried&#x27;</span>, <span class="string">&#x27;herb&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;palate&#x27;</span>, <span class="string">&#x27;isn&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;overly&#x27;</span>, <span class="string">&#x27;expressive&#x27;</span>, <span class="string">&#x27;offering&#x27;</span>, <span class="string">&#x27;unripened&#x27;</span>, <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;citrus&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;dried&#x27;</span>, <span class="string">&#x27;sage&#x27;</span>, <span class="string">&#x27;alongside&#x27;</span>, <span class="string">&#x27;brisk&#x27;</span>, <span class="string">&#x27;acidity&#x27;</span>]</span><br><span class="line">[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">0</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">0</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>]</span><br></pre></td></tr></table></figure>
<p>需要注意的是，如此得到的结果被称为<code>encoding</code>
编码而不是<code>embedding</code>
嵌入，而模型接受的输入是后者。可以使用<code>nn.Embedding</code>
完成前者向后者的转换。这一点在之后还会提到。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_8.png" /></p>
<h2 id="位置编码-positional-encoding">位置编码 Positional Encoding</h2>
<hr />
<p>注意力机制中，所有的键和查询之间是并行计算的，并不像循环神经网络那样从数据处理方式上体现了不同语速出现的次序。这会导致如下问题：</p>
<ul>
<li>狗看着男孩然后……（吠叫？）</li>
<li>男孩看着狗然后……（微笑？）</li>
</ul>
<p>为此，我们将每个token的位置也进行编码，与字符编码相加后输入后续的神经网络。</p>
<p>位置编码存在许多不同的的方式，Transformer中采用了被称为正弦曲线位置编码的一种绝对位置编码方式，GPT则采用了可学习位置编码，同样为绝对位置编码。除此之外还有相对位置编码和旋转位置编码。在本节中将讨论较为简单的前两种绝对编码，其他编码方式可能会在后续补充。</p>
<h3
id="正弦曲线位置编码-sinusoidal-positional-encoding">正弦曲线位置编码
<strong>Sinusoidal Positional Encoding</strong></h3>
<hr />
<p>这是《Attention Is All You
Need》中使用的编码方式，作者假设了这种编码方式可以使模型学到相对位置的信息。同时作者也尝试了可学习位置编码，并得到了几乎相同的结果。有着这种编码方式可以允许序列长度的外推，作者选择了这种方式。</p>
<p>在这种编码模式中，位置<code>pos</code> 和维度<code>i</code>
被考虑：</p>
<p><span class="math display">\[
\begin{aligned}P E_{(p o s, 2 i)} &amp; =\sin \left(p o s / 10000^{2 i /
d_{\text {model }}}\right) \\P E_{(p o s, 2 i+1)} &amp; =\cos \left(p o
s / 10000^{2 i / d_{\text {model }}}\right)\end{aligned}
\]</span></p>
<p>由于使用了绝对编码，这种编码的实现完全可以不依赖于输入，十分容易实现：</p>
<ul>
<li><p><strong>SinusoidualPositionEncoding</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SinusoidualPositionEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, max_len=<span class="number">512</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SinusoidualPositionEncoding, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.max_len = max_len</span><br><span class="line"></span><br><span class="line">        position_encoding_table = torch.zeros(max_len, embed_dim)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, embed_dim, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / embed_dim))</span><br><span class="line">        position_encoding_table[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        position_encoding_table[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        position_encoding_table = position_encoding_table.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;position_encoding_table&#x27;</span>, position_encoding_table)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size, seq_len, _ = x.size()</span><br><span class="line">        <span class="keyword">return</span> x + self.position_encoding_table[:,:seq_len, :]</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>使用随机生成的输入来测试代码，分别绘制输入，position_encoding_table和输出，结果如下：</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2FFigure_1.png" /></p>
<p>正弦曲线位置编码可以提供一定的相对位置信息。为了方便计算，考虑<code>embed_dim = 2</code>
的情况，此时<span class="math inline">\(i = 0\)</span>，有：</p>
<p><span class="math display">\[
\begin{aligned}P E_{(p o s, 0)} &amp; =\sin \left(p o s \right) \\P
E_{(p o s, 1)} &amp; =\cos \left(p o s\right)\end{aligned}
\]</span></p>
<p>根据和差化积公式，有：</p>
<p><span class="math display">\[
\begin{aligned}P E_{(p o s, 0)} &amp; =\sin \left(p o s \right)  =
sin(pos-k)cos(k)+cos(pos-k)sin(k)
\\P E_{(p o s, 1)} &amp; =\cos \left(p o s\right) =
cos(pos-k)cos(k)-sin(pos-k)sin(k)\end{aligned}
\]</span></p>
<p>即对于任意<span class="math inline">\(k&gt;0\)</span>，<span
class="math inline">\(PE_{pos}\)</span>和<span
class="math inline">\(PE_{pos-k}\)</span>始终存在相对位置关系：</p>
<p><span class="math display">\[
\binom{P E_{pos, 0}}{P E_{pos, 1}}=\left(\begin{array}{cc}\cos k &amp;
\sin k \\-\sin k &amp; \cos k\end{array}\right)\binom{P E_{pos-k, 0}}{P
E_{pos-k, 1}}
\]</span></p>
<h3 id="可学习的位置编码-learnable-positional-encoding">可学习的位置编码
<strong>Learnable Positional Encoding</strong></h3>
<hr />
<p>所谓可学习的位置编码，就是直接使用位置<code>pos</code>
作为位置信息输入，经过一个可学习的编码层，再与字符编码相加。下图展示了两种绝对编码的对比，前者的位置编码矩阵不会更新，因此可以提前计算，而后者对于输入的嵌入，需要生成其位置编码，然后经过处理转化为位置嵌入然后与字符嵌入相加。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_9.png" /></p>
<p>这种编码方式实现也非常简单。</p>
<ul>
<li><p><strong>LearnablePositionEncoding</strong></p>
<p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class LearnablePositionEncoding(nn.Module):</span><br><span class="line">    def __init__(self, embed_dim, max_len=512, device=&#x27;cpu&#x27;):</span><br><span class="line">        super(LearnablePositionEncoding, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.max_len = max_len</span><br><span class="line">        self.position_encoding_table = nn.Embedding(max_len, embed_dim).to(device)</span><br><span class="line">    </span><br><span class="line">    def forward(self, x, return_position_embedding=False):</span><br><span class="line">        batch_size, seq_len, _ = x.size()</span><br><span class="line">        positions = torch.arange(seq_len).long().to(x.device)</span><br><span class="line">        position_embeddings = self.position_encoding_table(positions)</span><br><span class="line">        if return_position_embedding:</span><br><span class="line">            return x + position_embeddings, position_embeddings</span><br><span class="line">        return x + position_embeddings</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h1 id="注意力机制-1">注意力机制</h1>
<hr />
<h2 id="注意力机制-attention">注意力机制 Attention</h2>
<hr />
<p>首先可以进行一个小的游戏</p>
<ul>
<li><p>小游戏的代码，可以尝试在本地运行。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pygame</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化Pygame</span></span><br><span class="line">pygame.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置屏幕大小</span></span><br><span class="line">screen_width, screen_height = <span class="number">900</span>, <span class="number">600</span></span><br><span class="line">screen = pygame.display.set_mode((screen_width, screen_height))</span><br><span class="line">pygame.display.set_caption(<span class="string">&#x27;猜单词游戏&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置字体和颜色</span></span><br><span class="line">font = pygame.font.Font(<span class="literal">None</span>, <span class="number">36</span>)</span><br><span class="line">large_font = pygame.font.Font(<span class="literal">None</span>, <span class="number">50</span>)</span><br><span class="line">white = (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>)</span><br><span class="line">black = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">gray = (<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>)</span><br><span class="line">blue = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">red = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">green = (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义句子和单词</span></span><br><span class="line">sentence = <span class="string">&quot;The pink elephant tried to get into the car but it was too&quot;</span></span><br><span class="line">words = sentence.split()</span><br><span class="line">num_words = <span class="built_in">len</span>(words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义掩蔽和点击次数</span></span><br><span class="line">masks = [<span class="literal">True</span>] * num_words</span><br><span class="line">click_counts = [<span class="number">0</span>] * num_words</span><br><span class="line"></span><br><span class="line"><span class="comment"># 掩蔽框的大小和位置</span></span><br><span class="line">mask_width = <span class="number">120</span></span><br><span class="line">mask_height = <span class="number">50</span></span><br><span class="line">padding_x = <span class="number">20</span></span><br><span class="line">padding_y = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算单词位置，自动换行</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_word_positions</span>():</span><br><span class="line">    positions = []</span><br><span class="line">    x_start, y_start = <span class="number">50</span>, <span class="number">50</span></span><br><span class="line">    x, y = x_start, y_start</span><br><span class="line">    max_width = screen_width - <span class="number">2</span> * x_start</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> x + mask_width &gt; max_width:</span><br><span class="line">            x = x_start</span><br><span class="line">            y += mask_height + padding_y</span><br><span class="line">        positions.append((x, y))</span><br><span class="line">        x += mask_width + padding_x</span><br><span class="line">    <span class="keyword">return</span> positions</span><br><span class="line"></span><br><span class="line">word_positions = calculate_word_positions()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入框</span></span><br><span class="line">input_box = pygame.Rect(<span class="number">50</span>, screen_height - <span class="number">100</span>, <span class="number">700</span>, <span class="number">50</span>)</span><br><span class="line">input_active = <span class="literal">False</span></span><br><span class="line">input_text = <span class="string">&quot;&quot;</span></span><br><span class="line">guess_word = <span class="string">&quot;big&quot;</span></span><br><span class="line">game_won = <span class="literal">False</span></span><br><span class="line">show_result = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录游戏轮数</span></span><br><span class="line">total_clicks = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示文本函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display_text</span>(<span class="params">text, font, color, position</span>):</span><br><span class="line">    text_surface = font.render(text, <span class="literal">True</span>, color)</span><br><span class="line">    screen.blit(text_surface, position)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示遮蔽或单词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display_words</span>():</span><br><span class="line">    <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(words):</span><br><span class="line">        x, y = word_positions[i]</span><br><span class="line">        <span class="keyword">if</span> masks[i]:</span><br><span class="line">            pygame.draw.rect(screen, gray, (x, y, mask_width, mask_height))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            display_text(word, font, black, (x + <span class="number">10</span>, y + <span class="number">10</span>))</span><br><span class="line">        <span class="comment"># 绘制单词边框</span></span><br><span class="line">        pygame.draw.rect(screen, black, (x, y, mask_width, mask_height), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 游戏主循环</span></span><br><span class="line">clock = pygame.time.Clock()</span><br><span class="line">running = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> running:</span><br><span class="line">    screen.fill(white)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 显示单词</span></span><br><span class="line">    display_words()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 显示输入框</span></span><br><span class="line">    <span class="keyword">if</span> input_active:</span><br><span class="line">        pygame.draw.rect(screen, blue, input_box, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pygame.draw.rect(screen, black, input_box, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 显示输入的文本</span></span><br><span class="line">    display_text(input_text, font, black, (input_box.x + <span class="number">10</span>, input_box.y + <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 显示提示信息</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> game_won <span class="keyword">and</span> <span class="keyword">not</span> show_result:</span><br><span class="line">        display_text(<span class="string">&quot;click these block to show the word behand it,guess the last word&quot;</span>, font, black, (<span class="number">50</span>, screen_height - <span class="number">150</span>))</span><br><span class="line">    <span class="keyword">if</span> game_won:</span><br><span class="line">        display_text(<span class="string">&quot;You win! R to play again or Q to quit this game&quot;</span>, large_font, green <span class="keyword">if</span> guess_word <span class="keyword">else</span> red, (<span class="number">200</span>, screen_height - <span class="number">200</span>))</span><br><span class="line">    <span class="keyword">if</span> show_result:</span><br><span class="line">        display_text(<span class="string">f&quot;total click times: <span class="subst">&#123;total_clicks&#125;</span>&quot;</span>, font, black, (<span class="number">50</span>, screen_height - <span class="number">200</span>))</span><br><span class="line">        <span class="keyword">for</span> i, count <span class="keyword">in</span> <span class="built_in">enumerate</span>(click_counts):</span><br><span class="line">            display_text(<span class="string">f&quot;word &#x27;<span class="subst">&#123;words[i]&#125;</span>&#x27; is clicked: <span class="subst">&#123;count&#125;</span>&quot;</span>, font, black, (<span class="number">50</span>, screen_height - <span class="number">170</span> + i * <span class="number">30</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 处理事件</span></span><br><span class="line">    <span class="keyword">for</span> event <span class="keyword">in</span> pygame.event.get():</span><br><span class="line">        <span class="keyword">if</span> event.<span class="built_in">type</span> == pygame.QUIT:</span><br><span class="line">            running = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> event.<span class="built_in">type</span> == pygame.MOUSEBUTTONDOWN:</span><br><span class="line">            mouse_x, mouse_y = event.pos</span><br><span class="line">            <span class="comment"># 检查是否点击在输入框内</span></span><br><span class="line">            <span class="keyword">if</span> input_box.collidepoint(mouse_x, mouse_y):</span><br><span class="line">                input_active = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                input_active = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> game_won <span class="keyword">and</span> <span class="keyword">not</span> show_result:</span><br><span class="line">                    <span class="comment"># 检查是否点击在某个遮蔽单词上</span></span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_words):</span><br><span class="line">                        x, y = word_positions[i]</span><br><span class="line">                        <span class="keyword">if</span> x &lt;= mouse_x &lt;= x + mask_width <span class="keyword">and</span> y &lt;= mouse_y &lt;= y + mask_height:</span><br><span class="line">                            <span class="keyword">if</span> masks[i]:</span><br><span class="line">                                masks[i] = <span class="literal">False</span></span><br><span class="line">                                click_counts[i] += <span class="number">1</span></span><br><span class="line">                                total_clicks += <span class="number">1</span></span><br><span class="line">                                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> event.<span class="built_in">type</span> == pygame.KEYDOWN:</span><br><span class="line">            <span class="keyword">if</span> input_active <span class="keyword">and</span> <span class="keyword">not</span> game_won <span class="keyword">and</span> <span class="keyword">not</span> show_result:</span><br><span class="line">                <span class="keyword">if</span> event.key == pygame.K_RETURN:</span><br><span class="line">                    <span class="keyword">if</span> input_text.strip().lower() == guess_word.lower():</span><br><span class="line">                        game_won = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        display_text(<span class="string">&quot;wrong guess again&quot;</span>, font, red, (<span class="number">50</span>, screen_height - <span class="number">60</span>))</span><br><span class="line">                        input_text = <span class="string">&quot;&quot;</span></span><br><span class="line">                <span class="keyword">elif</span> event.key == pygame.K_BACKSPACE:</span><br><span class="line">                    input_text = input_text[:-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 只允许输入字母</span></span><br><span class="line">                    <span class="keyword">if</span> event.unicode.isalpha():</span><br><span class="line">                        input_text += event.unicode</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> game_won:</span><br><span class="line">                <span class="keyword">if</span> event.key == pygame.K_r:</span><br><span class="line">                    <span class="comment"># 重新开始游戏</span></span><br><span class="line">                    masks = [<span class="literal">True</span>] * num_words</span><br><span class="line">                    click_counts = [<span class="number">0</span>] * num_words</span><br><span class="line">                    total_clicks = <span class="number">0</span></span><br><span class="line">                    input_text = <span class="string">&quot;&quot;</span></span><br><span class="line">                    game_won = <span class="literal">False</span></span><br><span class="line">                    show_result = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">elif</span> event.key == pygame.K_q:</span><br><span class="line">                    show_result = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    pygame.display.flip()</span><br><span class="line">    clock.tick(<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">pygame.quit()</span><br><span class="line">sys.exit()</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>这个游戏界面如下，可以通过点击小块来显示小块背后的单词，以帮助你猜测句子最后的单词。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_10.png" /></p>
<p>多次尝试或者与朋友一起尝试之后，你或许会发现：<strong>每个单词对于猜测出最后一个单词的作用并非等价的。</strong>如elephant和car明显比pink和the更有用。事实上，这就是注意力机制。Transformer
中的注意力机制（也称为注意力头）就是为了做到这一点而设计的。</p>
<p>它能够决定要从输入中的哪个位置提取信息，以便有效地提取有用的信息，而不会被不相关的细节所蒙蔽。这使得它对一系列环境具有高度适应性，因为它可以决定在推理时在哪里查找信息。</p>
<h3 id="查询键和值-queries-keys-and-values">查询、键和值 Queries, Keys,
and Values</h3>
<hr />
<p>为了完成这项任务，我们为每个单词赋予一个类似于“信心”的属性。使得elephant这个单词对自己更加自信，为下一个单词提供更多的信息；而was则对自己信心较少，为下一个单词提供更少的信息。</p>
<p>换句话说，我们可以将注意力头理解为一种信息查询机制。<code>下一个单词是什么</code>这个问题被引入到一个键值对存储系统。由查询<code>Q</code>与每一个键<code>K</code>的共振<code>*resonance*</code>决定权重，最终的预测结果为值<code>V</code>得加权求和。如下图所示。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2FUntitled.png" /></p>
<hr />
<p><strong>查询向量</strong><code>Q</code>是当前任务得一种表示。如在训练过程中，我们得任务就是简单的预测下一个单词。在上面得例子中，它是一个单词<code>too</code>，与其他单词得输入方式一致，在编码后传递给权重矩阵<span
class="math inline">\(W_Q\)</span>转换为一个<span
class="math inline">\(d_k\)</span>长度的向量<span
class="math inline">\(Q\)</span>。</p>
<p><strong>键向量</strong><code>K</code>是句子中每个单词的表示，在编码后经过权重矩阵<span
class="math inline">\(W_K\)</span>，所有的<code>K</code>都转换为一个<span
class="math inline">\(d_k\times \text{键的数量}\)</span>的矩阵<span
class="math inline">\(K\)</span>。</p>
<p>在注意力头中，使用点积计算权重；按照<span
class="math inline">\(d_k\)</span>缩放以保证方差稳定；最终经过<code>softmax</code>以保证总和为1：</p>
<p><span class="math display">\[
w_i = softmax(\frac{v_k\cdot v_q}{\sqrt{d_k}})；W_V=
softmax(\frac{Q\cdot K^T}{\sqrt{d_k}})
\]</span></p>
<p><strong>值向量</strong><code>V</code>也是句子中单词的表示，可以将它们视为每个单词的未加权贡献。也经过权重矩阵<span
class="math inline">\(W_V\)</span>转化为一个<span
class="math inline">\(d_v\times{值的数量}\)</span>的向量<span
class="math inline">\(V\)</span>。但是值向量不一定必须与键和查询具有相同的长度，只是为了简单起见，通常这样做。</p>
<p>由此，<strong>注意力<em>attention</em></strong>被定义为：</p>
<p><span class="math display">\[
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q
K^T}{\sqrt{d_k}}\right) V
\]</span></p>
<p>为了从注意力头获得最终的输出向量，注意力被求和以给出长度为 <span
class="math inline">\(d_v\)</span>
的向量。该上下文向量捕获了句子中单词的混合意见，以预测接下来的单词。</p>
<hr />
<p>我们给出一个这种计算的简略例子，以帮助在数学上的理解，如假设<span
class="math inline">\(Q，K，V\)</span>三个矩阵分别如下：</p>
<p><span class="math display">\[
Q=\left[\begin{array}{llll}
1 &amp; 0 &amp; 2 &amp; 1
\end{array}\right]，K=\left[\begin{array}{llll}
1 &amp; 0 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 1 &amp; 0 \\
2 &amp; 1 &amp; 0 &amp; 2 \\
1 &amp; 0 &amp; 2 &amp; 1
\end{array}\right]，V=\left[\begin{array}{lll}
1 &amp; 0 &amp; 2 \\
2 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 2 \\
1 &amp; 0 &amp; 1
\end{array}\right]
\]</span></p>
<p>首先计算<span class="math inline">\(Q K^T\)</span>：</p>
<p><span class="math display">\[
Q K^T=\left[\begin{array}{llll}
1 &amp; 0 &amp; 2 &amp; 1
\end{array}\right] \times\left[\begin{array}{llll}
1 &amp; 0 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 1 &amp; 0 \\
2 &amp; 1 &amp; 0 &amp; 2 \\
1 &amp; 0 &amp; 2 &amp; 1
\end{array}\right]=\left[\begin{array}{llll}
6 &amp; 2 &amp; 4 &amp; 6
\end{array}\right]
\]</span></p>
<p>然后进行缩放：</p>
<p><span class="math display">\[
\frac{Q K^T}{\sqrt{d_k}}=\frac{\left[\begin{array}{llll}
6 &amp; 2 &amp; 4 &amp; 6
\end{array}\right]}{2}=\left[\begin{array}{llll}
3 &amp; 1 &amp; 2 &amp; 3
\end{array}\right]
\]</span></p>
<p>并基于<code>softmax</code> 计算注意力权重：</p>
<p><span class="math display">\[
\operatorname{softmax}\left(\left[\begin{array}{llll}
3 &amp; 1 &amp; 2 &amp; 3
\end{array}\right]\right)=\left[\begin{array}{llll}
0.36 &amp; 0.05 &amp; 0.13 &amp; 0.36
\end{array}\right]
\]</span></p>
<p>最后计算值的加权和：</p>
<p><span class="math display">\[
\left[\begin{array}{llll}
0.36 &amp; 0.05 &amp; 0.13 &amp; 0.36
\end{array}\right] \times\left[\begin{array}{lll}
1 &amp; 0 &amp; 2 \\
2 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 2 \\
1 &amp; 0 &amp; 1
\end{array}\right] = \begin{array}{ccc}
{[1.18} &amp; 0.18 &amp; 1.34]
\end{array}
\]</span></p>
<p>因此最终的输出为：</p>
<p><span class="math display">\[
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q
K^T}{\sqrt{d_k}}\right) V = \begin{array}{ccc}
{[1.18} &amp; 0.18 &amp; 1.34]
\end{array}
\]</span></p>
<h3 id="多注意力头">多注意力头</h3>
<hr />
<p>Transformer架构的一个优势是能够同时在多个设备上并行计算，这很大程度上得益于多注意力头的机制。多注意力头结构如下，即每个注意力头给出自己的输出，然后将结果拼接并进行进一步的处理。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_11.png" /></p>
<h2 id="代码实现多注意力头">代码实现：多注意力头</h2>
<hr />
<h3 id="注意力头">注意力头</h3>
<hr />
<p>实现一个如上所述的注意力头十分简单，很容易发现，整个注意力头中可更新矩阵只包括三个矩阵，即三个线性层。因此我们可以实现以下代码：</p>
<p>我们定义三个函数：</p>
<ul>
<li><p><code>__init__</code>：用来初始化类，我们接受<code>embed_diam, v_dim=None, k_dim=None, out_proj=False, device="cpu"</code>
这些参数。类中包含三个或者四个线性层，取决于是否进行输出投影。</p></li>
<li><p><code>forward</code>：用于进行每次注意力计算，计算公式就是已经提到的注意力公式。掩蔽用于阻止某些单词的权重，在未来我们会解释。</p></li>
<li><p><code>to</code>
：实际上是一个很不负责任的函数，目的是替换注意力头所在的设备。</p></li>
<li><p><strong>AttentionHead</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionHead</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_diam, v_dim=<span class="literal">None</span>, k_dim=<span class="literal">None</span>, out_proj=<span class="literal">False</span>, device=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionHead, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> v_dim <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            v_dim = embed_diam</span><br><span class="line">        <span class="keyword">if</span> k_dim <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            k_dim = embed_diam</span><br><span class="line">        q_dim = k_dim</span><br><span class="line">        self.w_K = nn.Linear(embed_diam, k_dim).to(device)</span><br><span class="line">        self.w_v = nn.Linear(embed_diam, v_dim).to(device)</span><br><span class="line">        self.w_q = nn.Linear(embed_diam, q_dim).to(device)</span><br><span class="line">        self.softmax = F.softmax</span><br><span class="line">        self.embed_diam = embed_diam</span><br><span class="line">        self.v_dim = v_dim</span><br><span class="line">        self.q_dim = q_dim</span><br><span class="line">        self.k_dim = k_dim</span><br><span class="line">        self.if_out_proj = out_proj</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> out_proj:</span><br><span class="line">            self.out_proj = nn.Linear(</span><br><span class="line">                v_dim, embed_diam</span><br><span class="line">            ).to(device)  <span class="comment"># 为了保证输出的维度和输入的维度一致，所以有时需要一个线性变换</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, key, value, query, mask=<span class="literal">None</span></span>):</span><br><span class="line">        key = self.w_K(key)</span><br><span class="line">        value = self.w_v(value)</span><br><span class="line">        query = self.w_q(query)</span><br><span class="line">        scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / torch.sqrt(</span><br><span class="line">            torch.tensor(self.k_dim, dtype=torch.float32)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果应用掩蔽</span></span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, -torch.inf)</span><br><span class="line">        attention = self.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        attention = torch.matmul(attention, value)</span><br><span class="line">        <span class="keyword">if</span> self.if_out_proj:</span><br><span class="line">            attention = self.out_proj(attention)</span><br><span class="line">        <span class="keyword">return</span> attention</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to</span>(<span class="params">self,device</span>):</span><br><span class="line">        self.w_K.to(device)</span><br><span class="line">        self.w_v.to(device)</span><br><span class="line">        self.w_q.to(device)</span><br><span class="line">        <span class="keyword">if</span> self.if_out_proj:</span><br><span class="line">            self.out_proj.to(device)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>代码十分简单，有两点需要注意的。其一是<code>out_proj</code>
，这一般被称为输出投影，旨在将输出与输入的维度统一。其二是<code>mask</code>
，一般被称为因果掩蔽，之后我们会讨论。</p>
<h3 id="多注意力头-1">多注意力头</h3>
<hr />
<p>在定义完成注意力头之后，多注意力头也很容易定义，只需要将键，值和查询依次通过所有注意力头，然后拼接在一起即可。</p>
<ul>
<li><p><strong>MultiAttentionHead</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> attentionHead <span class="keyword">import</span> AttentionHead</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiAttentionHead</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, embed_dim, num_heads, k_dim=<span class="literal">None</span>, v_dim=<span class="literal">None</span>, out_proj=<span class="literal">False</span>, device=<span class="string">&quot;cpu&quot;</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiAttentionHead, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.device = device</span><br><span class="line">        self.attention_heads = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                AttentionHead(embed_dim, k_dim=k_dim, v_dim=v_dim, device=device)</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        self.out_proj = nn.Linear(num_heads * v_dim, embed_dim).to(device) <span class="keyword">if</span> out_proj <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, key, value, query, mask=<span class="literal">None</span></span>):</span><br><span class="line">        attention_heads = [</span><br><span class="line">            attention_head(key, value, query, mask)</span><br><span class="line">            <span class="keyword">for</span> attention_head <span class="keyword">in</span> self.attention_heads</span><br><span class="line">        ]</span><br><span class="line">        result =  torch.cat(attention_heads, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.out_proj != <span class="literal">None</span>:</span><br><span class="line">            result = self.out_proj(result)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to</span>(<span class="params">self, device</span>):</span><br><span class="line">        <span class="keyword">for</span> attention_head <span class="keyword">in</span> self.attention_heads:</span><br><span class="line">            attention_head.to(device)</span><br><span class="line">        self.device = device</span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>可以测试类是否正常工作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    multi_attention_head = MultiAttentionHead(</span><br><span class="line">        embed_dim=<span class="number">128</span>, num_heads=<span class="number">8</span>, k_dim=<span class="number">64</span>, v_dim=<span class="number">64</span>, device=device</span><br><span class="line">    )</span><br><span class="line">    key = torch.randn(<span class="number">16</span>, <span class="number">10</span>, <span class="number">128</span>).to(device)</span><br><span class="line">    value = torch.randn(<span class="number">16</span>, <span class="number">10</span>, <span class="number">128</span>).to(device)</span><br><span class="line">    query = torch.randn(<span class="number">16</span>, <span class="number">10</span>, <span class="number">128</span>).to(device)</span><br><span class="line">    attention = multi_attention_head(key, value, query)</span><br><span class="line">    <span class="built_in">print</span>(attention.shape)</span><br><span class="line">---</span><br><span class="line">torch.Size([<span class="number">16</span>, <span class="number">10</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure>
<p>可以看到512维的向量正是8个64维的向量（没有进行输出投影）拼接而成。有一点需要注意的是<code>embed_dim</code>
是每个注意力头的<code>embed</code>维度。</p>
<h3
id="使用torch.nn.multiheadattention">使用<strong>torch.nn.MultiheadAttention</strong></h3>
<hr />
<p>事实上，pytorch提供了多注意里头的模块，定义为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=<span class="number">0.0</span>, bias=<span class="literal">True</span>, add_bias_kv=<span class="literal">False</span>, add_zero_attn=<span class="literal">False</span>, kdim=<span class="literal">None</span>, vdim=<span class="literal">None</span>, batch_first=<span class="literal">False</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Pytorch提供的文档的翻译</p>
<ul>
<li><strong>embed_dim</strong> – 模型的总维度。</li>
<li><strong>num_heads</strong> –
并行注意力头的数量。注意，<code>embed_dim</code>
将会在这些头之间拆分（即每个头的维度为
<code>embed_dim // num_heads</code>）。</li>
<li><strong>dropout</strong> – 对注意力输出权重应用的 dropout
概率。默认不使用。</li>
<li><strong>bias</strong> –
如果指定，将会为输入/输出投影层添加偏置。默认值为 True。</li>
<li><strong>add_bias_kv</strong> – 如果指定，将会为键和值序列在
<code>dim=0</code> 上添加偏置。默认值为 False。</li>
<li><strong>add_zero_attn</strong> – 如果指定，将会在键和值序列的
<code>dim=1</code> 上添加一组全零的批次。默认值为 False。</li>
<li><strong>kdim</strong> – 键的特征总数。默认使用
<code>kdim=embed_dim</code>。</li>
<li><strong>vdim</strong> – 值的特征总数。默认使用
<code>vdim=embed_dim</code>。</li>
<li><strong>batch_first</strong> – 如果为 True，则输入和输出张量的形状为
<code>(batch, seq, feature)</code>。默认值为 False，形状为
<code>(seq, batch, feature</code>。</li>
</ul>
<hr />
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward(query, key, value, key_padding_mask=<span class="literal">None</span>, need_weights=<span class="literal">True</span>, attn_mask=<span class="literal">None</span>, average_attn_weights=<span class="literal">True</span>, is_causal=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>query (Tensor)</strong> – 查询嵌入，形状为
<code>(L, E_q)</code>（对于未批次输入），或者 <code>(L, N, E_q)</code>
当 <code>batch_first=False</code> 或者 <code>(N, L, E_q)</code> 当
<code>batch_first=True</code>。其中，<code>L</code>
为目标序列长度，<code>N</code> 为批次大小，<code>E_q</code>
为查询嵌入的维度 <code>embed_dim</code>。</li>
<li><strong>key (Tensor)</strong> – 键嵌入，形状为
<code>(S, E_k)</code>（未批次输入），或者 <code>(S, N, E_k)</code> 当
<code>batch_first=False</code>，或者 <code>(N, S, E_k)</code> 当
<code>batch_first=True</code>。其中，<code>S</code>
为源序列长度，<code>N</code> 为批次大小，<code>E_k</code> 为键嵌入维度
<code>kdim</code>。</li>
<li><strong>value (Tensor)</strong> – 值嵌入，形状为
<code>(S, E_v)</code>（未批次输入），或者 <code>(S, N, E_v)</code> 当
<code>batch_first=False</code>，或者 <code>(N, S, E_v)</code> 当
<code>batch_first=True</code>。其中，<code>S</code>
为源序列长度，<code>N</code> 为批次大小，<code>E_v</code> 为值嵌入维度
<code>vdim</code>。</li>
<li><strong>key_padding_mask (Optional[Tensor])</strong> –
如果指定，则为一个掩码，形状为
<code>(N, S)</code>，表示哪些键在注意力计算时将被忽略。对于未批次的查询，形状应为
<code>(S)</code>。支持二进制掩码和浮点掩码。对于二进制掩码，True
表示对应的键值将被忽略。对于浮点掩码，将会直接加到对应的键值上。</li>
<li><strong>need_weights (bool)</strong> – 如果指定，将在返回
<code>attn_outputs</code> 的同时返回
<code>attn_output_weights</code>。设置 <code>need_weights=False</code>
可以使用优化后的 <code>scaled_dot_product_attention</code>
并达到最佳性能。默认值为 True。</li>
<li><strong>attn_mask (Optional[Tensor])</strong> –
如果指定，将应用于防止某些位置的注意力。形状必须为 <code>(L, S)</code>
或者 <code>(N⋅num_heads, L, S)</code>，其中 <code>N</code>
为批次大小，<code>L</code> 为目标序列长度，<code>S</code>
为源序列长度。2D 掩码将会广播到整个批次，而 3D
掩码允许为批次中的每个条目使用不同的掩码。支持二进制和浮点掩码。二进制掩码中，True
值表示不允许对对应位置进行注意力。对于浮点掩码，掩码值将会加到注意力权重上。如果同时提供
<code>attn_mask</code> 和
<code>key_padding_mask</code>，它们的类型应该匹配。</li>
<li><strong>average_attn_weights (bool)</strong> – 如果为 True，返回的
<code>attn_weights</code>
将在各个头之间进行平均。否则，<code>attn_weights</code>
将会单独返回每个头的权重。此标志仅在 <code>need_weights=True</code>
时有效。默认值为 True（即在头之间平均权重）。</li>
<li><strong>is_causal (bool)</strong> –
如果指定，将会应用一个因果掩码作为注意力掩码。默认值为
False。警告：<code>is_causal</code> 提供了一个提示，表明
<code>attn_mask</code>
是因果掩码。提供错误的提示可能导致执行错误，包括正向和反向兼容性问题。</li>
</ul>
<hr />
<ul>
<li><strong>attn_output</strong> – 注意力输出，形状为
<code>(L, E)</code>（未批次输入），或者 <code>(L, N, E)</code> 当
<code>batch_first=False</code>，或者 <code>(N, L, E)</code> 当
<code>batch_first=True</code>。其中，<code>L</code>
为目标序列长度，<code>N</code> 为批次大小，<code>E</code> 为嵌入维度
<code>embed_dim</code>。</li>
<li><strong>attn_output_weights</strong> – 仅在
<code>need_weights=True</code> 时返回。如果
<code>average_attn_weights=True</code>，返回的注意力权重在各个头之间进行平均，形状为
<code>(L, S)</code>（未批次输入）或者 <code>(N, L, S)</code>。如果
<code>average_attn_weights=False</code>，则返回每个头的注意力权重，形状为
<code>(num_heads, L, S)</code>（未批次输入）或者
<code>(N, num_heads, L, S)</code>。</li>
</ul></li>
</ul>
<p>尝试实现一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch_multi_attention_head = nn.MultiheadAttention(</span><br><span class="line">        embed_dim=<span class="number">128</span>, num_heads=<span class="number">8</span>, batch_first=<span class="literal">True</span></span><br><span class="line">    ).to(device)</span><br><span class="line">torch_attension = torch_multi_attention_head(query, key, value)</span><br><span class="line"><span class="built_in">print</span>(torch_attension[<span class="number">0</span>].shape)</span><br><span class="line">---</span><br><span class="line">torch.Size([<span class="number">16</span>, <span class="number">10</span>, <span class="number">128</span>])</span><br></pre></td></tr></table></figure>
<p>在使用这个函数的时候必须注意，其参数中的<code>embed_dim, kdim, vdim</code>可能与文档中描述的作用并不相同。具体来说，这个类的计算可能经过一个如下过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.fused_attn:</span><br><span class="line">    x = F.scaled_dot_product_attention(</span><br><span class="line">        q, k, v,</span><br><span class="line">        dropout_p=self.attn_drop.p <span class="keyword">if</span> self.training <span class="keyword">else</span> <span class="number">0.</span>,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    q = q * self.scale</span><br><span class="line">    attn = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">    attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">    attn = self.attn_drop(attn)</span><br><span class="line">    x = attn @ v</span><br></pre></td></tr></table></figure>
<p>其中，<code>q,k,v</code>
的形状均为<code>(B, num_heads, N, head_dim)</code>
，其中<code>num_heads*head_dim = embed_dim</code>
。也就是原输入的embedding被拆分到不同的注意力头。</p>
<p>这里有一个比较有争议的地方，那就是线性层的设置问题。原文描述：</p>
<blockquote>
<p>we found it beneficial to linearly project the queries, keys and
values h times with different, learned linear projections to dk…</p>
</blockquote>
<p>也就是说，每个注意力头的线性层可能是不同的，正如在之前的代码中所实现的那样。而如今，很多实现使用的方式则类似于先使用三个线性层完成映射，再将得到的键，值，查询拆分到不同的注意力头，不同的注意力头仅仅计算注意力，不再进行权重计算。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_12.png" /></p>
<p>图中向右的路径为第一种实现，向左的路径为第二种实现。</p>
<p><span class="math display">\[
\begin{aligned}&amp; \operatorname{MultiHead}(Q, K,
V)=\operatorname{Concat}\left(\text { head }_1, \ldots, \text { head
}_{\mathrm{h}}\right) W^O \\&amp; \text { where head
}_{\mathrm{i}}=\operatorname{Attention}\left(Q W_i^Q, K W_i^K, V
W_i^V\right)\end{aligned}
\]</span></p>
<p>中明确显示了完整的键矩阵，值矩阵和查询向量被输入到每一个头中。</p>
<h3
id="参考torch的实现优化多注意力头">参考torch的实现，优化多注意力头</h3>
<hr />
<p>相比于最初直接根据原理实现的多注意力头，有几点优化方向是值得考虑的：</p>
<ul>
<li>增加偏置的选项，这是因为线性层的计算方式为：<span
class="math inline">\(f(x) = Wx+B\)</span>，其中<span
class="math inline">\(B\)</span>即为偏置。在<code>attention</code>的公式中，显然没有使用偏置。为此增加此选项。</li>
<li>增加<code>dropout</code>和归一化层的选项，关于其作用比较繁杂，可以自己查询。</li>
<li>增加<code>use_fused_attn</code>
选项，允许使用<strong>PyTorch</strong> 的
<code>scaled_dot_product_attention</code>函数来加速注意力计算。</li>
<li>增加<code>is_causal</code>
选项，以实施因果掩蔽，之后会讨论这一部分。</li>
<li>将原本的基于多个注意力类计算的过程直接集成在当前类中，理论上不会增加神经网络的计算速度，但将有效提升注意力的计算速度。同时，这种计算方式将允许使用<code>scaled_dot_product_attention</code>
函数。</li>
<li>允许使用布尔类型和浮点类型的掩蔽。</li>
</ul>
<p>优化后的代码如下，这是前文所描述的第一种实现方式的代码。代码中分离了神经网络计算和注意力计算，即分离了<span
class="math inline">\(Q_{im} = Q W_i^Q, K_{im}=K W_i^K, V_{im}=V
W_i^V\)</span>和<span
class="math inline">\(\operatorname{head}_{\mathrm{i}}=\operatorname{Attention}\left(Q_{im},
K_{im},
V_{im}\right)\)</span>两个过程。是否使用<code>scaled_dot_product_attention</code>
是可选的，但实际上，这个函数的实现基本上与<code>else</code>
部分描述的计算方式相同。</p>
<ul>
<li><p><strong>MultiAttentionHead_A</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiAttentionHead_A</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    A版本的多注意力头，这一版本的注意力头将键，值和查询完整的输入每一个注意力头中，每一个注意力头中都有完整的线性层，然后将每一个注意力头的输出拼接在一起，最后通过一个线性层进行输出。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ### 参数：</span></span><br><span class="line"><span class="string">    - embed_dim (int)：输入嵌入的维度大小。</span></span><br><span class="line"><span class="string">    - num_heads (int)：注意力头的数量。</span></span><br><span class="line"><span class="string">    - k_dim (int, 可选)：键（Key）和查询（Query）的维度大小。如果为 None，则使用 embed_dim 作为默认值。</span></span><br><span class="line"><span class="string">    - v_dim (int, 可选)：值（Value）的维度大小。如果为 None，则使用 embed_dim 作为默认值。</span></span><br><span class="line"><span class="string">    - out_proj (bool, 默认值为 False)：是否对输出进行线性投影，将输出向量的长度从v_dim映射到embed_dim。如果为 True，则在输出时应用一个线性层。</span></span><br><span class="line"><span class="string">    - device (str, 默认值为 &quot;cpu&quot;)：指定模型运行的设备，如 &quot;cpu&quot; 或 &quot;cuda&quot;。</span></span><br><span class="line"><span class="string">    - qkv_bias (bool, 默认值为 False)：是否在键、值和查询的线性层中使用偏置（Bias）。</span></span><br><span class="line"><span class="string">    - attn_drop (float, 默认值为 0.0)：注意力权重的 Dropout 概率。</span></span><br><span class="line"><span class="string">    - proj_drop (float, 默认值为 0.0)：输出投影的 Dropout 概率。</span></span><br><span class="line"><span class="string">    - norm_layer (nn.Module, 默认值为 nn.LayerNorm)：用于键和值的归一化层类型，只有在qk_norm为真时norm_layer才会启用。</span></span><br><span class="line"><span class="string">    - qk_norm (bool, 默认值为 False)：是否对键和查询进行归一化处理，只有在qk_norm为真时norm_layer才会启用。</span></span><br><span class="line"><span class="string">    - use_fused_attn (bool, 默认值为 False)：是否使用 PyTorch 的 scaled_dot_product_attention 函数来加速注意力计算。</span></span><br><span class="line"><span class="string">    - is_causal (bool, 默认值为 False)：是否启用因果遮掩，用于防止注意力机制关注未来的时间步，在使用因果掩蔽时必须保证不输入mask，否则会根据mask进行掩蔽。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ### 方法：</span></span><br><span class="line"><span class="string">    - __init__(self, embed_dim, num_heads, ...)</span></span><br><span class="line"><span class="string">        初始化 MultiAttentionHead_A 类，实例化内部的多头注意力机制、前馈神经网络、Dropout 和归一化层。</span></span><br><span class="line"><span class="string">    - forward(self, key, value, query, mask=None, return_attention=False)</span></span><br><span class="line"><span class="string">        - 输入：</span></span><br><span class="line"><span class="string">            - key (Tensor)：键（Key）的张量，形状为 (batch_size, sequence_length, k_dim)。</span></span><br><span class="line"><span class="string">            - value (Tensor)：值（Value）的张量，形状为 (batch_size, sequence_length, v_dim)。</span></span><br><span class="line"><span class="string">            - query (Tensor)：查询（Query）的张量，形状为 (batch_size, sequence_length, k_dim)。</span></span><br><span class="line"><span class="string">            - mask (Tensor, 可选)：注意力权重的遮掩张量，形状为 (sequence_length, sequence_length) 或 (batch_size, sequence_length, sequence_length)。</span></span><br><span class="line"><span class="string">            - return_attention (bool, 可选)：是否返回注意力权重，默认值为 False。</span></span><br><span class="line"><span class="string">        - 输出：</span></span><br><span class="line"><span class="string">            - 如果 return_attention=True，返回两项：</span></span><br><span class="line"><span class="string">                - result：多注意力头的输出，形状为 (batch_size, sequence_length, v_dim)。</span></span><br><span class="line"><span class="string">                - attention：注意力权重的均值，形状为 (batch_size, num_heads, sequence_length, sequence_length)。</span></span><br><span class="line"><span class="string">            - 如果 return_attention=False，仅返回 result。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        embed_dim,</span></span><br><span class="line"><span class="params">        num_heads,</span></span><br><span class="line"><span class="params">        k_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        v_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        out_proj=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        device=<span class="string">&quot;cpu&quot;</span>,</span></span><br><span class="line"><span class="params">        qkv_bias: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        attn_drop: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        proj_drop: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        norm_layer: nn.Module = nn.LayerNorm,</span></span><br><span class="line"><span class="params">        qk_norm: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        use_fused_attn: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        is_causal: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiAttentionHead_A, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.k_dim = k_dim <span class="keyword">if</span> k_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.v_dim = v_dim <span class="keyword">if</span> k_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.if_out_proj = out_proj</span><br><span class="line"></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line"></span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">        self.use_fused_attn = use_fused_attn</span><br><span class="line"></span><br><span class="line">        self.is_causal = is_causal</span><br><span class="line"></span><br><span class="line">        self.qk_norm = qk_norm</span><br><span class="line"></span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line">        self.attention_heads = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                nn.ModuleList(</span><br><span class="line">                    [</span><br><span class="line">                        nn.Linear(embed_dim, k_dim, bias=qkv_bias),</span><br><span class="line">                        nn.Linear(embed_dim, v_dim, bias=qkv_bias),</span><br><span class="line">                        nn.Linear(embed_dim, k_dim, bias=qkv_bias),</span><br><span class="line">                    ]</span><br><span class="line">                )</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        self.out_proj = (</span><br><span class="line">            nn.Linear(num_heads * v_dim, embed_dim) <span class="keyword">if</span> out_proj <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.v_norm_layer = norm_layer(self.v_dim) <span class="keyword">if</span> qk_norm <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.k_norm_layer = norm_layer(self.k_dim) <span class="keyword">if</span> qk_norm <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, key, value, query, mask=<span class="literal">None</span>, return_attention=<span class="literal">False</span></span>):</span><br><span class="line">        keys, values, queries = self.before_ultiattention(key, value, query)</span><br><span class="line">        batch_size, seq_len, k_dim = value.shape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.qk_norm:</span><br><span class="line">            keys = self.k_norm_layer(keys)</span><br><span class="line">            values = self.v_norm_layer(values)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_fused_attn:</span><br><span class="line">            x = F.scaled_dot_product_attention(</span><br><span class="line">                queries,</span><br><span class="line">                keys,</span><br><span class="line">                values,</span><br><span class="line">                dropout_p=self.attn_drop.p <span class="keyword">if</span> self.training <span class="keyword">else</span> <span class="number">0.0</span>,</span><br><span class="line">                attn_mask=mask,</span><br><span class="line">                is_causal=self.is_causal,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L, S = queries.size(-<span class="number">2</span>), keys.size(-<span class="number">2</span>)</span><br><span class="line">            attn_bias = torch.zeros(L, S, dtype=queries.dtype).to(self.device)</span><br><span class="line">            <span class="keyword">if</span> self.is_causal <span class="keyword">and</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">assert</span> mask <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                temp_mask = torch.ones(L, S, dtype=torch.<span class="built_in">bool</span>).tril(diagonal=<span class="number">0</span>)</span><br><span class="line">                attn_bias.masked_fill_(temp_mask.logical_not(), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">                attn_bias.to(queries.dtype)</span><br><span class="line">            <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> mask.dtype == torch.<span class="built_in">bool</span>:</span><br><span class="line">                    attn_bias.masked_fill_(mask.logical_not(), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    attn_bias += mask</span><br><span class="line">            scale_factor = <span class="number">1</span> / torch.sqrt(torch.tensor(queries.size(-<span class="number">1</span>)))</span><br><span class="line">            attn = queries @ keys.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * scale_factor</span><br><span class="line">            attn += attn_bias</span><br><span class="line">            attn = torch.softmax(attn, dim=-<span class="number">1</span>)</span><br><span class="line">            attn = self.attn_drop(attn)</span><br><span class="line">            x = attn @ values</span><br><span class="line"></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, self.num_heads * self.v_dim)</span><br><span class="line">        x = self.out_proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">if</span> return_attention <span class="keyword">and</span> <span class="keyword">not</span> self.use_fused_attn:</span><br><span class="line">            <span class="keyword">return</span> x, attn</span><br><span class="line">        <span class="keyword">return</span> x, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">before_ultiattention</span>(<span class="params">self, key, value, query</span>):</span><br><span class="line">        keys = torch.zeros(key.shape[<span class="number">0</span>], self.num_heads, key.shape[<span class="number">1</span>], self.k_dim).to(</span><br><span class="line">            self.device</span><br><span class="line">        )</span><br><span class="line">        values = torch.zeros(</span><br><span class="line">            value.shape[<span class="number">0</span>], self.num_heads, value.shape[<span class="number">1</span>], self.k_dim</span><br><span class="line">        ).to(self.device)</span><br><span class="line">        queries = torch.zeros(</span><br><span class="line">            query.shape[<span class="number">0</span>], self.num_heads, query.shape[<span class="number">1</span>], self.k_dim</span><br><span class="line">        ).to(self.device)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_heads):</span><br><span class="line">            m_key = self.attention_heads[i][<span class="number">0</span>](key)</span><br><span class="line">            m_value = self.attention_heads[i][<span class="number">1</span>](value)</span><br><span class="line">            m_query = self.attention_heads[i][<span class="number">2</span>](query)</span><br><span class="line">            keys[:, i] = m_key</span><br><span class="line">            values[:, i] = m_value</span><br><span class="line">            queries[:, i] = m_query</span><br><span class="line">        <span class="keyword">return</span> keys, values, queries</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>关于输出映射，实际上在每一个注意力头中使用输出映射和在最终拼接之后使用注意力映射本质上是等价的。</p>
<p>如果不使用输出映射，最终输出的尺寸是<span
class="math inline">\((batch\_size, number \ of \ query&#39;s \
tokens,v\_dim\times num\_heads)\)</span>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">key = torch.randn(<span class="number">16</span>, <span class="number">10</span>, <span class="number">128</span>).to(device)</span><br><span class="line">value = torch.randn(<span class="number">16</span>, <span class="number">10</span>, <span class="number">128</span>).to(device)</span><br><span class="line">query = torch.randn(<span class="number">16</span>, <span class="number">2</span>, <span class="number">128</span>).to(device)</span><br><span class="line">multi_attention_head_A = MultiAttentionHead_A(</span><br><span class="line">    embed_dim=<span class="number">128</span>,    num_heads=<span class="number">8</span>,    k_dim=<span class="number">64</span>,    </span><br><span class="line">    v_dim=<span class="number">64</span>,    out_proj=<span class="literal">False</span>,    device=device,    </span><br><span class="line">    qkv_bias=<span class="literal">True</span>,    attn_drop=<span class="number">0.1</span>,    proj_drop=<span class="number">0.1</span>,    </span><br><span class="line">    norm_layer=nn.LayerNorm,    qk_norm=<span class="literal">True</span>,    use_fused_attn=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">result = multi_attention_head_A(key, value, query)</span><br><span class="line"><span class="built_in">print</span>(result.shape)</span><br><span class="line">---</span><br><span class="line">torch.Size([<span class="number">16</span>, <span class="number">2</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure>
<p>经过测试，这种实现方式的参数量略多于pytorch中的实现，性能没有太大差异。</p>
<hr />
<p>为了方便之后测试，同样也实现一版第二种实现方式的多注意力头。只需要在第一种多注意力头上做一些调整：</p>
<ul>
<li><p><strong>MultiAttentionHead_B</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiAttentionHead_B</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    B版本的多注意力头，这一版本的注意力头将键，值和查询完整的输入线性层中，然后将结果平分到每一个注意力头中，每一个注意力头中没有线性层，然后将每一个注意力头的输出拼接在一起，最后通过一个线性层进行输出。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ### 参数：</span></span><br><span class="line"><span class="string">    - embed_dim (int)：输入嵌入的维度大小，注意如果不使用k_dim和v_dim这个大小必须是注意力头的倍数。</span></span><br><span class="line"><span class="string">    - num_heads (int)：注意力头的数量。</span></span><br><span class="line"><span class="string">    - k_dim (int, 可选)：键（Key）和查询（Query）的维度大小，这个大小必须是注意力头的倍数。如果为 None，则使用 embed_dim 作为默认值。</span></span><br><span class="line"><span class="string">    - v_dim (int, 可选)：值（Value）的维度大小，这个大小必须是注意力头的倍数。如果为 None，则使用 embed_dim 作为默认值。</span></span><br><span class="line"><span class="string">    - out_proj (bool, 默认值为 False)：是否对输出进行线性投影，将输出向量的长度从v_dim映射到embed_dim。如果为 True，则在输出时应用一个线性层。</span></span><br><span class="line"><span class="string">    - device (str, 默认值为 &quot;cpu&quot;)：指定模型运行的设备，如 &quot;cpu&quot; 或 &quot;cuda&quot;。</span></span><br><span class="line"><span class="string">    - qkv_bias (bool, 默认值为 False)：是否在键、值和查询的线性层中使用偏置（Bias）。</span></span><br><span class="line"><span class="string">    - attn_drop (float, 默认值为 0.0)：注意力权重的 Dropout 概率。</span></span><br><span class="line"><span class="string">    - proj_drop (float, 默认值为 0.0)：输出投影的 Dropout 概率。</span></span><br><span class="line"><span class="string">    - norm_layer (nn.Module, 默认值为 nn.LayerNorm)：用于键和值的归一化层类型，只有在qk_norm为真时norm_layer才会启用。</span></span><br><span class="line"><span class="string">    - qk_norm (bool, 默认值为 False)：是否对键和查询进行归一化处理，只有在qk_norm为真时norm_layer才会启用。</span></span><br><span class="line"><span class="string">    - use_fused_attn (bool, 默认值为 False)：是否使用 PyTorch 的 scaled_dot_product_attention 函数来加速注意力计算。</span></span><br><span class="line"><span class="string">    - is_causal (bool, 默认值为 False)：是否启用因果遮掩，用于防止注意力机制关注未来的时间步，在使用因果掩蔽时必须保证不输入mask，否则会根据mask进行掩蔽。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ### 方法：</span></span><br><span class="line"><span class="string">    - __init__(self, embed_dim, num_heads, ...)</span></span><br><span class="line"><span class="string">        初始化 MultiAttentionHead_B 类，实例化内部的多头注意力机制、前馈神经网络、Dropout 和归一化层。</span></span><br><span class="line"><span class="string">    - forward(self, key, value, query, mask=None, return_attention=False)</span></span><br><span class="line"><span class="string">        - 输入：</span></span><br><span class="line"><span class="string">            - key (Tensor)：键（Key）的张量，形状为 (batch_size, sequence_length, k_dim)。</span></span><br><span class="line"><span class="string">            - value (Tensor)：值（Value）的张量，形状为 (batch_size, sequence_length, v_dim)。</span></span><br><span class="line"><span class="string">            - query (Tensor)：查询（Query）的张量，形状为 (batch_size, sequence_length, k_dim)。</span></span><br><span class="line"><span class="string">            - mask (Tensor, 可选)：注意力权重的遮掩张量，形状为 (sequence_length, sequence_length) 或 (batch_size, sequence_length, sequence_length)。</span></span><br><span class="line"><span class="string">            - return_attention (bool, 可选)：是否返回注意力权重，默认值为 False。</span></span><br><span class="line"><span class="string">        - 输出：</span></span><br><span class="line"><span class="string">            - 如果 return_attention=True，返回两项：</span></span><br><span class="line"><span class="string">                - result：多注意力头的输出，形状为 (batch_size, sequence_length, v_dim)。</span></span><br><span class="line"><span class="string">                - attention：注意力权重的均值，形状为 (batch_size, num_heads, sequence_length, sequence_length)。</span></span><br><span class="line"><span class="string">            - 如果 return_attention=False，仅返回 result。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        embed_dim,</span></span><br><span class="line"><span class="params">        num_heads,</span></span><br><span class="line"><span class="params">        k_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        v_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        out_proj=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        device=<span class="string">&quot;cpu&quot;</span>,</span></span><br><span class="line"><span class="params">        qkv_bias: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        attn_drop: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        proj_drop: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        norm_layer: nn.Module = nn.LayerNorm,</span></span><br><span class="line"><span class="params">        qk_norm: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        use_fused_attn: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        is_causal: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiAttentionHead_B, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.k_dim = k_dim <span class="keyword">if</span> k_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.v_dim = v_dim <span class="keyword">if</span> k_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.if_out_proj = out_proj</span><br><span class="line"></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line"></span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">        self.use_fused_attn = use_fused_attn</span><br><span class="line"></span><br><span class="line">        self.is_causal = is_causal</span><br><span class="line"></span><br><span class="line">        self.qk_norm = qk_norm</span><br><span class="line"></span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line">        self.attention_heads = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                nn.Linear(embed_dim, k_dim, bias=qkv_bias),</span><br><span class="line">                nn.Linear(embed_dim, v_dim, bias=qkv_bias),</span><br><span class="line">                nn.Linear(embed_dim, k_dim, bias=qkv_bias),</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        self.out_proj = nn.Linear(v_dim, embed_dim) <span class="keyword">if</span> out_proj <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.v_norm_layer = norm_layer(self.v_dim//self.num_heads) <span class="keyword">if</span> qk_norm <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.k_norm_layer = norm_layer(self.k_dim//self.num_heads) <span class="keyword">if</span> qk_norm <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, key, value, query, mask=<span class="literal">None</span>, return_attention=<span class="literal">False</span></span>):</span><br><span class="line">        keys, values, queries = self.before_ultiattention(key, value, query)</span><br><span class="line">        batch_size, seq_len, k_dim = value.shape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_fused_attn:</span><br><span class="line">            x = F.scaled_dot_product_attention(</span><br><span class="line">                queries,</span><br><span class="line">                keys,</span><br><span class="line">                values,</span><br><span class="line">                dropout_p=self.attn_drop.p <span class="keyword">if</span> self.training <span class="keyword">else</span> <span class="number">0.0</span>,</span><br><span class="line">                attn_mask=mask,</span><br><span class="line">                is_causal=self.is_causal,</span><br><span class="line">            )</span><br><span class="line">            attn = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L, S = queries.size(-<span class="number">2</span>), keys.size(-<span class="number">2</span>)</span><br><span class="line">            attn_bias = torch.zeros(L, S, dtype=queries.dtype).to(self.device)</span><br><span class="line">            <span class="keyword">if</span> self.is_causal <span class="keyword">and</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">assert</span> mask <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                temp_mask = (</span><br><span class="line">                    torch.ones(L, S, dtype=torch.<span class="built_in">bool</span>).tril(diagonal=<span class="number">0</span>).to(self.device)</span><br><span class="line">                )</span><br><span class="line">                attn_bias.masked_fill_(temp_mask.logical_not(), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">                attn_bias.to(queries.dtype)</span><br><span class="line">            <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> mask.dtype == torch.<span class="built_in">bool</span>:</span><br><span class="line">                    attn_bias.masked_fill_(mask.logical_not(), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    attn_bias += mask</span><br><span class="line">            scale_factor = <span class="number">1</span> / torch.sqrt(</span><br><span class="line">                torch.tensor(queries.size(-<span class="number">1</span>), device=self.device)</span><br><span class="line">            )</span><br><span class="line">            attn = queries @ keys.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * scale_factor</span><br><span class="line">            attn += attn_bias</span><br><span class="line">            attn = torch.softmax(attn, dim=-<span class="number">1</span>)</span><br><span class="line">            attn = self.attn_drop(attn)</span><br><span class="line">            x = attn @ values</span><br><span class="line"></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, self.v_dim)</span><br><span class="line">        x = self.out_proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> return_attention:</span><br><span class="line">            <span class="keyword">return</span> x, attn</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">before_ultiattention</span>(<span class="params">self, key, value, query</span>):</span><br><span class="line">        m_key = self.attention_heads[<span class="number">0</span>](key)</span><br><span class="line">        m_value = self.attention_heads[<span class="number">1</span>](value)</span><br><span class="line">        m_query = self.attention_heads[<span class="number">2</span>](query)</span><br><span class="line">        keys = m_key.reshape(</span><br><span class="line">            key.shape[<span class="number">0</span>], self.num_heads, key.shape[<span class="number">1</span>], self.k_dim // self.num_heads</span><br><span class="line">        )</span><br><span class="line">        values = m_value.reshape(</span><br><span class="line">            value.shape[<span class="number">0</span>], self.num_heads, value.shape[<span class="number">1</span>], self.v_dim // self.num_heads</span><br><span class="line">        )</span><br><span class="line">        queries = m_query.reshape(</span><br><span class="line">            query.shape[<span class="number">0</span>], self.num_heads, query.shape[<span class="number">1</span>], self.k_dim // self.num_heads</span><br><span class="line">        )</span><br><span class="line">        keys = self.k_norm_layer(keys)</span><br><span class="line">        values = self.v_norm_layer(values)</span><br><span class="line">        <span class="keyword">return</span> keys, values, queries</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>这种注意力头要求<code>embed_dim</code>, <code>k_dim</code>,
<code>v_dim</code>必须是<code>num_heads</code>的倍数（如果同时使用了后两个，则不要求<code>embed_dim</code>是<code>num_heads</code>的倍数）。如果不使用输出映射，最终输出的尺寸为：<span
class="math inline">\((batch\_size, number \ of \ query&#39;s \
tokens,v\_dim)\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">key = torch.randn(<span class="number">16</span>, <span class="number">10</span>, <span class="number">128</span>).to(device)</span><br><span class="line">value = torch.randn(<span class="number">16</span>, <span class="number">10</span>, <span class="number">128</span>).to(device)</span><br><span class="line">query = torch.randn(<span class="number">16</span>, <span class="number">2</span>, <span class="number">128</span>).to(device)</span><br><span class="line">multi_attention_head_A = MultiAttentionHead_A(</span><br><span class="line">    embed_dim=<span class="number">128</span>,    num_heads=<span class="number">8</span>,    k_dim=<span class="number">64</span>,    </span><br><span class="line">    v_dim=<span class="number">64</span>,    out_proj=<span class="literal">False</span>,    device=device,    </span><br><span class="line">    qkv_bias=<span class="literal">True</span>,    attn_drop=<span class="number">0.1</span>,    proj_drop=<span class="number">0.1</span>,    </span><br><span class="line">    norm_layer=nn.LayerNorm,    qk_norm=<span class="literal">True</span>,    use_fused_attn=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">result = multi_attention_head_A(key, value, query)</span><br><span class="line"><span class="built_in">print</span>(result.shape)</span><br><span class="line">---</span><br><span class="line">torch.Size([<span class="number">16</span>, <span class="number">2</span>, <span class="number">64</span>])</span><br></pre></td></tr></table></figure>
<p>经测试，这种实现的性能略差官方实现和第一种实现，原因不明。如果需要使用第二种实现，建议使用pytorch的官方实现。</p>
<h2 id="掩蔽">掩蔽</h2>
<hr />
<p>根据注意力机制的描述，所有的值提供的信息都会被考虑。但这未必总是合理的。有些时候，我们可能希望手动的避免某些键值对为当前查询提供信息，对此，我们会对某些键值对进行掩蔽：</p>
<p><span class="math display">\[
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q
K^T}{\sqrt{d_k}}\right) MV
\]</span></p>
<hr />
<p>如我们曾经讨论过的例子，如果我们只希望前两组键值对提供信息，将在<code>softmax</code>
计算注意力权重之后，会与掩蔽矩阵相乘：</p>
<p><span class="math display">\[
\left[\begin{array}{llll}
0.36 &amp; 0.05 &amp; 0.13 &amp; 0.36
\end{array}\right]\times\left[\begin{array}{llll}
1 &amp; 1 &amp; 0 &amp; 0
\end{array}\right] = \left[\begin{array}{llll}
0.36 &amp; 0.05 &amp; 0 &amp; 0
\end{array}\right]
\]</span></p>
<p>最后计算值的加权和：</p>
<p><span class="math display">\[
\left[\begin{array}{llll}
0.36 &amp; 0.05 &amp; 0 &amp; 0
\end{array}\right] \times\left[\begin{array}{lll}
1 &amp; 0 &amp; 2 \\
2 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 2 \\
1 &amp; 0 &amp; 1
\end{array}\right] = \begin{array}{ccc}
{[0.46} &amp; 0.05 &amp; 0.72]
\end{array}
\]</span></p>
<p>因此最终的输出为：</p>
<p><span class="math display">\[
\operatorname{Attention}(Q, K, V,M)=\operatorname{softmax}\left(\frac{Q
K^T}{\sqrt{d_k}}\right) MV = \begin{array}{ccc}
{[0.46} &amp; 0.05 &amp; 0.72]
\end{array}
\]</span></p>
<p>给出了并不相同的结果。</p>
<h3 id="因果掩蔽">因果掩蔽</h3>
<hr />
<p>因果掩蔽是掩蔽的一种特殊情况，也可以认为是在进行某些任务是，为了直接给查询提供合适的键值对而来的一种掩蔽方式。以续写任务为例：</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_13.png" /></p>
<p>很明显，对于查询<code>the</code> 应该提供信息的的只有<code>the</code>
一个单词。而对于单词<code>pink</code> ，<code>the</code>
和<code>pink</code>
都可以被考虑。如此，使用矩阵的方式表达每个查询应该依赖的键值对，即可得到因果掩蔽矩阵：</p>
<p><span class="math display">\[
M^T=\left(\begin{array}{llllllllll}
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;
1 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;
1 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;
1 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;
1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;
1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;
1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp;
1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp;
1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp;
1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;
1
\end{array}\right)
\]</span></p>
<p>之所以称之为因果掩蔽矩阵，是因为该技术的设计基于保护因果关系，避免模型在训练过程中看到未来信息的方法，从而保持预测的因果顺序。而为什么有转置符号，会在之后进行解释。</p>
<blockquote>
<p><strong>必须要注意的是，在torch.nn.MultiheadAttention中，a
<code>True</code> value indicates that the corresponding position is not
allowed to
attend，即使用1来表示被掩蔽的部分，这与我们之前的描述正好相反。</strong></p>
</blockquote>
<h3 id="代码实现掩蔽">代码实现：掩蔽</h3>
<hr />
<p>之前的多注意力头中已经包括了掩蔽的实现，可以查看其效果。如果不使用<code>use_fused_attn</code>
原代码使用如下部分实现掩蔽。由此可见，如此实现的程序允许输入浮点类型或者布尔类型的掩蔽，或者直接在创建类时使用因果掩蔽选项。</p>
<ul>
<li><p>掩蔽的实现</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">attn_bias = torch.zeros(L, S, dtype=queries.dtype).to(self.device)</span><br><span class="line"><span class="keyword">if</span> self.is_causal <span class="keyword">and</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> mask <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    temp_mask = torch.ones(L, S, dtype=torch.<span class="built_in">bool</span>).tril(diagonal=<span class="number">0</span>)</span><br><span class="line">    attn_bias.masked_fill_(temp_mask.logical_not(), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">    attn_bias.to(queries.dtype)</span><br><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> mask.dtype == torch.<span class="built_in">bool</span>:</span><br><span class="line">        attn_bias.masked_fill_(mask.logical_not(), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_bias += mask</span><br><span class="line">scale_factor = <span class="number">1</span> / torch.sqrt(torch.tensor(queries.size(-<span class="number">1</span>)))</span><br><span class="line">attn = queries @ keys.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * scale_factor</span><br><span class="line">attn += attn_bias</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>首先展示一个使用<code>mask</code> 的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mask = torch.ones(<span class="number">10</span>, <span class="number">10</span>).to(device)</span><br><span class="line">mask = torch.triu(mask, diagonal=<span class="number">1</span>).to(<span class="built_in">bool</span>)</span><br><span class="line">result = multi_attention_head_B(key, value, query, mask)</span><br></pre></td></tr></table></figure>
<p>在进行掩蔽之前，权重矩阵：</p>
<ul>
<li><p>未经掩蔽的矩阵</p>
<p><span class="math display">\[
  \left(\begin{array}{cccccccccc}-0.0381 &amp; 0.5432 &amp; -0.0118
&amp; 0.2627 &amp; 0.0788 &amp; 0.1290 &amp; -0.3541 &amp; 0.2990 &amp;
0.3344 &amp; -0.0740 \\0.2167 &amp; -0.0504 &amp; -0.0263 &amp; -0.1630
&amp; -0.1355 &amp; 0.2601 &amp; 0.0644 &amp; -0.0341 &amp; -0.0834
&amp; 0.0492 \\-0.0306 &amp; 0.4404 &amp; -0.2725 &amp; 0.3975 &amp;
0.1741 &amp; 0.0236 &amp; -0.1656 &amp; 0.1605 &amp; 0.3072 &amp;
-0.1043 \\-0.1396 &amp; 0.0166 &amp; 0.0363 &amp; -0.0409 &amp; -0.3242
&amp; 0.0878 &amp; 0.1644 &amp; 0.4337 &amp; 0.4110 &amp; 0.2443
\\0.2115 &amp; 0.4581 &amp; -0.1963 &amp; 0.7275 &amp; -0.1038 &amp;
-0.1654 &amp; -0.0064 &amp; 0.1803 &amp; 0.4861 &amp; -0.1032 \\0.5943
&amp; 0.0956 &amp; -0.1599 &amp; -0.0260 &amp; 0.2320 &amp; -0.1796
&amp; -0.1026 &amp; -0.0930 &amp; 0.1034 &amp; -0.1179 \\-0.2529 &amp;
0.0564 &amp; 0.1895 &amp; -0.1583 &amp; -0.0082 &amp; 0.1960 &amp;
-0.0832 &amp; 0.0468 &amp; 0.1883 &amp; 0.0880 \\0.1068 &amp; 0.0028
&amp; 0.2026 &amp; -0.4659 &amp; -0.0571 &amp; 0.3256 &amp; -0.1600
&amp; 0.1143 &amp; 0.2052 &amp; 0.2048 \\0.0379 &amp; 0.2223 &amp;
0.0165 &amp; 0.1183 &amp; 0.1286 &amp; -0.1690 &amp; -0.1400 &amp;
0.1407 &amp; 0.0946 &amp; -0.1528 \\0.2522 &amp; -0.0824 &amp; 0.2295
&amp; 0.3596 &amp; -0.1602 &amp; -0.2169 &amp; 0.0931 &amp; -0.3860
&amp; 0.3353 &amp; 0.0802\end{array}\right)
  \]</span></p></li>
</ul>
<p>在进行掩蔽之后，权重矩阵：</p>
<ul>
<li><p>经过掩蔽的矩阵</p>
<p><span class="math inline">\(\left(\begin{array}{cccccccccc}-\infty
&amp; 0.5432 &amp; -0.0118 &amp; 0.2627 &amp; 0.0788 &amp; 0.1290 &amp;
-0.3541 &amp; 0.2990 &amp; 0.3344 &amp; -0.0740 \\-\infty &amp; -\infty
&amp; -0.0263 &amp; -0.1630 &amp; -0.1355 &amp; 0.2601 &amp; 0.0644
&amp; -0.0341 &amp; -0.0834 &amp; 0.0492 \\-\infty &amp; -\infty &amp;
-\infty &amp; 0.3975 &amp; 0.1741 &amp; 0.0236 &amp; -0.1656 &amp;
0.1605 &amp; 0.3072 &amp; -0.1043 \\-\infty &amp; -\infty &amp; -\infty
&amp; -\infty &amp; -0.3242 &amp; 0.0878 &amp; 0.1644 &amp; 0.4337 &amp;
0.4110 &amp; 0.2443 \\-\infty &amp; -\infty &amp; -\infty &amp; -\infty
&amp; -\infty &amp; -0.1654 &amp; -0.0064 &amp; 0.1803 &amp; 0.4861
&amp; -0.1032 \\-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp;
-\infty &amp; -\infty &amp; -0.1026 &amp; -0.0930 &amp; 0.1034 &amp;
-0.1179 \\-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp;
-\infty &amp; -\infty &amp; -\infty &amp; 0.0468 &amp; 0.1883 &amp;
0.0880 \\-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty
&amp; -\infty &amp; -\infty &amp; -\infty &amp; 0.2052 &amp; 0.2048
\\-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp;
-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -0.1528
\\-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp;
-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp;
-\infty\end{array}\right)\)</span></p></li>
</ul>
<p>或者使用因果掩蔽：</p>
<ul>
<li><p>未经掩蔽的矩阵</p>
<p><span class="math display">\[
  \left[\begin{array}{cccccccccc}0.0559 &amp; 0.4464 &amp; -0.0436 &amp;
-0.1111 &amp; 0.0050 &amp; -0.3171 &amp; 0.0839 &amp; -0.1784 &amp;
-0.0082 &amp; 0.1409 \\0.2037 &amp; 0.2918 &amp; 0.1126 &amp; 0.3927
&amp; 0.0867 &amp; -0.3940 &amp; 0.4468 &amp; -0.0292 &amp; -0.0809
&amp; -0.0723 \\-0.0866 &amp; -0.7469 &amp; 0.0212 &amp; 0.4034 &amp;
0.0483 &amp; 0.4977 &amp; -0.1441 &amp; 0.1939 &amp; 0.1353 &amp;
-0.2094 \\-0.1088 &amp; 0.1867 &amp; -0.3031 &amp; -0.7647 &amp; 0.0023
&amp; -0.3046 &amp; -0.5611 &amp; -0.3762 &amp; 0.2211 &amp; 0.3879
\\1.1036 &amp; 0.6895 &amp; 0.4887 &amp; -0.0168 &amp; 0.6623 &amp;
0.4858 &amp; 0.4474 &amp; 0.3889 &amp; -0.5162 &amp; -0.2902 \\-0.1813
&amp; 0.4428 &amp; -0.2485 &amp; -0.5210 &amp; -0.2160 &amp; 0.2273
&amp; -0.3022 &amp; -0.3756 &amp; 0.1138 &amp; 0.2180 \\0.2345 &amp;
-0.3673 &amp; -0.0208 &amp; -0.1335 &amp; 0.1385 &amp; 0.2149 &amp;
0.6787 &amp; -0.7335 &amp; -0.0106 &amp; 0.1181 \\-0.3721 &amp; 0.1472
&amp; -0.3148 &amp; -0.3902 &amp; -0.5528 &amp; -0.3957 &amp; 0.3321
&amp; -0.4586 &amp; 0.0931 &amp; -0.1888 \\0.2077 &amp; -0.1135 &amp;
0.3442 &amp; 0.6898 &amp; 0.2972 &amp; -0.0460 &amp; 0.5437 &amp; 0.3840
&amp; -0.2072 &amp; -0.1516 \\0.3234 &amp; -0.2309 &amp; 0.0452 &amp;
-0.1091 &amp; 0.2687 &amp; -0.0085 &amp; 0.3502 &amp; -0.3033 &amp;
-0.0314 &amp; 0.0379\end{array}\right]
  \]</span></p></li>
<li><p>掩蔽之后的矩阵</p>
<p><span class="math display">\[
  \left[\begin{array}{cccccccccc}
  0.0559 &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp;
-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty \\
  0.2037 &amp; 0.2918 &amp; -\infty &amp; -\infty &amp; -\infty &amp;
-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty \\
  -0.0866 &amp; -0.7469 &amp; 0.0212 &amp; -\infty &amp; -\infty &amp;
-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty \\
  -0.1088 &amp; 0.1867 &amp; -0.3031 &amp; -0.7647 &amp; -\infty &amp;
-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty \\
  1.1036 &amp; 0.6895 &amp; 0.4887 &amp; -0.0168 &amp; 0.6623 &amp;
-\infty &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty \\
  -0.1813 &amp; 0.4428 &amp; -0.2485 &amp; -0.5210 &amp; -0.2160 &amp;
0.2273 &amp; -\infty &amp; -\infty &amp; -\infty &amp; -\infty \\
  0.2345 &amp; -0.3673 &amp; -0.0208 &amp; -0.1335 &amp; 0.1385 &amp;
0.2149 &amp; 0.6787 &amp; -\infty &amp; -\infty &amp; -\infty \\
  -0.3721 &amp; 0.1472 &amp; -0.3148 &amp; -0.3902 &amp; -0.5528 &amp;
-0.3957 &amp; 0.3321 &amp; -0.4586 &amp; -\infty &amp; -\infty \\
  0.2077 &amp; -0.1135 &amp; 0.3442 &amp; 0.6898 &amp; 0.2972 &amp;
-0.0460 &amp; 0.5437 &amp; 0.3840 &amp; -0.2072 &amp; -\infty \\
  0.3234 &amp; -0.2309 &amp; 0.0452 &amp; -0.1091 &amp; 0.2687 &amp;
-0.0085 &amp; 0.3502 &amp; -0.3033 &amp; -0.0314 &amp; 0.0379
  \end{array}\right]
  \]</span></p></li>
</ul>
<h3 id="关于掩蔽矩阵的讨论">关于掩蔽矩阵的讨论</h3>
<hr />
<p>我们展示了两个例子，很明显的是，第一个例子的掩蔽是一个上三角矩阵，而第二个掩蔽的例子是一个下三角矩阵。为什么第二个例子才是因果掩蔽呢？我们需要更细致的讨论掩蔽矩阵到底对什么进行了掩蔽。</p>
<p>我们考虑一个例子：</p>
<p><span class="math display">\[
\begin{aligned}Q &amp; =\left[\begin{array}{lllll}1 &amp; 0 &amp; 2
&amp; 0 &amp; 1 \\0 &amp; 1 &amp; 1 &amp; 2 &amp; 0\end{array}\right] ,K
&amp; =\left[\begin{array}{lllll}1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\0
&amp; 1 &amp; 1 &amp; 1 &amp; 0 \\1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\0
&amp; 0 &amp; 1 &amp; 0 &amp; 1\end{array}\right] ,V &amp;
=\left[\begin{array}{lllll}1 &amp; 2 &amp; 0 &amp; 0 &amp; 1 \\0 &amp; 1
&amp; 2 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\2 &amp; 1
&amp; 0 &amp; 2 &amp; 2\end{array}\right]\end{aligned}
\]</span></p>
<p>计算注意力：</p>
<p><span class="math display">\[
\text { Attention Scores }=\left[\begin{array}{llll}
4 &amp; 2 &amp; 1 &amp; 1 \\
2 &amp; 4 &amp; 2 &amp; 1
\end{array}\right]
\]</span></p>
<p>很显然，两行的注意力得分对应的是两个查询。假设我们希望第一个查询只参考第一个键值对的信息，第二个查询参考前两个键值对的信息。那么我们的掩蔽矩阵应该是：</p>
<p><span class="math display">\[
\text { Mask }=\left[\begin{array}{llll}
1 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0
\end{array}\right]
\]</span></p>
<p>那么，假设我们有四条查询，对应的因果掩蔽矩阵应该是：</p>
<p><span class="math display">\[
\text { Mask }=\left[\begin{array}{llll}
1 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; 1
\end{array}\right]
\]</span></p>
<p>这与本节开始时展示的因果掩蔽的图示正好互为转置。更近一部分，考虑公式：</p>
<p><span class="math display">\[
\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) MV
\]</span></p>
<p>中的后两项的矩阵乘积规则，也可以得知对于每一个查询，掩蔽是一个行向量而非列向量。</p>
<h1 id="encoder-block">Encoder Block</h1>
<hr />
<h3 id="关于gpt的仅编码器结构">关于GPT的仅编码器结构</h3>
<hr />
<p>在论文《<em>attention is all you
need</em>》中，作者绘制了Transformer的结构，这是一种由编码器和解码器组成的结构。在论文《<em>Improving
Language Understanding by Generative
Pre-Training</em>》，作者也描述了本篇笔记所关注的GPT模型的结构，这是一种仅包含解码器的结构。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_14.png" /></p>
<p>尽管最早的Transformer模型同时具备编码器和解码器，但这并不是必须的。现在的Transformer通常会根据任务类型，选择仅编码器，仅解码器或者编码器-解码器结构。</p>
<ul>
<li>仅编码器结构更适合将文本转化为数字的任务，如分类或者识别等等；</li>
<li>仅解码器更适合给定文本提示，如“Thank for lunch， I had
a...”，通过迭代预测最可能的下一个单词来自动完成序列的模型，如GPT模型。</li>
<li>编码器解码器结构更适合从一个文本序列到另一个文本序列的任务。</li>
</ul>
<h3 id="encoder-block的结构">Encoder block的结构</h3>
<hr />
<p>尽管我们称GPT是一种仅仅包含解码器的结构，但从上图可以看出，其实现中的基本结构与原Transformer模型的编码器结构相同。我们将这种基础结构称为Encoder
Block，或者Transfromer Block，或者Encoder
layer等等。总之这是一种包含一个多注意力头，两个层归一化层和一组线性神经网络的结构。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_15.png" /></p>
<h3 id="层归一化">层归一化</h3>
<hr />
<p>Transformer结构中通常不使用在其他代码中广为应用的批归一化，以避免在批次内的序列之间产生归一化依赖。但是较新的研究也指出，批归一化的某种形式仍然可以在Transformer中使用，并且在性能上优于传统的层归一化方法。两种归一化方式的区别如下图所示。</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_16.png" /></p>
<h3 id="代码实现encoder-block">代码实现：encoder block</h3>
<hr />
<p>我们已经实现了多注意力，实现encoder block也是十分简单的：</p>
<ul>
<li><p><strong>EncoderBlock</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    EncoderBlock 是一个用于 Transformer 架构中的编码器块，结合了多头自注意力机制和前馈神经网络，帮助模型有效捕捉输入序列中不同位置之间的依赖关系。该模块利用了 MultiAttentionHead_B，这是一个支持并行注意力计算的多头注意力机制，通过将输入嵌入向量映射到多个注意力头，再合并输出进行进一步处理。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ### 参数：</span></span><br><span class="line"><span class="string">    - embed_dim (int)：输入嵌入向量的维度。</span></span><br><span class="line"><span class="string">    - num_heads (int)：多头自注意力机制中的头数。</span></span><br><span class="line"><span class="string">    - k_dim (int, 可选)：键（Key）和查询（Query）的维度大小。这个大小必须是 num_heads 的倍数。如果未指定，默认值为 embed_dim。</span></span><br><span class="line"><span class="string">    - v_dim (int, 可选)：值（Value）的维度大小。这个大小也必须是 num_heads 的倍数。如果未指定，默认值为 embed_dim。</span></span><br><span class="line"><span class="string">    - hidden_dim (int, 可选)：前馈网络的隐藏层维度。如果未指定，默认值为 embed_dim。</span></span><br><span class="line"><span class="string">    - qkv_bias (bool, 可选)：是否在生成查询（Query）、键（Key）、值（Value）时使用偏置项，默认值为 False。</span></span><br><span class="line"><span class="string">    - attn_drop (float, 可选)：多头自注意力机制中的注意力权重的 Dropout 概率，默认值为 0.0。</span></span><br><span class="line"><span class="string">    - proj_drop (float, 可选)：多头自注意力机制的输出投影的 Dropout 概率，默认值为 0.0。</span></span><br><span class="line"><span class="string">    - dense_drop (float, 可选)：前馈网络的 Dropout 概率，默认值为 0.0。</span></span><br><span class="line"><span class="string">    - encoder_norm_layer (nn.Module, 可选)：用于编码器的归一化层类型，默认值为 nn.LayerNorm。</span></span><br><span class="line"><span class="string">    - attention_norm_layer (nn.Module, 可选)：用于注意力层的归一化层类型，默认值为 nn.LayerNorm。</span></span><br><span class="line"><span class="string">    - qk_norm (bool, 可选)：是否对查询（Query）和键（Key）进行归一化处理，默认值为 False。仅当 qk_norm=True 时，attention_norm_layer 才会启用。</span></span><br><span class="line"><span class="string">    - use_fused_attn (bool, 可选)：是否使用 PyTorch 的 scaled_dot_product_attention 函数来加速注意力计算，默认值为 False。</span></span><br><span class="line"><span class="string">    - is_causal (bool, 可选)：是否启用因果遮掩，用于防止注意力机制关注未来的时间步，默认值为 True。</span></span><br><span class="line"><span class="string">    - device (str, 可选)：指定模型运行的设备，如 &quot;cpu&quot; 或 &quot;cuda&quot;，默认值为 &quot;cpu&quot;。</span></span><br><span class="line"><span class="string">    ### 方法：</span></span><br><span class="line"><span class="string">    - __init__(self, embed_dim, num_heads, ...)</span></span><br><span class="line"><span class="string">        初始化 EncoderBlock 类，实例化内部的多头注意力机制、前馈神经网络、Dropout 和归一化层。</span></span><br><span class="line"><span class="string">    - forward(self, x, return_attention=True)</span></span><br><span class="line"><span class="string">        - 输入：</span></span><br><span class="line"><span class="string">            - x (Tensor)：输入的嵌入向量，形状为 (batch_size, sequence_length, embed_dim)。</span></span><br><span class="line"><span class="string">            - return_attention (bool, 可选)：是否返回注意力权重，默认值为 True。</span></span><br><span class="line"><span class="string">        - 输出：</span></span><br><span class="line"><span class="string">            - 如果 return_attention=True，返回两项：</span></span><br><span class="line"><span class="string">                - result：编码器块的输出，形状为 (batch_size, sequence_length, embed_dim)。</span></span><br><span class="line"><span class="string">                - attention：注意力权重的均值，形状为 (batch_size, num_heads, sequence_length, sequence_length)。</span></span><br><span class="line"><span class="string">                - 如果 return_attention=False，仅返回 result。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        embed_dim,</span></span><br><span class="line"><span class="params">        num_heads,</span></span><br><span class="line"><span class="params">        k_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        v_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        hidden_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        attn_drop=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        proj_drop=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        dense_drop=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        encoder_norm_layer=nn.LayerNorm,</span></span><br><span class="line"><span class="params">        attention_norm_layer=nn.LayerNorm,</span></span><br><span class="line"><span class="params">        qk_norm=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        use_fused_attn=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        is_causal=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        device=<span class="string">&quot;cpu&quot;</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.k_dim = k_dim <span class="keyword">if</span> k_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.v_dim = v_dim <span class="keyword">if</span> v_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.hidden_dim = hidden_dim <span class="keyword">if</span> hidden_dim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.attn_drop = attn_drop</span><br><span class="line">        self.proj_drop = proj_drop</span><br><span class="line">        self.dense_drop = dense_drop</span><br><span class="line">        self.qk_norm = qk_norm</span><br><span class="line">        self.qkv_bias = qkv_bias</span><br><span class="line">        self.encoder_norm_layer = encoder_norm_layer</span><br><span class="line">        self.attention_norm_layer = attention_norm_layer</span><br><span class="line">        self.use_fused_attn = use_fused_attn</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">        self.attn = MultiAttentionHead_A(</span><br><span class="line">            embed_dim=self.embed_dim,</span><br><span class="line">            num_heads=self.num_heads,</span><br><span class="line">            k_dim=self.k_dim,</span><br><span class="line">            v_dim=self.v_dim,</span><br><span class="line">            out_proj=<span class="literal">True</span>,</span><br><span class="line">            qkv_bias=self.qkv_bias,</span><br><span class="line">            attn_drop=self.attn_drop,</span><br><span class="line">            proj_drop=self.proj_drop,</span><br><span class="line">            qk_norm=self.qk_norm,</span><br><span class="line">            norm_layer=self.attention_norm_layer,</span><br><span class="line">            use_fused_attn=self.use_fused_attn,</span><br><span class="line">            device=self.device,</span><br><span class="line">            is_causal=is_causal,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># self.attn = nn.MultiheadAttention(</span></span><br><span class="line">        <span class="comment">#     embed_dim=self.embed_dim,</span></span><br><span class="line">        <span class="comment">#     num_heads=self.num_heads,</span></span><br><span class="line">        <span class="comment">#     kdim=self.k_dim,</span></span><br><span class="line">        <span class="comment">#     vdim=self.v_dim,</span></span><br><span class="line">        <span class="comment">#     dropout=self.attn_drop,</span></span><br><span class="line">        <span class="comment">#     add_zero_attn=False,</span></span><br><span class="line">        <span class="comment">#     bias=self.qkv_bias,</span></span><br><span class="line">        <span class="comment">#     batch_first=True,</span></span><br><span class="line">        <span class="comment">#     device=self.device,</span></span><br><span class="line">        <span class="comment"># )        </span></span><br><span class="line">        self.dropout1 = nn.Dropout(self.dense_drop)</span><br><span class="line">        self.lnorm1 = self.encoder_norm_layer(self.embed_dim)</span><br><span class="line">        self.linear1 = nn.Linear(self.embed_dim, self.hidden_dim)</span><br><span class="line">        self.ReLU = nn.ReLU()</span><br><span class="line">        self.linear2 = nn.Linear(self.hidden_dim, self.embed_dim)</span><br><span class="line">        self.dropout2 = nn.Dropout(self.dense_drop)</span><br><span class="line">        self.lnorm2 = self.encoder_norm_layer(self.embed_dim)</span><br><span class="line">        self.dropout3 = nn.Dropout(self.dense_drop)</span><br><span class="line">        self.to(self.device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, return_attention=<span class="literal">False</span></span>):</span><br><span class="line">        seq_len = x.size(<span class="number">1</span>)</span><br><span class="line">        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=<span class="number">1</span>).<span class="built_in">bool</span>().to(x.device)</span><br><span class="line">        result, attention = self.attn(x, x, x, mask=<span class="literal">None</span>, return_attention=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># result, attention = self.attn(x, x, x, need_weights=True, attn_mask=mask)</span></span><br><span class="line">        result = self.dropout1(result)</span><br><span class="line">        result = x + result</span><br><span class="line">        result = self.lnorm1(result)</span><br><span class="line">        x = result</span><br><span class="line">        result = self.linear1(result)</span><br><span class="line">        result = self.ReLU(result)</span><br><span class="line">        result = self.dropout2(result)</span><br><span class="line">        result = self.linear2(result)</span><br><span class="line">        result = self.dropout3(result)</span><br><span class="line">        result = self.lnorm2(result + x)</span><br><span class="line">        <span class="keyword">if</span> return_attention <span class="keyword">and</span> attention <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> result, torch.mean(attention, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> return_attention:</span><br><span class="line">            <span class="keyword">return</span> result, <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>在实现之后可以测试代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">encoder_block = EncoderBlock(  embed_dim=<span class="number">512</span>,  num_heads=<span class="number">8</span>,  k_dim=<span class="number">64</span>,  v_dim=<span class="number">64</span>,  hidden_dim=<span class="number">2048</span>,  qkv_bias=<span class="literal">False</span>,  attn_drop=<span class="number">0.0</span>,  proj_drop=<span class="number">0.0</span>,  dense_drop=<span class="number">0.0</span>,  encoder_norm_layer=nn.LayerNorm,  attention_norm_layer=nn.LayerNorm,  qk_norm=<span class="literal">False</span>,  use_fused_attn=<span class="literal">False</span>,  is_causal=<span class="literal">True</span>,  device=<span class="string">&quot;cpu&quot;</span>,)</span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">10</span>, <span class="number">512</span>)</span><br><span class="line">result, attention = encoder_block(x)</span><br><span class="line"><span class="built_in">print</span>(result.shape)</span><br><span class="line"><span class="built_in">print</span>(attention.shape)</span><br><span class="line">---</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">512</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>值得注意的是，在此程序的<code>forward</code>中增加了<code>return_attention</code>
参数，以返回每组键值对得到的权重。这依赖于之前一直没有讨论的多注意力<code>forward</code>的<code>return_attention</code>
参数。这个参数是可选的，大部分时候我们都并不需要这一项。</p>
<hr />
<p><strong>使用nn.TransformerEncoderLayer和nn.TransformerEncoder实现</strong></p>
<p>使用类似如下的方式可以实现相同的encoderblock</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">transformer_layer = nn.TransformerEncoderLayer(</span><br><span class="line">  d_model=embedding_dim, </span><br><span class="line">  nhead=num_heads, </span><br><span class="line">  dim_feedforward=feed_forward_dim,</span><br><span class="line">  batch_first=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)</span><br></pre></td></tr></table></figure>
<p>更多的参数和使用方式可以参照官方文档。</p>
<h1 id="gpt模型">GPT模型</h1>
<hr />
<h3 id="gpt模型的构建">GPT模型的构建</h3>
<hr />
<p>GPT模型的基础结构如下：</p>
<p><img
src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/post%2FTransformer-01%2Fimage_17.png" /></p>
<p>模型中包括12个encoder
block。数据经过字符编码和位置编码之后，输入encoder
block种，然后根据不同的任务输入不同的头中。</p>
<hr />
<p>尝试实现GPT模型，此模型包含一个预测任务的任务头，即那个线性层。</p>
<ul>
<li><p><strong>GPT</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vocab_size,</span></span><br><span class="line"><span class="params">        embed_dim,</span></span><br><span class="line"><span class="params">        num_heads,</span></span><br><span class="line"><span class="params">        num_layers,</span></span><br><span class="line"><span class="params">        max_len,</span></span><br><span class="line"><span class="params">        feed_forward_dim = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        device=<span class="string">&quot;cpu&quot;</span>,</span></span><br><span class="line"><span class="params">        use_fused_attn=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(GPT, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.max_len = max_len</span><br><span class="line">        self.device = device</span><br><span class="line">        feed_forward_dim = embed_dim <span class="keyword">if</span> feed_forward_dim <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> feed_forward_dim</span><br><span class="line"></span><br><span class="line">        self.token_embedding = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.position_embedding = nn.Embedding(max_len, embed_dim)</span><br><span class="line">        self.encoder_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                EncoderBlock(embed_dim, num_heads, device=device, use_fused_attn=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(embed_dim, vocab_size)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, return_attention=<span class="literal">False</span></span>):</span><br><span class="line">        seq_len = x.size(<span class="number">1</span>)</span><br><span class="line">        positions = torch.arange(<span class="number">0</span>, seq_len, device=x.device).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        token_embeddings = self.token_embedding(x)</span><br><span class="line">        position_embeddings = self.position_embedding(positions)</span><br><span class="line">        x = token_embeddings + position_embeddings</span><br><span class="line">        attentions = []</span><br><span class="line">        <span class="keyword">for</span> encoder_block <span class="keyword">in</span> self.encoder_blocks:</span><br><span class="line">            x, attention = encoder_block(x, return_attention=<span class="literal">True</span>)</span><br><span class="line">            attentions.append(attention)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">if</span> return_attention:</span><br><span class="line">            <span class="keyword">return</span> x, attentions</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>假设输入的尺寸为<span
class="math inline">\(x(batchsize,len)\)</span>，那么输出的尺寸为<span
class="math inline">\(y(batchsize,len,vocabsize)\)</span>，其中<span
class="math inline">\(vocabsize\)</span>是词汇表尺寸，即最终输出是一个词汇表大小的概率分布，即词汇表中每个词成为下一个词的可能的概率。</p>
<p>在使用pytorch的encoderblock时，需注意因果掩蔽的实现方式。</p>
<h3 id="gpt模型的预训练">GPT模型的预训练</h3>
<hr />
<p>GPT模型，如期所称，是一种生成式预训练模型。预训练过程中，我们关注模型“理解文本”的能力。具体来时，我们希望最大化如下似然函数：</p>
<p><span class="math display">\[
L_1(U)=\sum_i \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ;
\Theta\right)
\]</span></p>
<p>其中，</p>
<ul>
<li><span class="math inline">\(k\)</span>
表示上下文窗口的大小，到目前为止我们默认这一项为文本长度。</li>
<li><span class="math inline">\(P\)</span> 是条件概率，由带参数 <span
class="math inline">\(\Theta\)</span> 的神经网络建模。</li>
<li>参数 <span class="math inline">\(\Theta\)</span> 通过随机梯度下降
(Stochastic Gradient Descent, SGD) 进行训练。</li>
</ul>
<p>及我们希望通过随机梯度下降，调整神经网络参数，以使其在已知现有信息的情况下，推测下一个信息。即在第二节中的开始展示的游戏。</p>
<p>我们使用Adam优化器，原文同时使用了学习率在最初的2000次更新中从零线性增加，并随后使用余弦调度（cosine
schedule）逐步衰减至0的学习率更新策略，可以使用如下的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  <span class="comment"># 初始学习率为0</span></span><br><span class="line">TOTAL_STEPS = <span class="built_in">len</span>(train_loader) * EPOCHS</span><br><span class="line">WARMUP_STEPS = TOTAL_STEPS // <span class="number">5</span></span><br><span class="line"><span class="comment"># 定义学习率调度策略</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lr_lambda</span>(<span class="params">current_step</span>):</span><br><span class="line">    <span class="keyword">if</span> current_step &lt; WARMUP_STEPS:</span><br><span class="line">        <span class="keyword">return</span> current_step / WARMUP_STEPS</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * (</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">            + torch.cos(</span><br><span class="line">                (current_step - WARMUP_STEPS) / (TOTAL_STEPS - WARMUP_STEPS) * torch.pi</span><br><span class="line">            )</span><br><span class="line">        )  <span class="comment"># 余弦调度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建学习率调度器</span></span><br><span class="line">scheduler = LambdaLR(optimizer, lr_lambda)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> idx, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        x, y = x.to(device), y.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(x)</span><br><span class="line">        loss = criterion(output.view(-<span class="number">1</span>, VOCAB_SIZE), y.view(-<span class="number">1</span>))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        <span class="keyword">if</span> idx % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span>, Step: <span class="subst">&#123;idx&#125;</span>, Loss: <span class="subst">&#123;loss.item() / BATCH_SIZE&#125;</span>, LR: <span class="subst">&#123;scheduler.get_last_lr()[<span class="number">0</span>]&#125;</span>&quot;</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<p>此外，预训练任务的结果与分类任务十分相似。最终的向量是一个长度为<code>vocab_size</code>
的张量，而目标值为一个长度为<code>vocab_size</code>
的one-hot编码。因此我们使用交叉熵函数作为损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h3 id="gpt模型的预训练结果测试">GPT模型的预训练结果测试</h3>
<hr />
<p>预训练模型根据所有已经获得的知识预测下一个单词，因此可以直接向它提供一个开头，指定最大长度和温度，使其完成续写。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text</span>(<span class="params">model, start_token, max_len, device, temperature=<span class="number">1.0</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    generated_tokens = [start_token]</span><br><span class="line">    input_tensor = torch.tensor([generated_tokens], dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_len - <span class="number">1</span>):</span><br><span class="line">            outputs = model(input_tensor)</span><br><span class="line">            next_token_logits = outputs[<span class="number">0</span>, -<span class="number">1</span>, :]</span><br><span class="line">            next_token_logits = next_token_logits / temperature</span><br><span class="line">            next_token_probs = F.softmax(next_token_logits, dim=-<span class="number">1</span>)</span><br><span class="line">            next_token = torch.multinomial(next_token_probs, num_samples=<span class="number">1</span>).item()</span><br><span class="line">            generated_tokens.append(next_token)</span><br><span class="line">            input_tensor = torch.tensor(</span><br><span class="line">                [generated_tokens], dtype=torch.long, device=device</span><br><span class="line">            )</span><br><span class="line">    <span class="keyword">return</span> generated_tokens</span><br></pre></td></tr></table></figure>
<p>在这个函数中，temperature只影响随机选择时的概率。</p>
<h3
id="关于在模型中只指定一部分参数参加训练">关于在模型中只指定一部分参数参加训练</h3>
<hr />
<p><strong>使用 <code>requires_grad</code> 设置梯度计算开关</strong></p>
<p>对不希望训练的参数，将其属性 <code>requires_grad</code> 设置为
<code>False</code>。在这种情况下，这些参数的梯度不会被计算，也不会参与优化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.trainable_layer = nn.Linear(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        self.frozen_layer = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.trainable_layer(x)</span><br><span class="line">        x = self.frozen_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = MyModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 冻结 frozen_layer 的参数</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.frozen_layer.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 requires_grad 状态</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: requires_grad=<span class="subst">&#123;param.requires_grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>在优化器中仅添加需要训练的参数</strong></p>
<p>只将 <code>requires_grad=True</code> 的参数添加到优化器中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters()), lr=<span class="number">0.001</span></span><br></pre></td></tr></table></figure>
<p><strong>手动指定参数组</strong></p>
<p>可以在定义优化器时手动划分参数组。例如，某些层的参数使用一个学习率，其他层则冻结。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam([</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.trainable_layer.parameters()&#125;,  <span class="comment"># 参与训练的参数</span></span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.frozen_layer.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.0</span>&#125;  <span class="comment"># 不参与训练的参数</span></span><br><span class="line">], lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<h3 id="使gpt模型应用于不同的任务">使GPT模型应用于不同的任务</h3>
<hr />
<p>除了我们在预训练时使用的预测任务之外，还可以进行多种语言相关的任务。为此，我们希望在预训练之后，保存gpt模型的一部分不再改动，而是仅训练任务头，即finetune。</p>
<p>为此，稍微修改gpt模型以实现在pretrain时训练gpt模型，并在finetune时训练任务头。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vocab_size,</span></span><br><span class="line"><span class="params">        embed_dim,</span></span><br><span class="line"><span class="params">        num_heads,</span></span><br><span class="line"><span class="params">        num_layers,</span></span><br><span class="line"><span class="params">        max_len,</span></span><br><span class="line"><span class="params">        feed_forward_dim = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        device=<span class="string">&quot;cpu&quot;</span>,</span></span><br><span class="line"><span class="params">        use_fused_attn=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(GPT, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.max_len = max_len</span><br><span class="line">        self.device = device</span><br><span class="line">        feed_forward_dim = embed_dim <span class="keyword">if</span> feed_forward_dim <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> feed_forward_dim</span><br><span class="line"></span><br><span class="line">        self.token_embedding = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.position_embedding = nn.Embedding(max_len, embed_dim)</span><br><span class="line">        self.encoder_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                EncoderBlock(embed_dim, num_heads, device=device, use_fused_attn=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(embed_dim, vocab_size)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, return_attention=<span class="literal">False</span></span>):</span><br><span class="line">        seq_len = x.size(<span class="number">1</span>)</span><br><span class="line">        positions = torch.arange(<span class="number">0</span>, seq_len, device=x.device).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        token_embeddings = self.token_embedding(x)</span><br><span class="line">        position_embeddings = self.position_embedding(positions)</span><br><span class="line">        x = token_embeddings + position_embeddings</span><br><span class="line">        attentions = []</span><br><span class="line">        <span class="keyword">for</span> encoder_block <span class="keyword">in</span> self.encoder_blocks:</span><br><span class="line">            x, attention = encoder_block(x, return_attention=<span class="literal">True</span>)</span><br><span class="line">            attentions.append(attention)</span><br><span class="line">        <span class="comment"># x = self.linear(x)</span></span><br><span class="line">        <span class="keyword">if</span> return_attention:</span><br><span class="line">            <span class="keyword">return</span> x, attentions</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TaskHead</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, num_classes, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        num_classes: int, number of classes, if task is &#x27;classification&#x27;, num_classes is the number of classes, if task is &#x27;prediction&#x27;, num_classes is the size of vocabulary</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(TaskHead, self).__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.linear = nn.Linear(embed_dim, num_classes)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        x = self.softmax(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPT_AT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, gpt, task_head, mode = <span class="string">&#x27;pretrain&#x27;</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        mode: str, &#x27;pretrain&#x27; or &#x27;finetune&#x27;</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(GPT_AT, self).__init__()</span><br><span class="line">        self.mode = mode</span><br><span class="line">        self.gpt = gpt</span><br><span class="line">        self.task_head = task_head</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="string">&#x27;finetune&#x27;</span>:</span><br><span class="line">            self.gpt.requires_grad = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.gpt(x)</span><br><span class="line">        x = self.task_head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">return_module</span>(<span class="params">self, module_name</span>):</span><br><span class="line">        <span class="keyword">return</span> self.gpt, self.task_head</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn">Raphael Hyaan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://raphaelhyaan.cn/2024/12/04/informatique/deeplearning/Transformer-01/">http://raphaelhyaan.cn/2024/12/04/informatique/deeplearning/Transformer-01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://raphaelhyaan.cn" target="_blank">Raphael's Home</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/10/informatique/ts/ts-dc-1/" title="降噪：维纳滤波"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F清1.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">降噪：维纳滤波</div></div></a></div><div class="next-post pull-right"><a href="/2024/12/04/nature/animal-2-1/" title="柏林自然博物馆的金刚鹦鹉"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F叶01.png" onerror="onerror=null;src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">柏林自然博物馆的金刚鹦鹉</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/02/26/informatique/deeplearning/Generative-11/" title="GAN,VAE和流模型的原理（以流模型为主）"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2F蝶1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-26</div><div class="title">GAN,VAE和流模型的原理（以流模型为主）</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_1/" title="Chapter 1 Generative Modeling"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 1 Generative Modeling</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_10/" title="Chapter 10 Advanced GANs 各种各样的GAN"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 10 Advanced GANs 各种各样的GAN</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_2/" title="Chapter 2 Deep Learning"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 2 Deep Learning</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_3/" title="Chapter 3 Variational Autoencoders 自动变分编码器"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 3 Variational Autoencoders 自动变分编码器</div></div></a></div><div><a href="/2024/01/20/informatique/deeplearning/Generative_4/" title="Chapter 4 Generative Adversarial Networks 生成对抗网络"><img class="cover" src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2Fgenerative.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-20</div><div class="title">Chapter 4 Generative Adversarial Networks 生成对抗网络</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-2.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffriend_404.gif'" alt="avatar"/></div><div class="author-info__name">Raphael Hyaan</div><div class="author-info__description">何日可谓归去来</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">186</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/raphaelhyaan"><i class="fab fa-github"></i><span>Bonjour</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/raphaelhyaan" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:raphael.ma.yuhan@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">仰观宇宙之大，俯察品类之盛</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer01-gpt%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0"><span class="toc-text">Transformer01
gpt模型的原理与实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8Ernn%E5%88%B0transformer"><span class="toc-text">从RNN到Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84-encoder-decoder-framework"><span class="toc-text">编码器-解码器架构
Encoder-Decoder Framework</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-transfer-learning"><span class="toc-text">迁移学习 Transfer Learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-text">数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81-token-encoding"><span class="toc-text">字符编码 token encoding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%97%E7%AC%A6%E5%88%86%E8%AF%8D"><span class="toc-text">字符分词</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E8%AF%8D%E5%88%86%E8%AF%8D"><span class="toc-text">单词分词</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">代码实现：数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dataset"><span class="toc-text">Dataset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-positional-encoding"><span class="toc-text">位置编码 Positional Encoding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%BC%A6%E6%9B%B2%E7%BA%BF%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-sinusoidal-positional-encoding"><span class="toc-text">正弦曲线位置编码
Sinusoidal Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-learnable-positional-encoding"><span class="toc-text">可学习的位置编码
Learnable Positional Encoding</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-1"><span class="toc-text">注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-attention"><span class="toc-text">注意力机制 Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E9%94%AE%E5%92%8C%E5%80%BC-queries-keys-and-values"><span class="toc-text">查询、键和值 Queries, Keys,
and Values</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%A4%B4"><span class="toc-text">多注意力头</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%A4%B4"><span class="toc-text">代码实现：多注意力头</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%A4%B4"><span class="toc-text">注意力头</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%A4%B4-1"><span class="toc-text">多注意力头</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8torch.nn.multiheadattention"><span class="toc-text">使用torch.nn.MultiheadAttention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83torch%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BC%98%E5%8C%96%E5%A4%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%A4%B4"><span class="toc-text">参考torch的实现，优化多注意力头</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A9%E8%94%BD"><span class="toc-text">掩蔽</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%A0%E6%9E%9C%E6%8E%A9%E8%94%BD"><span class="toc-text">因果掩蔽</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E6%8E%A9%E8%94%BD"><span class="toc-text">代码实现：掩蔽</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E6%8E%A9%E8%94%BD%E7%9F%A9%E9%98%B5%E7%9A%84%E8%AE%A8%E8%AE%BA"><span class="toc-text">关于掩蔽矩阵的讨论</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#encoder-block"><span class="toc-text">Encoder Block</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8Egpt%E7%9A%84%E4%BB%85%E7%BC%96%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84"><span class="toc-text">关于GPT的仅编码器结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder-block%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-text">Encoder block的结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">层归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0encoder-block"><span class="toc-text">代码实现：encoder block</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#gpt%E6%A8%A1%E5%9E%8B"><span class="toc-text">GPT模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="toc-text">GPT模型的构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-text">GPT模型的预训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C%E6%B5%8B%E8%AF%95"><span class="toc-text">GPT模型的预训练结果测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E5%9C%A8%E6%A8%A1%E5%9E%8B%E4%B8%AD%E5%8F%AA%E6%8C%87%E5%AE%9A%E4%B8%80%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E5%8F%82%E5%8A%A0%E8%AE%AD%E7%BB%83"><span class="toc-text">关于在模型中只指定一部分参数参加训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BFgpt%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E4%BA%8E%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BB%BB%E5%8A%A1"><span class="toc-text">使GPT模型应用于不同的任务</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-5/" title="第五部分-应对不确定性与大规模问题"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第五部分-应对不确定性与大规模问题"/></a><div class="content"><a class="title" href="/2026/01/06/algo-5/" title="第五部分-应对不确定性与大规模问题">第五部分-应对不确定性与大规模问题</a><time datetime="2026-01-05T16:07:15.000Z" title="发表于 2026-01-06 00:07:15">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-4/" title="第四部分-计算复杂性与近似解"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第四部分-计算复杂性与近似解"/></a><div class="content"><a class="title" href="/2026/01/06/algo-4/" title="第四部分-计算复杂性与近似解">第四部分-计算复杂性与近似解</a><time datetime="2026-01-05T16:07:14.000Z" title="发表于 2026-01-06 00:07:14">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-3/" title="第三部分-精确最优化策略"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第三部分-精确最优化策略"/></a><div class="content"><a class="title" href="/2026/01/06/algo-3/" title="第三部分-精确最优化策略">第三部分-精确最优化策略</a><time datetime="2026-01-05T16:07:12.000Z" title="发表于 2026-01-06 00:07:12">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-2/" title="第二部分-基于规模的策略：分解与变换 (Structure Decomposition)"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第二部分-基于规模的策略：分解与变换 (Structure Decomposition)"/></a><div class="content"><a class="title" href="/2026/01/06/algo-2/" title="第二部分-基于规模的策略：分解与变换 (Structure Decomposition)">第二部分-基于规模的策略：分解与变换 (Structure Decomposition)</a><time datetime="2026-01-05T16:07:11.000Z" title="发表于 2026-01-06 00:07:11">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/algo-1/" title="第一部分-算法分析基础与开发规范 (Foundations &amp; Methodology)"><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ftop_image%2FR-7.png" onerror="this.onerror=null;this.src='https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2Ffavicon.png'" alt="第一部分-算法分析基础与开发规范 (Foundations &amp; Methodology)"/></a><div class="content"><a class="title" href="/2026/01/06/algo-1/" title="第一部分-算法分析基础与开发规范 (Foundations &amp; Methodology)">第一部分-算法分析基础与开发规范 (Foundations &amp; Methodology)</a><time datetime="2026-01-05T16:07:10.000Z" title="发表于 2026-01-06 00:07:10">2026-01-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By Raphael Hyaan</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="framework-info"><span>备案号: </span><a href="href=&quot;https://beian.miit.gov.cn/&quot; ">京ICP备2024051904号</a><span class="footer-separator">|</span><img src="https://raphaelhyaan-1322456377.cos.ap-beijing.myqcloud.com/source%2F%E5%A4%87%E6%A1%88%E5%9B%BE%E6%A0%87.png" alt="MIT License" height="20" align="top"/><span> </span><a href="href=&quot;https://beian.mps.gov.cn/#/query/webSearch?code=11010802044068&quot; ">京公网安备11010802044068号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>